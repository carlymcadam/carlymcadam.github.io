[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is Carly’s CSCI 451 blog"
  },
  {
    "objectID": "posts/new-new-test-post/index.html",
    "href": "posts/new-new-test-post/index.html",
    "title": "Timnit Gebru",
    "section": "",
    "text": "from source import Perceptron\np = Perceptron()\n\nI did it!!\nnot implemented\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-new-test-post/index.html#math",
    "href": "posts/new-new-test-post/index.html#math",
    "title": "Timnit Gebru",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/new-test-post/index.html",
    "href": "posts/new-test-post/index.html",
    "title": "Second Post",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-test-post/index.html#math",
    "href": "posts/new-test-post/index.html#math",
    "title": "Second Post",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "from source import Perceptron\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/auditing-bias/index.html",
    "href": "posts/auditing-bias/index.html",
    "title": "Auditing Bias",
    "section": "",
    "text": "Abstract\nIn this blog post we’ll be looking at data from a bank. We’ll start with some data exploration and visualization to get a better understanding of the features in the dataset. Our ultimate goal here is to create a decision-making system that will determine whether a prospective borrower should be offered a loan. We will use logistic regression to assign weights to different features in the dataset, which we will then use to give each prospective borrower a “score”. This score will be used to determine whether the borrower will prospective receive a loan based on a threshold that will optimize profits for the bank. After creating this decision-making system, we’ll test it out on a test dataset and do some exploration into who our system predicted would be able to repay their loans, and who it predicted wouldn’t.\n\n# Read in the data\n\nSTATE = \"CA\"\n\ndata_source = ACSDataSource(survey_year='2018', \n                            horizon='1-Year', \n                            survey='person')\n\nacs_data = data_source.get_data(states=[STATE], download=True)\nacs_data=acs_data[['PINCP', 'AGEP', 'SCHL', 'MAR', 'RELP', 'DIS', 'ESP', 'CIT', 'MIG', 'MIL', 'ANC', 'NATIVITY', 'DEAR', 'DEYE', 'DREM', 'SEX', 'RAC1P', 'ESR']]\n\nacs_data.head()\n\n\n\n\n\n\n\n\nPINCP\nAGEP\nSCHL\nMAR\nRELP\nDIS\nESP\nCIT\nMIG\nMIL\nANC\nNATIVITY\nDEAR\nDEYE\nDREM\nSEX\nRAC1P\nESR\n\n\n\n\n0\n48500.0\n30\n14.0\n1\n16\n2\nNaN\n1\n3.0\n4.0\n1\n1\n2\n2\n2.0\n1\n8\n6.0\n\n\n1\n0.0\n18\n14.0\n5\n17\n2\nNaN\n1\n1.0\n4.0\n1\n1\n2\n2\n2.0\n2\n1\n6.0\n\n\n2\n13100.0\n69\n17.0\n1\n17\n1\nNaN\n1\n1.0\n2.0\n2\n1\n2\n2\n2.0\n1\n9\n6.0\n\n\n3\n0.0\n25\n1.0\n5\n17\n1\nNaN\n1\n1.0\n4.0\n1\n1\n1\n2\n1.0\n1\n1\n6.0\n\n\n4\n0.0\n31\n18.0\n5\n16\n2\nNaN\n1\n1.0\n4.0\n1\n1\n2\n2\n2.0\n2\n1\n6.0\n\n\n\n\n\n\n\n\n# echo : false\ndf_train.head()\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_home_ownership\nperson_emp_length\nloan_intent\nloan_grade\nloan_amnt\nloan_int_rate\nloan_status\nloan_percent_income\ncb_person_default_on_file\ncb_person_cred_hist_length\n\n\n\n\n0\n25\n43200\nRENT\nNaN\nVENTURE\nB\n1200\n9.91\n0\n0.03\nN\n4\n\n\n1\n27\n98000\nRENT\n3.0\nEDUCATION\nC\n11750\n13.47\n0\n0.12\nY\n6\n\n\n2\n22\n36996\nRENT\n5.0\nEDUCATION\nA\n10000\n7.51\n0\n0.27\nN\n4\n\n\n3\n24\n26000\nRENT\n2.0\nMEDICAL\nC\n1325\n12.87\n1\n0.05\nN\n4\n\n\n4\n29\n53004\nMORTGAGE\n2.0\nHOMEIMPROVEMENT\nA\n15000\n9.63\n0\n0.28\nN\n10\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n26059\n36\n150000\nMORTGAGE\n8.0\nEDUCATION\nA\n3000\n7.29\n0\n0.02\nN\n17\n\n\n26060\n23\n48000\nRENT\n1.0\nVENTURE\nA\n4325\n5.42\n0\n0.09\nN\n4\n\n\n26061\n22\n60000\nRENT\n0.0\nMEDICAL\nB\n15000\n11.71\n0\n0.25\nN\n4\n\n\n26062\n30\n144000\nMORTGAGE\n12.0\nPERSONAL\nC\n35000\n12.68\n0\n0.24\nN\n8\n\n\n26063\n25\n60000\nRENT\n5.0\nEDUCATION\nA\n21450\n7.29\n1\n0.36\nN\n4\n\n\n\n\n26064 rows × 12 columns\n\n\n\n\n\nExploring The Data\nLet’s start with some visualizations to explore patterns in this dataset:\n\n\nPlotting Code\n# Filter out rows with employment length greater than 25 years\ndf_train = df_train[df_train['person_emp_length'] &lt;= 25]\n\n# Bucket the employment length into 5-year chunks\nbins = range(0, 25 + 5, 5)\nlabels = [\"0 to 5 years\", \"5 to 10 years\", \"10 to 15 years\", \"15 to 20 years\", \"20 to 25 years\"]\n\ndf_train['emp_length_bucket'] = pd.cut(\n    df_train['person_emp_length'],\n    bins=bins,\n    right=True,\n    labels=labels,\n    include_lowest=True\n)\n\n# Create a Seaborn plot using histplot with multiple=\"fill\" for normalized (proportional) stacked bars\nplt.figure(figsize=(10, 6))\nsns.histplot(\n    data=df_train,\n    x='emp_length_bucket',\n    hue='loan_intent',\n    multiple='fill',  # normalizes each bar to sum to 1 (proportions)\n    shrink=0.8, \n    palette=\"Set2\"\n)\nplt.title(\"Loan Intent Proportions by Employment Length\")\nplt.xlabel(\"Employment Length\")\nplt.ylabel(\"Proportion\")\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nFigure 1: Loan intent by length of employment\n\n\n\n\n\n\nPlotting Code\n# Plot the distribution of interest rates of each of the home ownership statuses \nsns.boxplot(data=df_train, x=\"person_home_ownership\", y=\"loan_int_rate\", palette=\"Set2\")\nplt.xlabel(\"Home Ownership Status\")\nplt.ylabel(\"Loan Interest Rate\")\n\n\nText(0, 0.5, 'Loan Interest Rate')\nFigure 2: Interest rate distribution by home ownership status\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncredit_history_length\n\n\nloan_grade\n\n\n\n\n\nA\n5.7\n\n\nB\n5.8\n\n\nC\n5.9\n\n\nD\n5.8\n\n\nE\n5.8\n\n\nF\n6.1\n\n\nG\n6.6\n\n\n\n\n\nFigure 3: mean credit history length by loan grade\n\n\nWe’re looking here to understand some of the patterns in this dataset.\nFirst, in Figure 1, we can see that borrowers’ intentions for their loans change a bit as the length of their employment increases. Specifically, after 20 years of employment people stop borrowing to pay for education, which makes sense. People also start taking out more loans for venture and medical uses as their employment goes on. It looks like the proportion of loans that are intended to be used for home improvement, personal reasons, and debt consolidation remain fairly constant across length of employment.\nMoving on to Figure 2, we can see that borrowers with different home ownership statuses (renting, owning, mortgaging or other), have differences in the interest rate they might receive on their loan. Borrowers who rent their home seem more likely to have an interest rate of over 20%, and we can see that borrowers who own their homes rarely see interests rates over 20%. Borrowers who rent and mortgage their homes have more variability in the interest rates they might see, as well.\nFinally, we can see in Figure 3 that loan grades have differences in average length of credit of the borrower. It seems that borrowers with higher loan grades have longer credit histories, on average.\nWe can see that we should be able to get some information about whether a borrower will pay back a loan from some of these features.\n\nPrepping the Data\n\n# Encode the qualitative variables quantitatively\ndef preprocess(df):\n    \"\"\"\n    Function to preprocess the data\n    \"\"\"\n    le_home = LabelEncoder()\n    df[\"person_home_ownership\"] = le_home.fit_transform(df[\"person_home_ownership\"])\n\n    le_intent = LabelEncoder()\n    df[\"loan_intent\"] = le_intent.fit_transform(df[\"loan_intent\"])\n    intent_mapping = dict(zip(le_intent.transform(le_intent.classes_), le_intent.classes_))\n\n    df = pd.get_dummies(df, columns=['cb_person_default_on_file'], prefix='default')\n    df = df.dropna()\n\n    # Change the interest rate column to a percentage\n    df[\"loan_int_rate\"] = df[\"loan_int_rate\"]/100\n\n    # Separate our the target variable\n    y = df[\"loan_status\"]\n\n    return df, y, intent_mapping\n\ndf_train, y_train, intent_map = preprocess(df_train)\n\nWe will use this function prepare our datasets for analysis. We’re encoding the qualitative columns for home ownership and loan intent as integers, one-hot encoding the historical default true/false column as 0s and 1s, separating the target variable\n\n\n\nBuilding A Model\n\nChoosing Features\n\ndf_train['loan_amnt'].mean()\n\n9627.86831527438\n\n\nFirst, let’s figure out which features we want to use for the model. We’ll generate a few new features that we can use to better understand the relationships between our features: - Credit history ratio: \\(\\frac{\\text{credit history length}}{\\text{age}}\\) - Employment stability: \\(\\frac{\\text{employment length}}{\\text{age}}\\) - Income stability: \\(\\text{income}*\\text{employment length}\\)\nWe’ll use cross validation to test a few combinations of features: 1. Employment stability, income, and percentage of income that the loan is 2. Loan intent, credit history ratio, home ownership status 3. income stability, loan interest rate, age\n\n# Create new features \ndef new_features(df):\n    df[\"credit_history_ratio\"] = round(df[\"cb_person_cred_hist_length\"]/df_train[\"person_age\"],2)\n    df[\"employment_stability\"] = round(df[\"person_emp_length\"]/df_train[\"person_age\"], 2)\n    df[\"income_stability\"] = round(df[\"person_income\"]*df_train[\"person_emp_length\"],2)\n    return df\ndf_train = new_features(df_train)\n\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\n\n# Define three different sets of features\nfeature_sets = {\n    \"Set 1\": [\"employment_stability\", \"person_income\", \"loan_percent_income\"],\n    \"Set 2\": [\"loan_intent\", \"credit_history_ratio\", \"person_home_ownership\"],\n    \"Set 3\": [\"income_stability\", \"loan_int_rate\", \"person_age\"]\n}\n\nmodel = LogisticRegression()\n\n# Evaluate each feature set\nresults = {}\nfor set_name, features in feature_sets.items():\n    X_train = df_train[features]\n    X_train = StandardScaler().fit_transform(X_train)  # Standardize features \n    \n    scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')\n    mean_accuracy = np.mean(scores)\n    results[set_name] = mean_accuracy\n    print(f\"{set_name}: Mean Accuracy = {mean_accuracy:.4f}\")\n\nSet 1: Mean Accuracy = 0.8246\nSet 2: Mean Accuracy = 0.7850\nSet 3: Mean Accuracy = 0.8096\n\n\n\n\nEstimating weights\nAfter fitting an LR to each of the feature sets and using cross validation to test how accurate the models are, we can see that our first set of features, which includes employment stability, income, and percentage of income that the loan is has the highest average accuracy. Now, we’ll fit an LR model to these features to obtain our vector of weights that we’ll use in our scoring function.\n\nweights_model = LogisticRegression()\nX_train = df_train[feature_sets[\"Set 1\"]]\nX_train = StandardScaler().fit_transform(X_train)\nweights_model.fit(X_train, y_train)\n\nfor feature, coef in zip(feature_sets[\"Set 1\"], weights_model.coef_[0]):\n    print(f\"{feature}: {coef:.4f}\")\n\nemployment_stability: -0.1480\nperson_income: -0.6256\nloan_percent_income: 0.7925\n\n\nNow we can use these coefficients in our as the values in our weight vector when we create a scoring function, which we’ll do next.\n\n\n\nCreating the score function\nOur next step is to give each borrower a score, using the features and weights that we decided on above. We’ll add a “score” column to the train set that has the result of our score function:\n\\(\\text{score} = -0.1486 * x_1 - 0.6252 * x_2 + 0.7925 * x_3\\)\nwhere\n\\(x_1\\) = employment_stability\n\\(x_2\\) = person_income\n\\(x_3\\) = loan_percent_income\n\ndef score_dataframe(df):\n    \"\"\"\n    Compute the score for each borrower using the weights \n    determined by the logistic regression\n\n    inputs:\n    df: Pandas DataFrame with all features used to generate weights\n    and loan status, amount, and interest rate for profit calculations\n    \"\"\"\n    df_scored = df\n    df_scored[[\"employment_stability\", \"person_income\", \"loan_percent_income\"]] = StandardScaler().fit_transform(df_scored[[\"employment_stability\", \"person_income\", \"loan_percent_income\"]])\n    df_scored[\"score\"] =  round(-0.1486 * df_train[\"employment_stability\"] - 0.6252 * df_scored[\"person_income\"] + 0.7925 * df_scored[\"loan_percent_income\"],2)\n    return df_scored\n\n\ndef compute_profit(threshold, df):\n    \"\"\"\n    Compute the profit that the bank would have made if they had\n    used the predictions determined by our score function\n    and threshold\n\n    inputs:\n    threshold: integer value for score threshold \n    df: Pandas DataFrame with loan amount, interest rate, actual loan status, and score from \n    scoring function\n    \"\"\"\n    df[\"pred_default\"] = df[\"score\"].apply(lambda x: 1 if x &gt; threshold else 0)\n    \n    # Calculate potential profit for repayment or defaulting \n    df[\"repaid_profit\"] = round(df[\"loan_amnt\"] * (1 + 0.25 * df[\"loan_int_rate\"])**10 - df[\"loan_amnt\"],2)\n    df[\"defaulted_profit\"] = round(df[\"loan_amnt\"] * (1 + 0.25 * df[\"loan_int_rate\"])**3 - 1.7 * df[\"loan_amnt\"],2)\n    \n    # Profit based on actual loan status versus predicted status\n    df[\"profit\"] = 0  # Default to zero profit\n    df.loc[(df[\"loan_status\"] == 0) & (df[\"pred_default\"] == 0), \"profit\"] = df[\"repaid_profit\"]  # Successful repayment\n    df.loc[(df[\"loan_status\"] == 1) & (df[\"pred_default\"] == 0), \"profit\"] = df[\"defaulted_profit\"]  # Incorrectly predicted as good loan, default\n    df.loc[(df[\"loan_status\"] == 1) & (df[\"pred_default\"] == 1), \"profit\"] = 0  # Correctly predicted as default, no profit\n    df.loc[(df[\"loan_status\"] == 0) & (df[\"pred_default\"] == 1), \"profit\"] = 0  # Incorrectly predicted as default, no profit\n\n    # The borrowers are only the people we predict will not default\n    # borrowers = df.loc[(df[\"pred_default\"] == 0)]\n\n    return sum(df[\"profit\"]) / len(df)\n\n\n# Find optimal threshold\nscored_df = score_dataframe(df_train)\nthresholds = np.arange(0.5,2,0.05)\nprofits = [compute_profit(t, scored_df) for t in thresholds]\n\n# Find best threshold\noptimal_threshold = thresholds[np.argmax(profits)]\noptimal_profit = max(profits)\nprint(f\"Optimal Threshold for Maximum Profit: {optimal_threshold:.4f}\")\nprint(f\"Maximum Profit: {round(optimal_profit,2):.4f}\")\n\n\n# Plot profit vs. threshold\nsns.set_style(\"whitegrid\")\nsns.set_palette(\"Set2\")\n\nplt.figure(figsize=(8, 5))\nplt.scatter(optimal_threshold, optimal_profit, color=sns.color_palette(\"Set2\")[1], label=f'Optimal t={optimal_threshold:.4f}', zorder=2)\nsns.lineplot(x=thresholds, y=profits, label=\"Profit\", zorder=1)\nplt.xlabel(\"Threshold\")\nplt.ylabel(\"Profit per borrower\")\nplt.title(\"Threshold vs Profit\")\nplt.legend()\nplt.show()\n\nOptimal Threshold for Maximum Profit: 1.1000\nMaximum Profit: 1325.2300\n\n\n\n\n\n\n\n\n\n\n\nEvaluating from the bank’s perspective\nNow, we want to look at how this decision making system works on a test dataset. We’ll apply all of the functions we used on the training set with the test set, and see what the profit per borrower will look like using the threshold from above, 0.95.\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/test.csv\"\ndf_test = pd.read_csv(url)\ndf_test, y_test, intent_map = preprocess(df_test)\ndf_test = new_features(df_test)\n    \nscored_test_df = score_dataframe(df_test)\nprint(round(compute_profit(0.95, scored_test_df),2))\n\n1140.1\n\n\nIt looks like we ended up with a fairly similar profit per borrower on the test set to what we got from the train set, $X per borrower on the test set and $X on the train set. From the bank’s perspective, this seems positive. If they are making, on average, $1146 per borrower, they would make around $6,000,000 off of this dataset of borrowers.\n\n\nEvaluating from the borrower’s perspective\nNow, let’s try to get a better understanding of who in the test dataset we are predicting is going to default on their loans, and who we are predicting is going to repay them. We’ll look at the following questions:\n\nIs it more difficult for people in certain age groups to access credit under your proposed system?\nIs it more difficult for people to get loans in order to pay for medical expenses? How does this compare with the actual rate of default in that 3. group? What about people seeking loans for business ventures or education?\nHow does a person’s income level impact the ease with which they can access credit under your decision system?\n\n\nQuestion 1\n\n# Split the borrower ages into 4 buckets and see \nscored_test_df[\"age_group\"] = pd.qcut(scored_test_df[\"person_age\"], q=4, labels=[\"Young\", \"Middle-aged\", \"Older\", \"Senior\"])\nage_access = scored_test_df.groupby(\"person_age\")[\"pred_default\"].mean()\n\n\n\nperson_age\nYoung          0.213508\nMiddle-aged    0.153792\nOlder          0.159735\nSenior         0.163636\nName: pred_default, dtype: float64\n\n\nThese probabilities are the mean of the prediction column, so they tell us the rate at which a 1 is predicted in each of the age buckets using our decision making system. That is, they tell us the loan rejection rate for each age group. We can see that younger people and seniors are more likely to be rejected for a loan than middle aged people are.\n\n\nQuestion 2\n\n\n# Calculate rejection rates and actual default rates for loan intent\nloan_intent_access = scored_test_df.groupby(\"loan_intent\")[\"pred_default\"].mean()\nloan_intent_default_rates = scored_test_df.groupby(\"loan_intent\")[\"loan_status\"].mean()\n\nloan_intent_df = pd.DataFrame({\n    \"Loan Intent (Decoded)\": loan_intent_access.index.map(intent_map),\n    \"Rejection Rate\": loan_intent_access.values,\n    \"Actual Default Rate\": loan_intent_default_rates.values\n})\nloan_intent_df\n\n\n\n\n\n\n\n\nLoan Intent (Decoded)\nRejection Rate\nActual Default Rate\n\n\n\n\n0\nDEBTCONSOLIDATION\n0.159292\n0.287611\n\n\n1\nEDUCATION\n0.162415\n0.167517\n\n\n2\nHOMEIMPROVEMENT\n0.100649\n0.250000\n\n\n3\nMEDICAL\n0.169618\n0.284250\n\n\n4\nPERSONAL\n0.159319\n0.220441\n\n\n5\nVENTURE\n0.145228\n0.146266\n\n\n\n\n\n\n\n\n# echo: false\nloan_intent_df\n\nWe’re interested in the loans that are intended for medical uses. We can see that medical loans have the highest rate of rejection from our system (17% of medical loan requests are rejected), but they also have one of the highest rates of actual default (28%). Venture and business loans have much lower rates of rejection and their actual default rates are much closer to their rejection rates.\n\n\nQuestion 3\n\nscored_test_df[\"income_group\"] = pd.qcut(scored_test_df[\"person_income\"], q=4, labels=[\"Low\", \"Lower-middle\", \"Upper-middle\", \"High\"])\nincome_access = scored_test_df.groupby(\"income_group\")[\"pred_default\"].mean()\n\nincome_group\nLow             0.342638\nLower-middle    0.178375\nUpper-middle    0.081466\nHigh            0.005827\nName: pred_default, dtype: float64\n\n\n::: {#cell-47 .cell 0=‘e’ 1=‘c’ 2=‘h’ 3=‘o’ 4=’ ’ 5=‘:’ 6=‘f’ 7=‘a’ 8=‘l’ 9=‘s’ 10=‘e’}\nincome_access\n:::\nHere, we are again looking at the loan rejection rates, this time by income group. It is clear that people with lower incomes are much more likely to be rejected using our decision making system (34% of the time), as opposed to high income people who are only rejected 0.5% of the time with our system.\n\n\n\nWrite and Reflect"
  },
  {
    "objectID": "posts/palmer-penguins/index.html",
    "href": "posts/palmer-penguins/index.html",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "Abstract\nIn this blog post we are going to train a random forest model to classify penguin species based on three features in the Palmer Penguins dataset. We’ll start with some visualizations to get an understanding of the different features in the dataset, and then we will use a random forest to choose the best three features to train our model on. After training and testing the model, we will explore how well the model did and how it made its classifications using the confusion matrix and plotting decision regions.\n\n\nPreparing the Data\n\n# Read in the data\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\n\nle = LabelEncoder()\ntrain[\"Species\"].replace('Adelie Penguin (Pygoscelis adeliae)','Adelie',inplace=True)\ntrain[\"Species\"].replace('Chinstrap penguin (Pygoscelis antarctica)','Chinstrap',inplace=True)\ntrain[\"Species\"].replace('Gentoo penguin (Pygoscelis papua)','Gentoo',inplace=True)\nle.fit(train[\"Species\"])\n\n# Function to prepare the data \ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1) # Remove unnecessary columns\n  df = df[df[\"Sex\"] != \".\"]                                                                                 \n  df = df.dropna() # Drop NAs                                                                                          \n  y = le.transform(df[\"Species\"]) # Encode the species column                                                                      \n  df = df.drop([\"Species\"], axis = 1)                                                                       \n  df = pd.get_dummies(df) # One-hot encode the boolean columns                                                                \n  return df, y\n\nX_train, y_train = prepare_data(train)\n\nIn the above code, we processed the dataset to prepare it for analysis as follows:\n\nRemove the unnecessary columns and NAs\nPrepare the qualitative columns for analysis by encoding them as quantitative columns\n\nEncode the species column\n\nEach species is assigned a number\n\n“One-hot encode” the boolean columns\n\nThe get_dummies function converts these columns to 0-1 instead of True/False\n\n\n\n\n\narray([1, 1, 2, 2, 1, 0, 0, 1, 2, 1, 0, 1, 0, 1, 1, 2, 0, 2, 2, 2, 2, 0,\n       0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 2, 2, 1, 1, 2, 2, 1, 0,\n       0, 2, 2, 1, 2, 2, 1, 2, 0, 0, 2, 2, 0, 1, 2, 2, 1, 2, 1, 2, 2, 2,\n       0, 0, 0, 2, 2, 2, 0, 1, 2, 2, 2, 0, 0, 2, 0, 0, 2, 0, 0, 0, 1, 0,\n       0, 1, 0, 0, 0, 1, 0, 0, 2, 2, 0, 0, 2, 0, 2, 1, 0, 2, 2, 1, 2, 2,\n       2, 0, 2, 0, 0, 0, 1, 0, 2, 2, 0, 2, 2, 1, 2, 0, 0, 1, 2, 2, 1, 0,\n       2, 0, 1, 2, 0, 0, 2, 0, 2, 1, 0, 0, 2, 1, 0, 2, 0, 2, 0, 1, 0, 0,\n       0, 2, 2, 2, 0, 0, 2, 0, 2, 1, 1, 0, 1, 2, 0, 0, 1, 1, 0, 0, 1, 0,\n       0, 2, 1, 2, 1, 0, 0, 0, 1, 0, 2, 2, 2, 2, 1, 1, 1, 2, 2, 2, 0, 1,\n       0, 0, 1, 0, 0, 0, 0, 0, 1, 2, 2, 2, 0, 2, 2, 1, 0, 2, 0, 0, 2, 0,\n       2, 0, 2, 0, 2, 2, 2, 2, 0, 2, 1, 0, 2, 1, 1, 0, 2, 1, 0, 0, 0, 1,\n       0, 1, 0, 0, 2, 1, 0, 0, 0, 2, 0, 0, 0, 1])\n\n\n\n\nVisualizing the data\nNow, we need to figure out which features best distinguish each species so that we can train a model to predict the species based on the features. We’ll start by looking at visualizations of some of the features in the dataset in order to get an idea of which ones will work well for classification.\n\nsns.set_theme()\n# Plot the culmen depth and flipper length \nsns.relplot(\n    data=train,\n    x=\"Flipper Length (mm)\", y=\"Culmen Depth (mm)\",\n    hue=\"Species\", style=\"Species\"\n).set(title=\"Penguin Species by Culmen Depth and Flipper Length\")\n\n\n\n\nFigure 1: culmen length and flipper depth of each penguin species\n\n\n\n\n\n# Plot the distribution of each species on each of the 3 islands \nax = sns.countplot(data=train, x=\"Island\", hue=\"Species\")\nax.set(xlabel=\"Island\", ylabel=\"Number of Penguins\", title=\"Species Distribution Across Islands\")\nax.set_xticklabels(ax.get_xticklabels(), rotation=30)\n\n\n\n\nFigure 2: species distribution across each island\n\n\n\n\n\n\n\n\n\n\n\n\n\nCulmen Length (mm)\n\n\nSpecies\n\n\n\n\n\nAdelie\n38.970588\n\n\nChinstrap\n48.826316\n\n\nGentoo\n47.073196\n\n\n\n\n\nFigure 3: mean culmen length by species\n\n\nOur goal is to find three features that will allow us to train a model which will be 100% accurate in predicting the species of a penguin in the training set. We can ascertain some information about the top scored features from these plots that will be helpful to determine if these features will work.\nFirst, as we can see in Figure 1, the Gentoo penguin has different culmen and flipper measurements than the Chinstrap and Adelie penguins. Specifically, the Gentoo penguin seems to have a larger flipper length and a shallower culmen depth than the Chinstrap and Adelie penguins. This relationship could be used to identify the Gentoo penguins, but we still need to find a way to distinguish between the Chinstrap and Adelie penguins.\nFigure 2 provides a way to do this; the Chinstrap penguins are only found on Dream Island. We can also see that the Adelie penguins are found on all three islands, and the Gentoo penguin is only found on Biscoe Island. In the summary table, we can see that there is also a difference in culmen length between the species. The Chinstrap and Gentoo penguins have similar average culmen lengths, but the Adelie penguins have much shorter culmens on average.\nUsing some combination of these features, we should be able to accurately predict the species of any penguin in the training set. We will use tree-based feautre selection to determine which of the features we will use to train our model.\n\n\nOptimizing model parameters\nWe will be using a random forest to choose the best features to train our model on. We’ll also use a random forest to execute our classification. First, let’s understand what a random forest is:\n\nRandom Forest\n\nRandom forests are an ensemble learning method. That is, they combine predictions from multiple models to make a final prediction. Random forests use multiple decision trees to make a prediction. Each tree is trained on a different subset of the training set, so different trees can learn different patterns in the data. The trees use a process of “voting” to determine the prediction–whichever species is predicted by the most trees is the final prediction.\n\n\n\nChoosing the best features\n\n# Fit a model to find the best features in the training set\ntree_selector = ExtraTreesClassifier(n_estimators=100, random_state=42)\ntree_selector.fit(X_train, y_train)\nfeature_importances = tree_selector.feature_importances_\n\n# Create DataFrame with the feature and its importance score\nfeature_scores = pd.DataFrame({\"Feature\": X_train.columns, \"Importance\": feature_importances})\nfeature_scores = feature_scores.sort_values(by=\"Importance\", ascending=False)\n\n\n\nTop 3 Features:\n               Feature  Importance\n0   Culmen Length (mm)    0.166972\n2  Flipper Length (mm)    0.139796\n6        Island_Biscoe    0.132696\n\n\nWhen we use the ExtraTreesClassifier function, we are building a random forest from the training set. A random subset of features is used for each tree, then the best features are selected based on an importance score. The importance score is calculated using an impurity score for each feature that captures how well a feature is able to partition the dataset. These scores are then normalized to create the importance score. We can see from the feature selector that the culmen length, flipper length, and the island are the most important features in the dataset. So, we will use these features to train our model.\n\n\nChoosing maximum tree depth\nOne of the parameters for any tree method is the maximum depth at which the tree can go. That is, the number of partitions of the dataset allowed in each tree. In order to find the optimal maximum depth, we will use cross validation to test a range of potential maximum depths:\n\n# The columns that we want to include in our analysis -- based on the highest-scoring features\ncols = [\"Flipper Length (mm)\", \"Culmen Length (mm)\", \"Island_Biscoe\", \"Island_Dream\",\"Island_Torgersen\"]\n\ndepths = range(1, 21)\nmean_scores = []\n\n# Loop through potential depths and use cross validation to score each depth\nfor depth in depths:\n    dt = RandomForestClassifier(max_depth=depth, random_state=42)\n    scores = cross_val_score(dt, X_train[cols], y_train, cv=5, scoring='accuracy')\n    mean_scores.append(np.mean(scores))\n\n# Find the depth with the highest scoring validation\nbest_depth = depths[np.argmax(mean_scores)]\nbest_score = max(mean_scores)\n\n\n\nBest max tree depth: 7\nBest cross-validated accuracy: 0.9843891402714933\n\n\nBased on the cross-validation, a random forest with maximum depth 7 is the best option for our model.\n\n\n\nFitting the model\nNow that we know which features we want to use and have done some work to optimize the parameters for our random forest, we’re ready to train a random forest on our training set.\n\n# Initialize the model and fit it to the train set\ndt_model = RandomForestClassifier(max_depth=5, random_state=42)\ndt_model.fit(X_train[cols], y_train)\n\n# Score the model accuracy to see how well it did \ntrain_accuracy = dt_model.score(X_train[cols], y_train)\n\n\n\nTraining Accuracy: 0.9961\n\n\nThis random forest works pretty well! It was 99% accurate on our testing set. Now that we’ve trained this random forest tree, we can test it on the test dataset.\n\n# Read in the test data\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\ntest[\"Species\"].replace('Adelie Penguin (Pygoscelis adeliae)','Adelie',inplace=True)\ntest[\"Species\"].replace('Chinstrap penguin (Pygoscelis antarctica)','Chinstrap',inplace=True)\ntest[\"Species\"].replace('Gentoo penguin (Pygoscelis papua)','Gentoo',inplace=True)\n\nX_test, y_test = prepare_data(test)\ntest_acc = dt_model.score(X_test[cols], y_test)\n\n\n\nTesting Accuracy: 0.9853\n\n\n\n\nUnderstanding the results\nLet’s get a better understanding of what it’s actually doing. First, we can use the confusion matrix to look at how all of the penguins were classified.\n\ny_test_pred = dt_model.predict(X_test[cols])\nconfusion_matrix(y_test, y_test_pred)\n\narray([[31,  0,  0],\n       [ 0, 11,  0],\n       [ 1,  0, 25]])\n\n\nWhat does this matrix mean? We can understand it using each species as follows:\n\n\nThere were 31 Adelie penguin(s) who were classified as Adelie penguin(s).\nThere were 0 Adelie penguin(s) who were classified as Chinstrap penguin(s).\nThere were 0 Adelie penguin(s) who were classified as Gentoo penguin(s).\nThere were 0 Chinstrap penguin(s) who were classified as Adelie penguin(s).\nThere were 11 Chinstrap penguin(s) who were classified as Chinstrap penguin(s).\nThere were 0 Chinstrap penguin(s) who were classified as Gentoo penguin(s).\nThere were 1 Gentoo penguin(s) who were classified as Adelie penguin(s).\nThere were 0 Gentoo penguin(s) who were classified as Chinstrap penguin(s).\nThere were 25 Gentoo penguin(s) who were classified as Gentoo penguin(s).\n\n\nThis tells us that our model only misclassified one penguin – a Gentoo classified as an Adelie. We can plot the decision regions for each island to get an understanding of why this happened.\n\n\nPlotting Code\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Gentoo\", \"Chinstrap\", \"Adelie\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\n\n\n\n\n\n\n\n\n\n\nThese plots show us how our model made its classifications. On the far left, we can see that for the Biscoe Island data, the model was able to correctly distinguish between the Adelie (blue) and Gentoo (red) penguins using the flipper length. Moving to the far right, on Torgensen Island, there are only Adelie penguins, so the model was able to classify all of them correctly. In the middle plot we can see that the penguins on Dream Island were harder to classify – we can see where the model classified a Chinstrap (green) penguin as an Adelie penguin (blue) because it had a shorter culmen than the other Chinstrap penguins, causing its point to be within the blue region.\n\n\nDiscussion\nWe can see that our random forest was a very accurate model. It only misclassified one of the penguins, giving us 98.5% testing accuracy. We can see from the decision regions that the random forest was able to learn patterns within each island fairly well, even creating some accurate non-linear decision regions. In order to get more accuracy for the Dream island data, we would probably need to use another classification method that could learn the differences between the Chinstrap and Adelie penguins on Dream island. Or, going back to our initial visualizations, we could use an ensemble learning method that first uses one model to separate one species from the other two using the quantitative measurements and another that uses a qualitative feature like the island to distinguish between penguins with similar physical measurements."
  },
  {
    "objectID": "posts/automated-decision-systems/index.html",
    "href": "posts/automated-decision-systems/index.html",
    "title": "Automated Decison Systems",
    "section": "",
    "text": "Abstract\nIn this blog post we’ll be looking at data from a bank. We’ll start with some data exploration and visualization to get a better understanding of the features in the dataset. Our ultimate goal here is to create a decision-making system that will determine whether a prospective borrower should be offered a loan. We will use logistic regression to assign weights to different features in the dataset, which we will then use to give each prospective borrower a “score”. This score will be used to determine whether the borrower will prospective receive a loan based on a threshold that will optimize profits for the bank. After creating this decision-making system, we’ll test it out on a test dataset and do some exploration into who our system predicted would be able to repay their loans, and who it predicted wouldn’t.\n\n# Read in the data\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/train.csv\"\ndf_train = pd.read_csv(url)\n\n\n# echo : false\ndf_train.head()\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_home_ownership\nperson_emp_length\nloan_intent\nloan_grade\nloan_amnt\nloan_int_rate\nloan_status\nloan_percent_income\ncb_person_default_on_file\ncb_person_cred_hist_length\n\n\n\n\n0\n25\n43200\nRENT\nNaN\nVENTURE\nB\n1200\n9.91\n0\n0.03\nN\n4\n\n\n1\n27\n98000\nRENT\n3.0\nEDUCATION\nC\n11750\n13.47\n0\n0.12\nY\n6\n\n\n2\n22\n36996\nRENT\n5.0\nEDUCATION\nA\n10000\n7.51\n0\n0.27\nN\n4\n\n\n3\n24\n26000\nRENT\n2.0\nMEDICAL\nC\n1325\n12.87\n1\n0.05\nN\n4\n\n\n4\n29\n53004\nMORTGAGE\n2.0\nHOMEIMPROVEMENT\nA\n15000\n9.63\n0\n0.28\nN\n10\n\n\n\n\n\n\n\n\n\nExploring The Data\nLet’s start with some visualizations to explore patterns in this dataset:\n\n\nPlotting Code\n# Filter out rows with employment length greater than 25 years\ndf_train = df_train[df_train['person_emp_length'] &lt;= 25]\n\n# Bucket the employment length into 5-year chunks\nbins = range(0, 25 + 5, 5)\nlabels = [\"0 to 5 years\", \"5 to 10 years\", \"10 to 15 years\", \"15 to 20 years\", \"20 to 25 years\"]\n\ndf_train['emp_length_bucket'] = pd.cut(\n    df_train['person_emp_length'],\n    bins=bins,\n    right=True,\n    labels=labels,\n    include_lowest=True\n)\n\n# Create a Seaborn plot using histplot with multiple=\"fill\" for normalized (proportional) stacked bars\nplt.figure(figsize=(10, 6))\nsns.histplot(\n    data=df_train,\n    x='emp_length_bucket',\n    hue='loan_intent',\n    multiple='fill',  # normalizes each bar to sum to 1 (proportions)\n    shrink=0.8, \n    palette=\"Set2\"\n)\nplt.title(\"Loan Intent Proportions by Employment Length\")\nplt.xlabel(\"Employment Length\")\nplt.ylabel(\"Proportion\")\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nFigure 1: Loan intent by length of employment\n\n\n\n\n\n\nPlotting Code\n# Plot the distribution of interest rates of each of the home ownership statuses \nsns.boxplot(data=df_train, x=\"person_home_ownership\", y=\"loan_int_rate\", palette=\"Set2\")\nplt.xlabel(\"Home Ownership Status\")\nplt.ylabel(\"Loan Interest Rate\")\n\n\nText(0, 0.5, 'Loan Interest Rate')\nFigure 2: Interest rate distribution by home ownership status\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncredit_history_length\n\n\nloan_grade\n\n\n\n\n\nA\n5.7\n\n\nB\n5.8\n\n\nC\n5.9\n\n\nD\n5.8\n\n\nE\n5.8\n\n\nF\n6.1\n\n\nG\n6.6\n\n\n\n\n\nFigure 3: mean credit history length by loan grade\n\n\nWe’re looking here to understand some of the patterns in this dataset.\nFirst, in Figure 1, we can see that borrowers’ intentions for their loans change a bit as the length of their employment increases. Specifically, after 20 years of employment people stop borrowing to pay for education, which makes sense. People also start taking out more loans for venture and medical uses as their employment goes on. It looks like the proportion of loans that are intended to be used for home improvement, personal reasons, and debt consolidation remain fairly constant across length of employment.\nMoving on to Figure 2, we can see that borrowers with different home ownership statuses (renting, owning, mortgaging or other), have differences in the interest rate they might receive on their loan. Borrowers who rent their home seem more likely to have an interest rate of over 20%, and we can see that borrowers who own their homes rarely see interests rates over 20%. Borrowers who rent and mortgage their homes have more variability in the interest rates they might see, as well.\nFinally, we can see in Figure 3 that loan grades have differences in average length of credit of the borrower. It seems that borrowers with higher loan grades have longer credit histories, on average.\nWe can see that we should be able to get some information about whether a borrower will pay back a loan from some of these features.\n\nPrepping the Data\n\n# Encode the qualitative variables quantitatively\ndef preprocess(df):\n    \"\"\"\n    Function to preprocess the data\n    \"\"\"\n    le_home = LabelEncoder()\n    df[\"person_home_ownership\"] = le_home.fit_transform(df[\"person_home_ownership\"])\n\n    le_intent = LabelEncoder()\n    df[\"loan_intent\"] = le_intent.fit_transform(df[\"loan_intent\"])\n    intent_mapping = dict(zip(le_intent.transform(le_intent.classes_), le_intent.classes_))\n\n    df = pd.get_dummies(df, columns=['cb_person_default_on_file'], prefix='default')\n    df = df.dropna()\n\n    # Change the interest rate column to a percentage\n    df[\"loan_int_rate\"] = df[\"loan_int_rate\"]/100\n\n    #Filter out columns with weird values\n    df = df[(df[\"person_age\"] &lt; 100) & (df[\"person_income\"] &gt; 1000) &  (df[\"loan_int_rate\"] &gt; 0)]\n\n    # Separate our the target variable\n    y = df[\"loan_status\"]\n\n    return df, y, intent_mapping\n\ndf_train_clean, y_train, intent_map = preprocess(df_train)\n\nWe will use this function prepare our datasets for analysis. We’re encoding the qualitative columns for home ownership and loan intent as integers, one-hot encoding the historical default true/false column as 0s and 1s, separating the target variable\n\n\n\nBuilding A Model\n\nChoosing Features\nFirst, let’s figure out which features we want to use for the model. We’ll generate a few new features that we can use to better understand the relationships between our features: - Credit history ratio: \\(\\frac{\\text{credit history length}}{\\text{age}}\\) - Employment stability: \\(\\frac{\\text{employment length}}{\\text{age}}\\) - Income stability: \\(\\text{income}*\\text{employment length}\\)\nWe’ll use cross validation to test a few combinations of features: 1. Employment stability, income, and percentage of income that the loan is 2. Loan intent, credit history ratio, home ownership status 3. income stability, loan interest rate, age\n\n# Create new features \ndef new_features(df):\n    df[\"credit_history_ratio\"] = round(df[\"cb_person_cred_hist_length\"]/df[\"person_age\"],2)\n    df[\"employment_stability\"] = round(df[\"person_emp_length\"]/df[\"person_age\"], 2)\n    df[\"income_stability\"] = round(df[\"person_income\"]*df[\"person_emp_length\"],2)\n    return df\ndf_train_clean = new_features(df_train_clean)\n\n\n# Define three different sets of features\nfeature_sets = {\n    \"Set 1\": [\"employment_stability\", \"person_income\", \"loan_percent_income\"],\n    \"Set 2\": [\"loan_intent\", \"credit_history_ratio\", \"person_home_ownership\"],\n    \"Set 3\": [\"income_stability\", \"loan_int_rate\", \"person_age\"]\n}\n\nmodel = LogisticRegression()\n\n# Evaluate each feature set\nresults = {}\nfor set_name, features in feature_sets.items():\n    X_train = df_train_clean[features]\n    X_train = StandardScaler().fit_transform(X_train)  # Standardize features \n    \n    scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')\n    mean_accuracy = np.mean(scores)\n    results[set_name] = mean_accuracy\n    print(f\"{set_name}: Mean Accuracy = {mean_accuracy:.4f}\")\n\nSet 1: Mean Accuracy = 0.8246\nSet 2: Mean Accuracy = 0.7850\nSet 3: Mean Accuracy = 0.8095\n\n\n\n\nEstimating weights\nAfter fitting an LR to each of the feature sets and using cross validation to test how accurate the models are, we can see that our first set of features, which includes employment stability, income, and percentage of income that the loan is has the highest average accuracy. Now, we’ll fit an LR model to these features to obtain our vector of weights that we’ll use in our scoring function.\n\nweights_model = LogisticRegression()\nX_train = df_train_clean[feature_sets[\"Set 1\"]]\nX_train = StandardScaler().fit_transform(X_train)\nweights_model.fit(X_train, y_train)\n\nfor feature, coef in zip(feature_sets[\"Set 1\"], weights_model.coef_[0]):\n    print(f\"{feature}: {coef:.4f}\")\n\nemployment_stability: -0.1482\nperson_income: -0.4953\nloan_percent_income: 0.7925\n\n\nNow we can use these coefficients in our as the values in our weight vector when we create a scoring function, which we’ll do next.\n\n\n\nCreating the score function\nOur next step is to give each borrower a score, using the features and weights that we decided on above. We’ll add a “score” column to the train set that has the result of our score function:\n\\(\\text{score} = -0.1482 * x_1 - 0.4953 * x_2 + 0.7925 * x_3\\)\nwhere\n\\(x_1\\) = employment_stability\n\\(x_2\\) = person_income\n\\(x_3\\) = loan_percent_income\n\ndef score_dataframe(df):\n    \"\"\"\n    Compute the score for each borrower using the weights \n    determined by the logistic regression\n\n    inputs:\n    df: Pandas DataFrame with all features used to generate weights\n    and loan status, amount, and interest rate for profit calculations\n    \"\"\"\n    df_scored = df.copy()\n    df_scored[[\"employment_stability\", \"person_income\", \"loan_percent_income\"]] = StandardScaler().fit_transform(df_scored[[\"employment_stability\", \"person_income\", \"loan_percent_income\"]])\n    df_scored[\"score\"] =  round(-0.1486 * df_scored[\"employment_stability\"] - 0.6252 * df_scored[\"person_income\"] + 0.7925 * df_scored[\"loan_percent_income\"],2)\n    return df_scored\n\n\ndef compute_profit(threshold, df):\n    \"\"\"\n    Compute the profit that the bank would have made if they had\n    used the predictions determined by our score function\n    and threshold\n\n    inputs:\n    threshold: integer value for score threshold \n    df: Pandas DataFrame with loan amount, interest rate, actual loan status, and score from \n    scoring function\n    \"\"\"\n    df[\"pred_default\"] = df[\"score\"].apply(lambda x: 1 if x &gt; threshold else 0)\n    \n    # Calculate potential profit for repayment or defaulting \n    df[\"repaid_profit\"] = round(df[\"loan_amnt\"] * (1 + 0.25 * df[\"loan_int_rate\"])**10 - df[\"loan_amnt\"],2)\n    df[\"defaulted_profit\"] = round(df[\"loan_amnt\"] * (1 + 0.25 * df[\"loan_int_rate\"])**3 - 1.7 * df[\"loan_amnt\"],2)\n    \n    # Profit based on actual loan status versus predicted status\n    df[\"profit\"] = 0  # Default to zero profit\n    df.loc[(df[\"loan_status\"] == 0) & (df[\"pred_default\"] == 0), \"profit\"] = df[\"repaid_profit\"]  # Successful repayment\n    df.loc[(df[\"loan_status\"] == 1) & (df[\"pred_default\"] == 0), \"profit\"] = df[\"defaulted_profit\"]  # Incorrectly predicted as good loan, default\n    df.loc[(df[\"loan_status\"] == 1) & (df[\"pred_default\"] == 1), \"profit\"] = 0  # Correctly predicted as default, no profit\n    df.loc[(df[\"loan_status\"] == 0) & (df[\"pred_default\"] == 1), \"profit\"] = 0  # Incorrectly predicted as default, no profit\n\n    return sum(df[\"profit\"]) / len(df)\n\n\n# Find optimal threshold\nscored_df = score_dataframe(df_train_clean)\nthresholds = np.arange(0,3,0.05)\nprofits = [compute_profit(t, scored_df) for t in thresholds]\n\n# Find best threshold\noptimal_threshold = thresholds[np.argmax(profits)]\noptimal_profit = max(profits)\nprint(f\"Optimal Threshold for Maximum Profit: {optimal_threshold:.4f}\")\nprint(f\"Maximum profit per prospective borrower: {round(optimal_profit,2):.4f}\")\n\n\n# Plot profit vs. threshold\nsns.set_style(\"whitegrid\")\nsns.set_palette(\"Set2\")\n\nplt.figure(figsize=(8, 5))\nplt.scatter(optimal_threshold, optimal_profit, color=sns.color_palette(\"Set2\")[1], label=f'Optimal t={optimal_threshold:.4f}', zorder=2)\nsns.lineplot(x=thresholds, y=profits, label=\"Profit\", zorder=1)\nplt.xlabel(\"Threshold\")\nplt.ylabel(\"Profit per borrower\")\nplt.title(\"Threshold vs Profit\")\nplt.legend()\nplt.show()\n\nOptimal Threshold for Maximum Profit: 1.1500\nMaximum profit per prospective borrower: 1316.4800\n\n\n\n\n\n\n\n\n\n\n\nEvaluating from the bank’s perspective\nNow, we want to look at how this decision making system works on a test dataset. We’ll apply all of the functions we used on the training set with the test set, and see what the profit per borrower will look like using the threshold from above, 1.15.\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/test.csv\"\ndf_test = pd.read_csv(url)\ndf_test, y_test, intent_map = preprocess(df_test)\ndf_test = new_features(df_test)\n    \nscored_test_df = score_dataframe(df_test)\nprint(\"profit per prospective borrower:\", round(compute_profit(1.15, scored_test_df),2))\n\nprofit per prospective borrower: 1230.42\n\n\nIt looks like we ended up with a fairly similar profit per borrower on the test set to what we got from the train set, $1316 per borrower on the test set and $1230 on the train set. From the bank’s perspective, this seems positive. If they are making, on average, $1230 per borrower, they would make around $6,000,000 off of this dataset of borrowers.\n\n\nEvaluating from the borrower’s perspective\nNow, let’s try to get a better understanding of who in the test dataset we are predicting is going to default on their loans, and who we are predicting is going to repay them. We’ll look at the following questions:\n\nIs it more difficult for people in certain age groups to access credit under your proposed system?\nIs it more difficult for people to get loans in order to pay for medical expenses? How does this compare with the actual rate of default in that 3. group? What about people seeking loans for business ventures or education?\nHow does a person’s income level impact the ease with which they can access credit under your decision system?\n\n\nQuestion 1\n\n# Split the borrower ages into 4 buckets and see \nscored_test_df[\"age_group\"] = pd.qcut(scored_test_df[\"person_age\"], q=4, labels=[\"Young\", \"Middle-aged\", \"Older\", \"Senior\"])\nage_access = scored_test_df.groupby(\"age_group\")[\"pred_default\"].mean()\nage_access\n\nage_group\nYoung          0.171756\nMiddle-aged    0.121554\nOlder          0.142263\nSenior         0.134460\nName: pred_default, dtype: float64\n\n\nThese probabilities are the mean of the prediction column, so they tell us the rate at which a 1 is predicted in each of the age buckets using our decision making system. That is, they tell us the loan rejection rate for each age group. We can see that younger people are mich more likely to be rejected for a loan than middle-aged and older people are, with a 17% rejection rate as opposed to a 12-14% rejection rate for older groups.\n\n\nQuestion 2\n\n# Calculate rejection rates and actual default rates for loan intent\nloan_intent_access = scored_test_df.groupby(\"loan_intent\")[\"pred_default\"].mean()\nloan_intent_default_rates = scored_test_df.groupby(\"loan_intent\")[\"loan_status\"].mean()\n\nloan_intent_df = pd.DataFrame({\n    \"Loan Intent (Decoded)\": loan_intent_access.index.map(intent_map),\n    \"Rejection Rate\": loan_intent_access.values,\n    \"Actual Default Rate\": loan_intent_default_rates.values\n})\n\n\n# echo: false\nloan_intent_df\n\n\n\n\n\n\n\n\nLoan Intent (Decoded)\nRejection Rate\nActual Default Rate\n\n\n\n\n0\nDEBTCONSOLIDATION\n0.150442\n0.287611\n\n\n1\nEDUCATION\n0.150510\n0.167517\n\n\n2\nHOMEIMPROVEMENT\n0.095779\n0.250000\n\n\n3\nMEDICAL\n0.164958\n0.284250\n\n\n4\nPERSONAL\n0.146293\n0.220441\n\n\n5\nVENTURE\n0.127593\n0.146266\n\n\n\n\n\n\n\nWe’re interested in the loans that are intended for medical uses. We can see that medical loans have the highest rate of rejection from our system (20% of medical loan requests are rejected), but they also have one of the highest rates of actual default (28%). Venture and business loans have lower rates of rejection and their actual default rates are closer to their rejection rates.\n\n\nQuestion 3\n\nscored_test_df[\"income_group\"] = pd.qcut(scored_test_df[\"person_income\"], q=4, labels=[\"Low\", \"Lower-middle\", \"Upper-middle\", \"High\"])\nincome_access = scored_test_df.groupby(\"income_group\")[\"pred_default\"].mean()\n\n::: {#cell-45 .cell 0=‘e’ 1=‘c’ 2=‘h’ 3=‘o’ 4=’ ’ 5=‘:’ 6=‘f’ 7=‘a’ 8=‘l’ 9=‘s’ 10=‘e’ execution_count=287}\nincome_access\n\nincome_group\nLow             0.342638\nLower-middle    0.168044\nUpper-middle    0.052274\nHigh            0.004370\nName: pred_default, dtype: float64\n\n:::\nHere, we are again looking at the loan rejection rates, this time by income group. It is clear that people with lower incomes are much more likely to be rejected using our decision making system (41% of the time), as opposed to high income people who are only rejected 0.8% of the time with our system.\n\n\n\nWrite and Reflect\nWe found from our model and evaluation that when we choose a threshold that optimizes the bank’s profit, it can result in certain groups having a harder time obtaining a loan. We saw that age, income, and loan intent groups have differences rejection rates all with our decision making system. We can also see that optimizing profits for a bank requires turning a way a significant chunk of loan applications, because the reality is that many people do end up defaulting on their loans. In our decision-making process, we only considered 3 features of a person’s application, but we found that these features correlated with certain groups having a harder time receiving a loan. This is a great example of a feature that is implicitly encoding information about another category. For example, a person’s income could be correlated with what they are planning to use the loan for. Even though loan intent wasn’t one of the features of our score function, we could see that different loan intents ended up with different rates of rejection in our system. So, is this system fair? We’ll explore that by answering this question: Considering that people seeking loans for medical expense have high rates of default, is it fair that it is more difficult for them to obtain access to credit?\nFirst, let’s define fairness. I will define fairness in deciding who should get a loan of a combination of how much the person needs the loan and how likely it is that they will pay the loan back. If we are only basing fairness on how much the person needs the loan, we would likely say that people who need loans for medical reasons should all receive their loans. But, is this fair to the bank? They would end up losing a lot of money because people who take out loans for medical expenses often default on them. If we are only basing fairness on how likely it is that they will pay the loan back, most people seeking medical loans probably shouldn’t get them, because they do often default. But, in this case, very few people would receive medical loans, which seems unfair because they may be able to pay them back, even if people haven’t historically paid medical loans back at a high rate. There are also some other confounding variables to consider, like whether people seeking medical loans are more likely to come from a historically marginalized group that might have a harder time proving they can pay a loan back or a smaller income on average than other groups? It may not be fair for these people to be subject to high rejection rates just because of their identity.\nUltimately, Looking at the rate of rejection versus the true rate of default for medical loans, I think it’s fair that it may be more difficult for people to access these loans. The rate of default on these loans is much higher than the rate of default for loans with other intended purposes, so it seems that people who are seeking these loans may need to provide additional proof that they would be able to pay the loan back. It’s fair for the bank to be skeptical. However, we can see that in our system, 80% of people seeking medical loans are being accepted. So it isn’t the case that it’s impossible for these people, who probably really need the loan, to obtain it. Therefore, I think it’s fair that it is more difficult for people seeking medical loans to obtain access to credit given that they default much more frequently."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Automated Decison Systems\n\n\n\n\n\nDesign and Impact of Automated Decision Systems\n\n\n\n\n\nFeb 27, 2025\n\n\nCarly McAdam\n\n\n\n\n\n\n\n\n\n\n\n\nAuditing Bias\n\n\n\n\n\nAuditing for gender bias in income predictions\n\n\n\n\n\nFeb 13, 2025\n\n\nCarly McAdam\n\n\n\n\n\n\n\n\n\n\n\n\nClassifying Palmer Penguins\n\n\n\n\n\nClassifying Palmer Penguins\n\n\n\n\n\nFeb 13, 2025\n\n\nCarly McAdam\n\n\n\n\n\n\n\n\n\n\n\n\nSecond Post\n\n\n\n\n\nA new blog post that I just made!\n\n\n\n\n\nMar 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\n\n\n\n\n\n\nTimnit Gebru\n\n\n\n\n\nA new blog post that I just made!\n\n\n\n\n\nMar 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\n\n\n\n\n\n\nHello Blog\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  }
]