[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is Carly’s CSCI 451 blog"
  },
  {
    "objectID": "posts/new-new-test-post/index.html",
    "href": "posts/new-new-test-post/index.html",
    "title": "Timnit Gebru",
    "section": "",
    "text": "from source import Perceptron\np = Perceptron()\n\nI did it!!\nnot implemented\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-new-test-post/index.html#math",
    "href": "posts/new-new-test-post/index.html#math",
    "title": "Timnit Gebru",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/new-test-post/index.html",
    "href": "posts/new-test-post/index.html",
    "title": "Second Post",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-test-post/index.html#math",
    "href": "posts/new-test-post/index.html#math",
    "title": "Second Post",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/implementing-logistic-regression/index.html",
    "href": "posts/implementing-logistic-regression/index.html",
    "title": "Implementing Logistic Regression",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\nimport torch\nfrom logistic import LogisticRegression, GradientDescentOptimizer\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n\nAbstract\nAbstract\n\n\nExperiments\n\nGenerating Experimental Data\n\ndef classification_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    y = 1.0*y\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    \n    return X, y\n\nX, y = classification_data(noise = 0.5)\n\n\n\nExperiments\n\nVanilla Gradient Descent\nWe’ll start with our most simple experiment. We want to look at a case with a small value for \\(\\alpha\\) and with \\(\\beta = 0\\) and show that this method of descending down the gradient to minimize the loss function will converge to a ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍weight ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍vector ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍w that looks visually correct. That is, when we plot the decision boundary we should see that this algorithm is able to split the data fairly well. We also want to show that as we do more iterations, the loss should be decreasing monotonically, which means that it is only decreasing and never increases as we do more iterations. We’ll plot both the decision boundary and the loss function after implementing the gradient descent method to check that these criteria have been met. First, we will generate some data using the above function and train it using a gradient descent loop.\n\nimport matplotlib.pyplot as plt\n\n# generate classification data\nX, y = classification_data(n_points = 500, noise=0.5, p_dims=2)\n\n# create model and optimizer\nmodel = LogisticRegression()\noptimizer = GradientDescentOptimizer(model)\n\n# track loss over time\nlosses_vanilla = []\n\n# run vanilla gradient descent loop (train the model)\nfor _ in range(100):\n    loss_vanilla = model.loss(X, y).item()\n    losses_vanilla.append(loss_vanilla)\n    optimizer.step(X, y, alpha=0.1, beta=0.0)\n\n::: {#cell-11 .cell 0=‘c’ 1=‘o’ 2=‘d’ 3=‘e’ 4=‘-’ 5=‘f’ 6=‘o’ 7=‘l’ 8=‘d’ 9=‘:’ 10=‘t’ 11=‘r’ 12=‘u’ 13=‘e’ execution_count=19}\n# plot the loss over iterations\nplt.plot(losses_vanilla)\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Logistic Loss\")\nplt.title(\"Loss Over Time (Vanilla Gradient Descent)\")\nplt.grid(True)\nplt.show()\n\n# plot decision boundary with data\ndef plot_decision_boundary(model, X, y):\n    x_min, x_max = X[:,0].min() - 0.5, X[:,0].max() + 0.5\n    y_min, y_max = X[:,1].min() - 0.5, X[:,1].max() + 0.5\n    xx, yy = torch.meshgrid(torch.linspace(x_min, x_max, 100),\n                            torch.linspace(y_min, y_max, 100),\n                            indexing='xy')\n    \n    # add bias column of ones\n    grid = torch.cat((xx.reshape(-1,1), yy.reshape(-1,1), torch.ones((xx.numel(), 1))), dim=1)\n    preds = model.predict(grid).reshape(xx.shape)\n\n    plt.contourf(xx, yy, preds, alpha=0.5, cmap=\"RdBu\")\n    plt.scatter(X[:,0], X[:,1], c=y, cmap=\"RdBu\", edgecolor='k')\n    plt.title(\"Decision Boundary (Logistic Regression)\")\n    plt.show()\n\nplot_decision_boundary(model, X, y)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n:::\nLooking at these plots, we can see that both of the criteria we described above are met by the vanilla gradient descent model. The loss function is only decreasing as we do more iterations of the gradient descent process, and we can see that the decision boundary generated by the model was able to split the data fairly well, although not perfectly. This makes sense because our dataset has noise and we can see from the plot that it would be impossible to separate this data using a linear decision boundary.\n\n\nBenefits ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍of ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍momentum\nNow, we’ll take a look at what this process looks like on the same data with a different \\(\\beta\\) (momentum) value. Gradient descent with momentum can converge to the correct weights in fewer iterations than it does without momentum (\\(\\beta = 0\\), as seen above with vanilla gradient descent).\n\n# run momentum gradient descent loop (train the model)\nlosses_momentum = []\nfor _ in range(100):\n    loss_momentum = model.loss(X, y).item()\n    losses_momentum.append(loss_momentum)\n    optimizer.step(X, y, alpha=0.1, beta=0.9)\n\n\n# plot the loss over iterations for momentum\nplt.plot(losses_vanilla, label=\"Vanilla GD (β = 0)\")\nplt.plot(losses_momentum, label=\"Momentum GD (β = 0.9)\")\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Logistic Loss\")\nplt.title(\"Loss vs Iteration: Vanilla vs Momentum\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nIn the plot we can see that when we add momentum to the gradient descent method (orange line), the loss is minimized more quickly than the vanilla gradient descent process (blue line). Thus, momentum can be a beneficial addition to the gradient descent process.\n\n\nOverfitting\nNow, we’ll take a look at an example of overfitting. We will generate two datasets, both with more dimensions (features) than the number points. Then, we’ll train a model on the train set that achieves 100% accurate for this train set and we will see how this model performs on the test set.\n\n# generate data\nX_train, y_train = classification_data(n_points = 50, noise=0.5, p_dims=100)\nX_test, y_test = classification_data(n_points = 50, noise=0.5, p_dims=100)\n\n\n# initialize model\nmodel = LogisticRegression()\nopt = GradientDescentOptimizer(model)\n\n# train model\nfor _ in range(300):\n    opt.step(X_train, y_train, alpha=0.1, beta=0.5)\n\n# evaluate model on train set\ntrain_preds = model.predict(X_train)\ntrain_acc = (train_preds == y_train).float().mean().item()\n\n::: {#cell-22 .cell 0=‘e’ 1=‘c’ 2=‘h’ 3=‘o’ 4=‘:’ 5=‘f’ 6=‘a’ 7=‘l’ 8=‘s’ 9=‘e’ execution_count=33}\nprint(f\"Train Accuracy: {train_acc:.2%}\")\n\nTrain Accuracy: 100.00%\n\n:::\nNow that we have a model that performs with 100% accuracy on the train set, let’s test it using the test set.\n\n# evaluate model on train set\ntest_preds = model.predict(X_test)\ntest_acc = (test_preds == y_test).float().mean().item()\n\n::: {#cell-25 .cell 0=‘e’ 1=‘c’ 2=‘h’ 3=‘o’ 4=‘:’ 5=‘f’ 6=‘a’ 7=‘l’ 8=‘s’ 9=‘e’ execution_count=35}\nprint(f\"Test Accuracy: {test_acc:.2%}\")\n\nTest Accuracy: 86.00%\n\n:::\nAs we can see from the lower testing accuracy than training accuracy, this is an example of overfitting. We created an overparameterized classification problem by making more features than points in the dataset. So, although the model was able to fit the training data perfectly, it was fitting a model that was too complex to generalize well. This is why we saw a much lower accuracy on the train set – the model learned patterns in the train data that didn’t actually apply to the data more generally.\n\n\nPerformance ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍on ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍empirical ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍data\nNow, we’ll test out this model on a real dataset. We’ll use a dataset called the Pima Indians Diabetes Dataset that originates from the National Institute of Diabetes and Digestive and Kidney Diseases and is available through the UCI Machine Learning Repository. ​The dataset comprises medical records for 768 female patients of Pima Indian heritage, aged 21 and above. Each record includes 8 numerical attributes related to medical measurements and personal information, along with a binary outcome indicating the presence or absence of diabetes. First, we’ll load and process the data so that we can use it with our custom logistic regression model. We need to normalize the features and add a bias column before train-test splitting.\n\n# load and preprocess the dataset\nurl = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\ncolumn_names = [\n    \"Pregnancies\", \"Glucose\", \"BloodPressure\", \"SkinThickness\", \"Insulin\",\n    \"BMI\", \"DiabetesPedigreeFunction\", \"Age\", \"Outcome\"\n]\ndf = pd.read_csv(url, names=column_names)\n\n\n\n\n\n\n\n\nPregnancies\nGlucose\nBloodPressure\nSkinThickness\nInsulin\nBMI\nDiabetesPedigreeFunction\nAge\nOutcome\n\n\n\n\n0\n6\n148\n72\n35\n0\n33.6\n0.627\n50\n1\n\n\n1\n1\n85\n66\n29\n0\n26.6\n0.351\n31\n0\n\n\n2\n8\n183\n64\n0\n0\n23.3\n0.672\n32\n1\n\n\n3\n1\n89\n66\n23\n94\n28.1\n0.167\n21\n0\n\n\n4\n0\n137\n40\n35\n168\n43.1\n2.288\n33\n1\n\n\n\n\n\n\n\n\n# normalize features\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\n\n\n# add bias column -- ensures compatibility with our custom model!\nX = torch.tensor(X, dtype=torch.float32)\nX = torch.cat([X, torch.ones((X.shape[0], 1))], dim=1)\ny = torch.tensor(y, dtype=torch.float32)\n\n/var/folders/23/gcqcls_x0glfvcgh87xqjsjr0000gn/T/ipykernel_90083/16192820.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  y = torch.tensor(y, dtype=torch.float32)\n\n\n\n# split off 20% of the data for the test set\nX_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# split remaining 80% into train (60%) and val (20%)\nX_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25, random_state=42)\n\nNow, we’ll train the model using the same method that we did above.\n\ndef train_model(X_train, y_train, X_val, y_val, alpha=0.1, beta=0.0, steps=100):\n    model = LogisticRegression()\n    opt = GradientDescentOptimizer(model)\n    train_losses = []\n    val_losses = []\n\n    for _ in range(steps):\n        # train model on the training set\n        train_losses.append(model.loss(X_train, y_train).item())\n        val_losses.append(model.loss(X_val, y_val).item())\n        opt.step(X_train, y_train, alpha=alpha, beta=beta)\n\n    return model, train_losses, val_losses\n\n# train model without momentum\nmodel_vanilla, losses_train_vanilla, losses_val_vanilla = train_model(X_train, y_train, X_val, y_val, beta=0.0)\n\n# train model with momentum\nmodel_momentum, losses_train_momentum, losses_val_momentum = train_model(X_train, y_train, X_val, y_val, beta=0.9)\n\nNow that we’ve trained the model two different ways (on both the train and validation sets) let’s take a the training ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍and ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍validation ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍loss ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍over the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍iterations, both with and without momentum.‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍\n\nimport matplotlib.pyplot as plt\n\nplt.plot(losses_train_vanilla, label=\"Train Loss (β=0.0)\", linestyle='-')\nplt.plot(losses_val_vanilla, label=\"Val Loss (β=0.0)\", linestyle='--')\nplt.plot(losses_train_momentum, label=\"Train Loss (β=0.9)\", linestyle='-')\nplt.plot(losses_val_momentum, label=\"Val Loss (β=0.9)\", linestyle='--')\n\nplt.xlabel(\"Iterations\")\nplt.ylabel(\"Loss\")\nplt.title(\"Training & Validation Loss\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nWe can see from the plot above that both the training a validation loss were minimized more quickly with momentum than without, which is expected given our above experiment with momentum. Now, let’s use the test set to see how accurate our model is on this dataset.\n\ndef evaluate(model, X, y):\n    preds = model.predict(X)\n    acc = (preds == y).float().mean().item()\n    loss = model.loss(X, y).item()\n    return acc, loss\n\ntest_acc, test_loss = evaluate(model_momentum, X_test, y_test)\nprint(f\"Test Accuracy: {test_acc:.2%}\")\nprint(f\"Test Loss: {test_loss:.2f}\")\n\nTest Accuracy: 89.00%\nTest Loss: 0.22\n\n\nOur model is fairly accurate – 89% on the test set.\n\n\n\n\nDiscussion"
  },
  {
    "objectID": "posts/limits-of-quanitative-approaches/index.html",
    "href": "posts/limits-of-quanitative-approaches/index.html",
    "title": "Limits of the Quantitative Approach to Bias and Fairness",
    "section": "",
    "text": "In a 2022 lecture at Princeton University, associate professor of computer science Arvind Narayanan stated: “currently, quantitative methods are primarily used to justify the status quo. I would argue that they do more harm than good” (Narayanan (2022)). As machine learning algorithms become more prevalent in our lives, and are used to aid in high-stakes decisions like prison sentencing, credit assessment, and pricing, it’s essential that we have a way to measure bias and fairness. Narayanan believes that the quantitative methods which we currently use to assess bias and fairness in machine learning are inadequate. In this essay, I will dissect Narayanan’s position on quantitative methods in bias and fairness, discuss some of the benefits of these quantitative methods, discuss some of the limitations of these quantitative methods, and unpack my own views on these methods.\nIn Narayanan’s speech, he discusses some key limitations he believes exists in our current quantitative methods for assessing fairness and bias in machine learning. He uses the COMPAS study that we have worked with in class as an example – he was initially excited about the data that exhibited bias in the COMPAS algorithm, but after further consideration he developed 7 serious limitations of the way quantitative methods are used to study discrimination. Although Narayanan believes that we should continue to use quantitative methods, he thinks these limitations need to be addressed if these methods continue to be used:\n\nWhat counts as evidence of discrimination is a subjective choice\nNull hypotheses allocate the burden of proof unfairly\nCompounding inequality is not detectable by quantitative methods\nSnapshot datasets can hide discrimination\nStatistical controls can mask discrimination\nDifferent fairness metrics can produce conflicting results\nNumbers are the language of policymaking, even when they are misleading\n\nWe’ll unpack these different ideas along the way as we bring in some more sources to highlight both the benefits and drawbacks of using these methods. First, let’s understand some of the ways in which these quantitative methods have been effective, despite their potential limitations.\nIn Fairness and Machine Learning, Barocas, Hardt, and Narayanan outline 3 quantitative definitions of fairness in machine learning: independence, separation, and sufficiency (Barocas, Hardt, and Narayanan (2023)). We’ll focus on independece here, which requires the sensitive characteristic to be statistically independent of the score. In the early 2010s, Amazon created and AI tool to aid their recruiting process by scoring candidates based on their resumes that did not meet this definition of independence across genders (Dastin (2018)). If we defined \\(R\\) as the score function and \\(\\hat{Y} = I\\{R &gt; t\\}\\) thresholds the score at \\(t\\):\n\\(P\\{\\hat{Y} | \\text{male candidate}\\} \\neq P\\{\\hat{Y} | \\text{female candidate}\\}\\)\nThat is, the probability of assigning a male candidate a score over the threshold was not the same as the probability of assigning a female candidate a score over the threshold. In this case, the male candidates were more likely to be assigned a higher score. This happened because the data the model was trained on was resumes submitted to the company over a 10-year period, of which most came from men. If resumes had the word “women” in them; for example, “women’s chess club captain”, these resumes were likely to be penalized. Once Amazon identified that the independence criteria was not being met for this system, the edited the tool for gender-neutrality, and decided to use the scores generated by the tool as one part of the hiring process, but not rule out any candidates based on the AI process. This example highlights some of the benefits of using quantitative methods to assess fairness and bias in machine learning algorithms. Amazon was able to use the quantitative criteria described above to identify a problem with its algorithm, and make changes to both the algorithm itself and the context in which it was used. Even though the methods like using the independence criteria certainly have some limitations, which we will discuss next, they are still able to quickly and objectively identify a problem and prompt developers to address it. Now that we’ve seen an example of the ways in which quantitative methods can be useful, we can start to understand the drawbacks and limitations of them.\nThe ProPublica article on machine bias, which we have looked at a few times in class, which analyzed the COMPAS algorithm for bias, is a great example of some of the concerns that Narayanan raises about quantitative methods to analyze bias (Angwin et al. (2016)). Even though the algorithm had similar accuracy rates across Black and White defendants, people using the algorithm failed to consider disparities in false positive and false negative rates between these groups. Many of the issues that Narayanan raises are at play here. First, there was a difference in what the algorithm’s creators thought counted as “fair” and what ProPublica thought was “fair”. To frame this in language from Fairness and Machine Learning, COMPAS’s creators were looking for the fact that, if \\(Y\\) is an indicator denoting whether or not a defendant is likely to recommit a crime and \\(\\hat{Y}\\) is an indicator denoting whether that defendant actually did recommit:\n\\(P\\{\\hat{Y} = Y| \\text{white defendant}\\} = P\\{\\hat{Y} = Y| \\text{black defendant}\\}\\)\nThat is, that the accuracy of the algorithm is the same for each racial group. In moral, this is the narrow view of equality as defined in Fairness and Machine Learning. But, the ProPublica authors wanted a more rigorous definition of fairness that aligns with the middle view of equality: better equality in false positive rates across racial groups. In more technical terms:\n\\(P\\{Y = 1 | \\hat{Y} = 0, \\text{white defendant}\\} = P\\{Y = 1 | \\hat{Y} = 0, \\text{black defendant}\\}\\)\nFor the algorithm’s creators, similar accuracy rates across racial groups counted as a fair algorithm, but ProPublica argued that the false positive rate for Black defendants was nearly twice that of white defendants – causing unfair differences in how Black defendants were treated. Second, the creators of COMPAS assumed the null hypothesis that their algorithm was fair unless proven otherwise, and their defense was that no direct racial variables were used in the model—shifting the burden of proof to critics like ProPublica. This ignores the reality that historical and structural biases are embedded in the data itself, making it unnecessary for explicit race-based discrimination to still result in biased outcomes​. Third, COMPAS methods of assessing fairness could not detect compounding inequality. They assessed individual risk without considering the structural inequalities that led to higher arrest rates among Black individuals like the fact that areas with high concentrations of Black residents are more likely to be over-policed and over-charged. The COMPAS algorithm treated past arrests and convictions as neutral indicators, failing to account for how past injustice compounds over time​. Fourth, COMPAS only used a snapshot dataset – a dataset that only considered historical arrest records, not defendants’ actual long-term outcomes. Their algorithm did not consider how a Black defendant’s false high-risk classification could lead to longer incarceration times, which might in turn reduce future employment opportunities and reinforces systemic inequality. Fifth, the COMPAS algorithm controlled for factors like criminal history, age, and gender, in order to argue that any disparities across racial groups were due to legitimate risk factors rather than systemic racism. But, these risk factors were actually shaped by systemic racism, making them a proxy for assigning different scores based on race. Sixth, similar to the first issue, using different fairness metrics, in this case predictive accuracy vs. false positive rates, created conflicting results about whether or not the algorithm was fair. Finally, the seventh issue showed up as COMPAS continued to be used in criminal justice decisions because its numerical outputs carried authority – judges, parole officers, and policymakers trusted the algorithm’s scores despite evidence of bias. People often feel that data is unbiased and algorithms can’t be racist, but we can some of the drawbacks of using quantitative methods do assess the fairness of the COMPAS algorithm across racial groups by applying each of Narayanan’s issues with the quantitative methods uses to assess bias in the COMPAS algorithm used.\nUltimately, I’d agree with Narayanan’s claim about quantitative methods for assessing bias. Although quantitative methods are important and we can use them to understand limitations of our machine learning algorithms, there are many other dimensions in algorithmic bias that need to be considered. As discussed in Data Feminism, power dynamics that can exacerbate bias are embedded in all aspects of data collection, analysis, and interpretation of results (D’Ignazio and Klein (2020)). Narayanan claims that the burden of proof serves to favor the status quo, and Data Feminism goes even further as to argue that quantitative methods are not neutral – they are shaped by and continue to hold up existing power structures. In my experience, people often view quantitative methods as unbiased, they can either be “right” or “wrong”, but when we begin to dig deeper into the systemic power imbalances that are embedded into collecting and interpreting data, even outside of machine learning algorithms, we can see that when quantitative methods are used on datasets that reflect biases in our society, the method themselves begin to reinforce these inequalities. We need to use context, history, and lived experiences in addition to quantitative methods in order to truly capture bias in these algorithms – as D’Ignazio and Klein and write, “context is queen” (D’Ignazio and Klein (2020)). This past J-term, I took a class called Questioning Technology, where we discussed algorithmic bias at length. One of the most interesting issues that we discussed, brough up in a book called More Than A Glitch: Confronting Race, Gender, and Ability Bias in Tech was technochauvanism, a bias that considers computational solutions to be superior to all other solutions (Broussard (2023)). I think that this issue is what lies at the center of the entire discussion around machine learning bias, the fact that people are so willing to accept technological solutions as unbiased and correct. So, like Narayanan claims, I think we can continue to use machine learning algorithms and use quantitative methods to assess their fairness. In fact, I think we should, in order to learn how we can improve these systems. But, we can’t come at this from a technochauvanist angle. Instead, we need to approach machine learning with skepticism, bringing our knowledge of the bias intrinsically present in our datasets to our assessment of machine learning algorithms, especially when using them to inform consequential decisions that can have immense impact on peoples’ lives. To put this into context, in the case of the ProPublica study, technochauvanism was certainly at play. The system we are looking at, the criminal justice system, is already known to have a problem with human bias. We know that judges tend to be biased against people of color, even when they don’t do it intentionally. So, when presented with an algorithmic solution, the criminal justice system was immediately inclined to accept this solution as potentially less biased than human judges. The algorithm is assumed not to include racial bias because developers don’t think that it has been exposed to years of systemic racism like humans have. Therefore, people started using this algorithm without much investigation into its “accuracy” besides its accuracy rates. It required skilled researchers to do a in-depth study in order to expose the problems with the algorithm. But, as we can see from the study, peoples’ lives have already been negatively impacted by this algorithm, despite it being called into question now. Instead, of finding these issues after the fact, a more skeptical approach would look like asking the critical questions that the ProPublica researchers asked, but doing so before the algorithm is deployed. Or, using the algorithm first on a very small subset of cases to understand its limitations and impacts before allowing it to be used on any case without restriction. Furthermore, developers should be skeptical of their data– if they are looking to reduce racial bias, they should be take the time to understand the underlying bias that might be present in the variables they are using to train their model. Ultimately, we cannot continue acting with technochauvanist views and accepting algorithms as unbiased without rigorous testing that involves exploring all the types of potential bias that an algorithm can exhibit. These algorithms have the potential to ruin lives, and we must use them with care.\n\n\n\n\nReferences\n\nAngwin, Julia, Jeff Larson, Surya Mattu, and Lauren Kirchner. 2016. “Machine Bias.” ProPublica. https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing.\n\n\nBarocas, Solon, Moritz Hardt, and Arvind Narayanan. 2023. Fairness and Machine Learning: Limitations and Opportunities. Cambridge, Massachusetts: The MIT Press.\n\n\nBroussard, Meredith. 2023. “More Than a Glitch: Confronting Race, Gender, and Ability Bias in Tech. Cambridge, Massachusetts: The MIT Press.\n\n\nD’Ignazio, Catherine, and Lauren Klein. 2020. Data Feminism. Cambridge, Massachusetts: The MIT Press.\n\n\nDastin, Jeffrey. 2018. “Amazon Scraps Secret AI Recruiting Tool That Showed Bias Against Women.” Reuters. https://www.reuters.com/article/us-amazon-com-jobs-automation-insight-idUSKCN1MK08G.\n\n\nNarayanan, Arvind. 2022. “The Limits of the Quantitative Approach to Discrimination.” Speech."
  },
  {
    "objectID": "posts/replication-study/index.html",
    "href": "posts/replication-study/index.html",
    "title": "Replication Study",
    "section": "",
    "text": "Abstract\nIn this blog post, we will be replicating a study published by Obermeyer et al. in 2019. They found evidence of racial bias in one widely used algorithm. We will visualize how Black patients are often not flagged for extra care even when their white counterparts with the same health conditions are flagged. We will see that Black patients are labeled as lower risk because healthcare costs are used as a proxy for healthcare needs, and less money is spent on Black patients, making it seem like they have less need for extra care. We will quantify this result using a linear regression, and discuss which ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍statistical ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍discrimination ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍criteria describes the bias we found in this algorithm.\nFirst, we’ll read in and take a look at the dataset.\n\n# Read in the data\nurl = \"https://gitlab.com/labsysmed/dissecting-bias/-/raw/master/data/data_new.csv?inline=false\"\ndf = pd.read_csv(url)\n\n\n\n\n\n\n\n\n\n\nrisk_score_t\nprogram_enrolled_t\ncost_t\ncost_avoidable_t\nbps_mean_t\nghba1c_mean_t\nhct_mean_t\ncre_mean_t\nldl_mean_t\nrace\n...\ntrig_min-high_tm1\ntrig_min-normal_tm1\ntrig_mean-low_tm1\ntrig_mean-high_tm1\ntrig_mean-normal_tm1\ntrig_max-low_tm1\ntrig_max-high_tm1\ntrig_max-normal_tm1\ngagne_sum_tm1\ngagne_sum_t\n\n\n\n\n0\n1.987430\n0\n1200.0\n0.0\nNaN\n5.4\nNaN\n1.110000\n194.0\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n7.677934\n0\n2600.0\n0.0\n119.0\n5.5\n40.4\n0.860000\n93.0\nwhite\n...\n0\n1\n0\n0\n1\n0\n0\n1\n4\n3\n\n\n2\n0.407678\n0\n500.0\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n3\n0.798369\n0\n1300.0\n0.0\n117.0\nNaN\nNaN\nNaN\nNaN\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n4\n17.513165\n0\n1100.0\n0.0\n116.0\nNaN\n34.1\n1.303333\n53.0\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n\n\n\n\n5 rows × 160 columns\n\n\n\nWe will be particularly focused on these columns: - risk_score_t ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍is ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍algorithm’s ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍risk ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍score ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍assigned ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍to ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍a ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍given ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍patient. - cost_t ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍is ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍patient’s ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍medical ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍costs ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍in ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍study ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍period. - race ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍is ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍patient’s ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍self-reported ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍race. ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍The ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍authors ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍filtered ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍data ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍to ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍include ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍only ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍white ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍and ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍black ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍patients. - gagne_sum_t ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍is ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍total ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍number ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍of ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍chronic ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍illnesses ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍presented ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍by ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍patient ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍during ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍study ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍period.\n\n\nReproducing Fig. 1\nFirst, we’ll look at some visualizations to get an understanding of our dataset. The first plot we will reproduce from the paper will look at risk score percentiles against mean number of active chronic conditions within each percentile. To make this plot, we’ll start by calculating the percentiles for the risk scores.\n\n# Calculate risk score percentiles\ndf[\"risk_percentile\"] = pd.qcut(df[\"risk_score_t\"], 100, labels=False)\n\nNext, we’ll create a subplot for males and females to further split the data. To plot the risk scores and number of chronic conditions, we will iterate through each race in the dataset (just white and black) and take the mean number of chronic conditions for each risk score percentile. We will use different color and marker for each race, blue circles for black patients and green x’s for white patients.\n\n\nPlotting code for risk score vs. number chronic conditions\n# Define markers for race on our plots\nmarkers = {\"white\": \"x\", \"black\": \"o\"}\ncolors = {\"white\": \"green\", \"black\": \"blue\"}\n\nsns.set_style(\"whitegrid\")\n\n# Create subplots for male (0) and female (1) patients\nfig, axes = plt.subplots(1, 2, figsize=(12, 6), sharey=True)\n\nfor i, female in enumerate([0, 1]):  # 0 = Male, 1 = Female\n    ax = axes[i]\n    # Iterate through the races in the df (black, white)\n    for race in df[\"race\"].unique():\n        subset = df[(df[\"dem_female\"] == female) & (df[\"race\"] == race)]\n        grouped = subset.groupby(\"risk_percentile\")[\"gagne_sum_t\"].mean().reset_index()\n        sns.scatterplot(\n            data=grouped, \n            x=\"gagne_sum_t\", \n            y=\"risk_percentile\", \n            marker=markers[race], \n            color = colors[race],\n            label=f\"{race.capitalize()} Patients\", \n            ax=ax\n        )\n    ax.set_title(f\"{'Female' if female == 1 else 'Male'} Patients\")\n    ax.set_xlabel(\"Mean Chronic Conditions\")\n    if i == 0:\n        ax.set_ylabel(\"Risk Score Percentile\")\n    ax.legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nLet’s take a look at this plot to understand what might happen in this scenario: Patient A is black and Patient B is white and both patients have the same chronic illnesses. We can see from the plot that across both genders, it seems that Patient A and Patient B are not equally likely to be referred to the high-risk care management program. The blue points, which represent black patients, consistently have lower risk scores than green points that have at the same number of chronic illnesses. This would make it less likely for these black patients to be referred to a high-risk care management program because they are being scored as lower risk than white patients with the same illnesses.\n\n\nReproducing Fig. 3\nNow, we’ll reproduce another figure from the paper that should give us some more insight into why Black patients are being given lower risk scores than white patients with the same number of chronic conditions. First, we’ll group across risk percentile and race to take the mean total medical expenditure for each race at each risk percentile. Then, we’ll group by number of chronic conditions and race so that we can take the mean total medical expenditure for each race at each number of chronic conditions.\n\nrisk_grouped = df.groupby([\"risk_percentile\", \"race\"])[\"cost_t\"].mean().reset_index()\nchronic_illness_grouped = df.groupby([\"gagne_sum_t\", \"race\"])[\"cost_t\"].mean().reset_index()\n\nNext, we’ll make two subplots – one that plots total medical expenditure by risk score percentile and one that plots total medical expenditure by number of chronic conditions. Again, we will use blue circles for black patients and green x’s for white patients.\n\n\nPlotting code for medical expenditure vs. number chronic conditions and risk score\n# Create the figure with two panels\nfig, axes = plt.subplots(1, 2, figsize=(12, 6), sharey=True)\n\n# Left panel: Mean expenditure vs. Risk Score Percentile\nfor race in df[\"race\"].unique():\n    subset = risk_grouped[risk_grouped[\"race\"] == race]\n    sns.scatterplot(\n        data=subset,\n        x=\"risk_percentile\",\n        y=\"cost_t\",\n        marker=markers[race],  \n        color=colors[race],  \n        label=f\"{race.capitalize()} Patients\",\n        ax=axes[0]\n    )\n\naxes[0].set_title(\"Medical Expenditure vs. Risk Score Percentile\")\naxes[0].set_xlabel(\"Risk Score Percentile\")\naxes[0].set_ylabel(\"Mean Medical Expenditure ($)\")\naxes[0].set_ylim(1000, 10000)\naxes[0].legend()\naxes[0].set_yscale(\"log\")\n\n# Right panel: Medical Expenditure vs. Number of Chronic Conditions\nfor race in df[\"race\"].unique():\n    subset = chronic_illness_grouped[chronic_illness_grouped[\"race\"] == race]\n    sns.scatterplot(\n        data=subset,\n        x=\"gagne_sum_t\",\n        y=\"cost_t\",\n        marker=markers[race],  \n        color=colors[race],  \n        label=f\"{race.capitalize()} Patients\",\n        ax=axes[1]\n    )\n\naxes[1].set_title(\"Medical Expenditure vs. Chronic Conditions\")\naxes[1].set_xlabel(\"Number of Chronic Conditions\")\naxes[1].set_ylabel(\"\")\naxes[1].set_ylim(1000, 100000)\naxes[1].legend()\naxes[1].set_yscale(\"log\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nIn this plot, we can get an understanding of how total medical expenditures are correlated with risk score and the number of chronic illnesses. It seems from the left panel that there is more variation in the mean medical expenditures for black patients by risk score than there is for white patients. Although we see many blue points (black patients) below green points with the same risk score, we also see blue points above green points with the same risk score. There is an interesting trend in the right panel – it appears that white patients tend to have more medical expenditures than blank patients with the same number of chronic conditions when the number of conditions is less than 5, but as the number of conditions increases, black patients tend to have more medical expenditures than white patients with the same number of conditions. Considering that the vast majority of patients in this dataset have fewer than 5 chronic conditions, this dataset seems to show black patients, on average, having fewer medical expenditures than white patients with the same number of chronic conditions.\n\n\nModeling Cost Disparity\nNow that we’ve gotten an understanding of the dataset and what’s going on with bias here, let’s make a model so that we can quantify it more accurately.We’ll start with prepping the data.\n\nData Prep\n\n\nPercentage of patients in DF with 5 or fewer chronic conditions: 0.96 %\n\n\nGiven that the vast majority of people in the dataset have fewer than 5 chronic conditions, I think it makes sense to just focus on these people in our analysis. We will take the log of the medical expenditures because they have a large range.\n\n# Take the log of the cost column (also remove costs that are 0)\ndf = df[df[\"cost_t\"] &gt; 0]\ndf[\"log_cost\"] = np.log(df[\"cost_t\"])\n\nNext we’ll encode the race dummy as an 0-1 variable, with 1 for black individuals and 0 for white individuals.\n\n# Encode the race variable as an integer \ndf[\"race_dummy\"] = (df[\"race\"] == \"black\").astype(int)\n\nLastly, we will separate out our features (race and number of chronic conditions) from our target variable (total medical expenditure).\n\n# Separate predictor and target variables\nX = df[[\"race_dummy\", \"gagne_sum_t\"]]\ny = df[\"log_cost\"]\n\n\n\nModeling\nWe will fit a linear regression model with polynomial features in the number of active chronic conditions to account for the nonlinearity in the relationship between number of active chronic conditions that is apparent from the left hand panel in the above plot. We’ll first define a function to add polynomial features to the dataset.\n\ndef add_polynomial_features(X, degree):\n  X_ = X.copy()\n  for j in range(1, degree):\n    X_[f\"poly_{j}\"] = X_[\"gagne_sum_t\"]**j\n  return X_\n\nNext, we will use this function to loop through polynomial features from \\(1\\) to \\(11\\) and regularization strengths \\(10^k\\) from \\(k = -4, -3, ..., 3, 4\\). The regularization strength is a parameter that describes how much the training process should prefer “simple” models with small values of the weights \\(w\\) as opposed to a more complex model. At each degree-regularization combination, we’ll train a model and compute the cross-validated mean squared error of the model. We will take the combination with the smallest mean squared error and use that degree-regularization combination to train the model we will use for our analysis.\n\n# Ranges of degrees and ks for regularization\ndegrees = range(1, 12)      \nks = range(-4, 4)    \n\nresults = []\n\nfor degree in degrees:\n    # Add polynomial features to X_base\n    X_poly = add_polynomial_features(X, degree)\n    \n    # Loop through each regularization strength\n    for k in ks:\n        alpha = 10**k\n        # Create a Ridge regression model with the given alpha\n        model = Ridge(alpha=alpha)\n        \n        # Use cross-validation to estimate the mean squared error\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\"ignore\")\n            scores = cross_val_score(model, X_poly, y, cv=5, scoring=\"neg_mean_squared_error\")\n        # cross_val_score returns negative MSE when scoring is set to \"neg_mean_squared_error\" so we negate it for the mean\n        mse = -np.mean(scores)\n            \n        # Save the combination and results\n        results.append({\n            \"degree\": degree,\n            \"alpha\": alpha,\n            \"cv_mse\": mse\n        })\nresults_df = pd.DataFrame(results)\nresults_df = results_df.sort_values(\"cv_mse\")\n\n# Identify the best (lowest CV MSE) combination\nbest = round(results_df.iloc[0],2)\n\n\n\n\nBest combination:\ndegree    10.00\nalpha      1.00\ncv_mse     1.51\nName: 76, dtype: float64\n\n\nSo, we can see that our optimal parameter combination is \\(\\alpha = 1\\) with a degree of \\(10\\). Next let’s fit a model to the entire dataset using these parameters.\n\n# Fit a model on the entire dataset \nX_poly = add_polynomial_features(X, 10)\n\n# Create a Ridge regression model with the alpha=1\nmodel = Ridge(alpha=1)\n\n# Fit the model on the entire dataset to extract coefficients\nmodel.fit(X_poly, y)\n\n# Find the coefficient corresponding to race_dummy\ncoef_index = list(X_poly.columns).index(\"race_dummy\")\nrace_coef = model.coef_[coef_index]\nexp_race_coef = np.exp(race_coef)\n\n\n\n\nBlack coefficient: -0.27\n\n\nBased on these results, we can see that the coefficient associated with the Black individuals in the dataset is \\(w_b = -0.27\\). We can calculate \\(e^{w_b} = e^{-0.27} = 0.76\\). So, percentage of cost incurred by a Black patient in comparison to an equally sick white patient, is around 76%. This supports the argument that was made in the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍Obermeyer paper because it suggests that less money is spent on Black patients who have the same level of need as white patients.\n\n\n\nDiscussion\nAfter replicating the study done in the Obermeyer paper, we can see that because less money is spent on Black patients’ healthcare, they are often classified as lower risk than white patients who have the same chronic conditions. This has concerning implications, as it seems like this algorithm was making it harder for Black patients to access care than white patients, even if the Black patients are sicker than the white patients.\nIn terms of the formal ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍statistical ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍discrimination ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍criteria ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍discussed in Chapter 3 of ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍ ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍Barocas, Hardt, and Narayanan, the separation criterion best describes the bias that Obermeyer et al. found in their paper. This is because separation would require that the risk score be conditionally independent of race given true health need. That is, if two individuals have the same underlying health status, their risk scores should be statistically indistinguishable across racial groups. But, our results show that this isn’t true when we use this algorithm. Our results showed that Black patients with the same chronic conditions as white patients are assigned lower risk scores because the algorithm uses healthcare expenditures as a proxy for health need. Even though the algorithm may be able to predict health care costs effectively, these effective predictions are not an accurate proxy for health need, which makes the algorithm violate the separation criterion. Even though the model may be accurate, it is not necessarily unbiased.\nI learned a lot from replicating this study. I think I went into it with an understanding of the importance of accuracy and equality in a model, but without considering the implications of the model. Even though this model may have been accurate, the context in which it was being applied made it so that it was biased against Black individuals. Using one feature as a proxy for another feature seems tricky, and requires a lot of auditing to ensure that it doesn’t create bias."
  },
  {
    "objectID": "posts/auditing-bias/index.html",
    "href": "posts/auditing-bias/index.html",
    "title": "Auditing Bias",
    "section": "",
    "text": "Abstract\nIn this blog post we’ll be looking at census data and trying to predict whether a person will have an income over $50,000. We will not use a persons’ sex to predict their income. After training and implementing this algorithm, we will audit our model for gender bias. We will see that although the model doesn’t appear to perform too differently across male and female groups, there are some differences in the model performance, which we will explore further and discuss.\n\n\nPulling and Prepping the Data\n\n# Read in the data\n\nSTATE = \"CA\"\n\ndata_source = ACSDataSource(survey_year='2018', \n                            horizon='1-Year', \n                            survey='person')\n\nacs_data = data_source.get_data(states=[STATE], download=True)\nacs_data=acs_data[['PINCP', 'AGEP', 'SCHL', 'MAR', 'RELP', 'DIS', 'ESP', 'CIT', 'MIG', 'MIL', 'ANC', 'NATIVITY', 'DEAR', 'DEYE', 'DREM', 'SEX', 'RAC1P', 'ESR']]\n\nacs_data.head()\n\n\n\n\n\n\n\n\nPINCP\nAGEP\nSCHL\nMAR\nRELP\nDIS\nESP\nCIT\nMIG\nMIL\nANC\nNATIVITY\nDEAR\nDEYE\nDREM\nSEX\nRAC1P\nESR\n\n\n\n\n0\n48500.0\n30\n14.0\n1\n16\n2\nNaN\n1\n3.0\n4.0\n1\n1\n2\n2\n2.0\n1\n8\n6.0\n\n\n1\n0.0\n18\n14.0\n5\n17\n2\nNaN\n1\n1.0\n4.0\n1\n1\n2\n2\n2.0\n2\n1\n6.0\n\n\n2\n13100.0\n69\n17.0\n1\n17\n1\nNaN\n1\n1.0\n2.0\n2\n1\n2\n2\n2.0\n1\n9\n6.0\n\n\n3\n0.0\n25\n1.0\n5\n17\n1\nNaN\n1\n1.0\n4.0\n1\n1\n1\n2\n1.0\n1\n1\n6.0\n\n\n4\n0.0\n31\n18.0\n5\n16\n2\nNaN\n1\n1.0\n4.0\n1\n1\n2\n2\n2.0\n2\n1\n6.0\n\n\n\n\n\n\n\n\nfeatures_to_use = ['AGEP', 'SCHL', 'MAR', 'RELP']\n\nFirst, we’re reading in the census data. We’re also going to select some features that we’re going to use for our mode. We’ll be predicting income, so I chose some features that I think will be relevant – age, education level, marriage status, and relationship to “head of household”. Now, ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍we’ll ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍construct ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍a ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍BasicProblem that expresses that we are going to use the features we specified above to predict if a person’s income (“PINCP”) is over $50,000.\n\nIncomeProblem = BasicProblem(\n    features=features_to_use,\n    target='PINCP',\n    target_transform=lambda x: x &gt; 50000,\n    group='SEX',\n    preprocess=lambda x: x,\n    postprocess=lambda x: np.nan_to_num(x, -1),\n)\nfeatures, label, group = IncomeProblem.df_to_numpy(acs_data)\n\n\n\nBasic Descriptives\nBefore we get into creating our model, let’s get to know our dataset so that we understand the demographics that we’re working with. We’ll split our dataset into train and test sets.\n\nX_train, X_test, y_train, y_test, group_train, group_test = train_test_split(\n    features, label, group, test_size=0.2, random_state=0)\n\nThis is what our dataset looks like. It has all of the features that we’re going to use to train our model, and it has a “group” column where 1 represents a male and 2 represents a female. The “label” column, which we will isolate as our target variable, is a boolean (true for a person with income &gt; $50,000, false otherwise).\n\n\n\n\n\n\n\n\n\nAGEP\nSCHL\nMAR\nRELP\ngroup\nlabel\n\n\n\n\n0\n77.0\n16.0\n2.0\n0.0\n2\nTrue\n\n\n1\n29.0\n20.0\n5.0\n2.0\n1\nFalse\n\n\n2\n60.0\n21.0\n1.0\n0.0\n2\nFalse\n\n\n3\n27.0\n19.0\n5.0\n13.0\n2\nFalse\n\n\n4\n63.0\n23.0\n3.0\n0.0\n1\nTrue\n\n\n\n\n\n\n\nNow, let’s get some information about the data:\n\n\nNumber of individuals in the training set: 303053\n\n\n\n\nNumber of individuals in the training set with income over $50,000: 73963\n\n\n\n\nNumber of males in the training set: 149294\nNumber of females in the training set: 153759\n\n\n\n\nMale proportion w/ income over $50,000: 0.3\nFemale proportion w/ income over $50,000: 0.19\n\n\nWe can see that there are more females than males in the training set, although not by much. It’s almost evenly split. There are also about 10% more males than females have an income of over $50,000. It’s important to understand the breakdown in our dataset because if we’re looking for a model to perform well for both groups, we want there to be enough data in the training set for each group. We also want to be sure that we have people from each group with income above and below the threshold, so that the model can learn patterns about people in each group with each label. Now, let’s look at intersectional trends in the data by breaking out the income groups by sex and education level.\n\n\nPlotting code\n# Compute proportions of positive labels by both SEX and SCHL\nintersectional_counts = df.groupby([\"group\", \"SCHL\"])[\"label\"].mean().reset_index()\n\n# Rename columns for clarity\nintersectional_counts.columns = [\"Sex\", \"Education Level\", \"Proportion with Positive Label\"]\n\n# Create a bar plot\nplt.figure(figsize=(12, 6))\nsns.barplot(\n    data=intersectional_counts, \n    x=\"Education Level\", \n    y=\"Proportion with Positive Label\", \n    hue=\"Sex\"\n)\n\n# Labels and title\nplt.xlabel(\"Education Level (SCHL)\")\nplt.ylabel(\"Proportion with Positive Label (e.g., Income &gt; $50k)\")\nplt.title(\"Proportion of Positive Labels by Sex and Education Level\")\nplt.legend(title=\"Sex\")\n\n# Rotate x-axis labels for better readability\nplt.xticks(rotation=45)\n\n# Show plot\nplt.show()\n\n\n\n\n\n\n\n\n\nHere, we can see that across all education levels, men are more likely to have an income over $50,000 than women are. We can also see that this gap seems to be exacerbated at lower education levels. There is a bigger gap between the proportion of men and proportion of women with income over $50,000 at 15-19 years of education than at the higher education levels of 21-24 years. This is an important example of intersectional trends, because we can see that the income disparity between men and women varies depending on education level.\n\n\nTraining the Model\nNow that we understand our dataset, we’ll train a support vector machine model to predict if an individual’s income is over $50,000. An SVM model aims to find an optimal hyperplane in an N-dimensional space to separate data points into different classes. It is similar to a linear classification model, but it can be in multiple dimensions. The SVM algorithm maximizes the margin between the closest points of different classes. One of the parameters in an SVM model is the regularization term “C”, which is used to balance the misclassification penalties with maximization. If the value of “C” is higher, the model more strictly penalizes misclassifications. Before training our model, we will use cross-validation to find an optimal value of this “C” parameter.\n\n\nCross-val code\nmodel = make_pipeline(StandardScaler(), LinearSVC(dual=False))\nmodel.fit(X_train, y_train)\n\n# Define the pipeline\npipeline = make_pipeline(StandardScaler(), LinearSVC(dual=False, max_iter=5000))\n\n# Define the hyperparameter grid for C\nparam_grid = {'linearsvc__C': [0.001, 0.01, 0.1, 1, 10, 100]}\n\n# Perform Grid Search with Cross-Validation\ngrid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\ngrid_search.fit(X_train, y_train)\n\n# Get the best C value\nbest_C = grid_search.best_params_['linearsvc__C']\nprint(f\"Best C: {best_C}\")\n\n# Best model with optimal C\nbest_model = grid_search.best_estimator_\n\n\nBest C: 0.001\n\n\nBased on our cross-validation, the value of “C” that we should use is 0.001. Now, we’ll create and train our model using a pipeline. The pipeline is used to sequentially apply transformations – here, we are using StandardScaler() to standardize all of the features in the dataset.\n\nmodel = make_pipeline(StandardScaler(), LinearSVC(C=0.001, dual=False))\nmodel.fit(X_train, y_train)\n\nPipeline(steps=[('standardscaler', StandardScaler()),\n                ('linearsvc', LinearSVC(C=0.001, dual=False))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('standardscaler', StandardScaler()),\n                ('linearsvc', LinearSVC(C=0.001, dual=False))])StandardScalerStandardScaler()LinearSVCLinearSVC(C=0.001, dual=False)\n\n\nNow, that we’ve fit our model, we’re going to test it on the test set and then audit the model for gender bias. Recall that we didn’t include sex as one of the features in our model, so we’re interested to see if the model we trained on other features might be implicitly biased by sex.\n\n\nAuditing the Model\n\ny_hat = model.predict(X_test)\n\n\n\nOverall accuracy: 0.8\n\n\n\n\nAccuracy for men: 0.79\n\n\n\n\nAccuracy for women: 0.81\n\n\nAt first glance, we can see that our model is slightly more accurate for women, but it performs fairly similarly across each group when looking at accurate. But, let’s get more precise to understand more about the differences in accuracy between groups. We’ll get the accuracy using the accuracy score function, the PPV using the precision score function, and the true positive, true negative, false negative, and false positive rates from the confusion matrix. We’ll get all of these metrics for the whole test set, as well as the test set filtered for the female and male groups. We will put all of these metrics into a dictionary for easy access when we plot these results next.\n\n\nCode to calculate accuracy and error rates\n# Make predictions\ny_pred = best_model.predict(X_test)\n\n### Overall Measures ###\n# Compute overall accuracy\noverall_accuracy = accuracy_score(y_test, y_pred)\n\n# Compute overall PPV (Positive Predictive Value / Precision)\noverall_ppv = precision_score(y_test, y_pred)\n\n# Compute overall confusion matrix\ntn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n\n# Compute overall False Positive Rate (FPR) and False Negative Rate (FNR)\noverall_fpr = fp / (fp + tn)\noverall_fnr = fn / (fn + tp)\n\n# Print overall metrics\nprint(\"\\n--- Overall Model Performance ---\")\nprint(f\"Overall Accuracy: {overall_accuracy:.2f}\")\nprint(f\"Overall PPV: {overall_ppv:.2f}\")\nprint(f\"Overall False Positive Rate (FPR): {overall_fpr:.2f}\")\nprint(f\"Overall False Negative Rate (FNR): {overall_fnr:.2f}\")\n\n### By-Group Measures ###\nprint(\"\\n--- Subgroup Model Performance ---\")\nsubgroup_results = []\n\nfor sex in [1, 2]:  # 1 = Male, 2 = Female\n    mask = (group_test == sex)\n    \n    # Compute subgroup accuracy\n    subgroup_accuracy = accuracy_score(y_test[mask], y_pred[mask])\n    \n    # Compute subgroup PPV\n    subgroup_ppv = precision_score(y_test[mask], y_pred[mask])\n    \n    # Compute subgroup confusion matrix\n    tn_s, fp_s, fn_s, tp_s = confusion_matrix(y_test[mask], y_pred[mask]).ravel()\n\n    # Compute subgroup FPR and FNR\n    subgroup_fpr = fp_s / (fp_s + tn_s)\n    subgroup_fnr = fn_s / (fn_s + tp_s)\n    \n    group_name = \"Male\" if sex == 1 else \"Female\"\n    print(f\"\\nGroup: {group_name}\")\n    print(f\"  Accuracy: {subgroup_accuracy:.2f}\")\n    print(f\"  PPV: {subgroup_ppv:.2f}\")\n    print(f\"  False Positive Rate (FPR): {subgroup_fpr:.2f}\")\n    print(f\"  False Negative Rate (FNR): {subgroup_fnr:.2f}\")\n\n    subgroup_results.append({\n        \"Sex\": group_name,\n        \"Accuracy\": subgroup_accuracy,\n        \"PPV\": subgroup_ppv,\n        \"FPR\": subgroup_fpr,\n        \"FNR\": subgroup_fnr\n    })\n\n### Bias Measures ###\n# Calibration: The model is considered calibrated if P(Y=1 | S=1) ≈ P(Y=1 | S=2)\ncalibration_diff = abs(y_pred[group_test == 1].mean() - y_pred[group_test == 2].mean())\n\n# Error Rate Balance: FNR and FPR should be similar across groups\nerror_rate_balance = abs(subgroup_results[0][\"FNR\"] - subgroup_results[1][\"FNR\"]) + \\\n                     abs(subgroup_results[0][\"FPR\"] - subgroup_results[1][\"FPR\"])\n\n# Statistical Parity: P(Y=1 | S=1) ≈ P(Y=1 | S=2)\nstatistical_parity_diff = abs(np.mean(y_pred[group_test == 1]) - np.mean(y_pred[group_test == 2]))\n\n# Print Bias Metrics\nprint(\"\\n--- Bias Metrics ---\")\nprint(f\"Calibration Difference: {calibration_diff:.2f}\")\nprint(f\"Error Rate Balance: {error_rate_balance:.2f}\")\nprint(f\"Statistical Parity Difference: {statistical_parity_diff:.2f}\")\n\n\n\n--- Overall Model Performance ---\nOverall Accuracy: 0.80\nOverall PPV: 0.63\nOverall False Positive Rate (FPR): 0.08\nOverall False Negative Rate (FNR): 0.56\n\n--- Subgroup Model Performance ---\n\nGroup: Male\n  Accuracy: 0.79\n  PPV: 0.75\n  False Positive Rate (FPR): 0.06\n  False Negative Rate (FNR): 0.56\n\nGroup: Female\n  Accuracy: 0.81\n  PPV: 0.50\n  False Positive Rate (FPR): 0.10\n  False Negative Rate (FNR): 0.57\n\n--- Bias Metrics ---\nCalibration Difference: 0.01\nError Rate Balance: 0.05\nStatistical Parity Difference: 0.01\n\n\nThese results tell us more information about the difference in model performance between the two groups. We can see that the male group has a lower false positive rate (FPR) than the female group, that is, our model is less likely to incorrectly predict that a man has income over $50,000 when he actually does not. The false negative rate (FNR) is very similar across groups, so we are pretty much equally likely to predict that a male or female does not have income over $50,000 when they actually do. The male group has a higher PPV value than the female group, which we’ll explore further in the plot below. We will reproduce a plot from the COMPAS paper that compares the FPR and FNR rates across groups and plots potential combinations of FPR/FNR rates for each group. The plot also looks at the potential combinations of FPR/FNR rates for the female group if we fix the PPV rate for the female group to the larger PPV value of the male group.\n\n\nCode to reproduce plot from COMPAS paper\n# Extract values for male and female\np_male = np.mean(y_test[group_test == 1])  # Prevalence of male\np_female = np.mean(y_test[group_test == 2])  # Prevalence of female\n\nPPV_female = subgroup_results[1][\"PPV\"] # Observed PPV for females\nPPV_male_fixed = PPV_female # Set PPV_male to PPV_female\n\n# Extract FNR and FPR values for observed points\nfnr_values = [subgroup_results[0][\"FNR\"], subgroup_results[1][\"FNR\"]]\nfpr_values = [subgroup_results[0][\"FPR\"], subgroup_results[1][\"FPR\"]]\nlabels = [subgroup_results[0][\"Sex\"], subgroup_results[1][\"Sex\"]]\n\n# Generate range of FNR values from 0 to 1\nfnr_range = np.linspace(0, 1, 100)\n\n# Compute feasible FPR values for males\nfpr_feasible_male = ((1 - fnr_range) * p_male * (1 - PPV_male_fixed)) / (PPV_male_fixed * (1 - p_male))\n\n# Compute feasible FPR values for females when PPV_female = PPV_male\nfpr_feasible_female = ((1 - fnr_range) * p_female * (1 - PPV_female)) / (PPV_female * (1 - p_female))\n\n# Define range of delta values for shading\ndelta_values = [0.05, 0.1, 0.15]  # Smaller delta -&gt; smaller feasible region\n\nplt.figure(figsize=(10, 6))\n\n# Scatter plot of observed values\nsns.scatterplot(x=fnr_values, y=fpr_values, hue=labels, palette=[\"lightblue\", \"lightpink\"], s=100)\n\n# Plot feasible set for males\nplt.plot(fnr_range, fpr_feasible_male, color=\"lightblue\", label=\"Feasible Set (Male)\")\n\n# Plot feasible set for females when PPV_female = PPV_male\nplt.plot(fnr_range, fpr_feasible_female, color=\"lightpink\", label=\"Feasible Set (Female, PPV=PPV_male)\")\n\n# Add nested shaded regions for varying PPV_female constraints\nfor i, delta in enumerate(delta_values):\n    # PPV_female_upper = PPV_male + delta\n    # PPV_female_lower = PPV_male - delta\n    \n    PPV_male_upper = PPV_female + delta\n    PPV_male_lower = PPV_female - delta\n\n\n    # Compute feasible FPR values for upper and lower PPV bounds\n    fpr_feasible_upper = ((1 - fnr_range) * p_male * (1 - PPV_male_upper)) / (PPV_male_upper * (1 - p_male))\n    fpr_feasible_lower = ((1 - fnr_range) * p_male * (1 - PPV_male_lower)) / (PPV_male_lower * (1 - p_male))\n\n    # Fill region between upper and lower bounds\n    plt.fill_between(fnr_range, fpr_feasible_lower, fpr_feasible_upper, color=\"lightgrey\", alpha=0.3)\n\n# Labels and title\nplt.xlabel(\"False Negative Rate (FNR)\")\nplt.ylabel(\"False Positive Rate (FPR)\")\nplt.title(\"Feasible (FNR, FPR) combinations\")\n\n# Show plot\nplt.show()\n\n\n\n\n\n\n\n\n\nThis chart is a reproduction of the Chouldechova paper that we discussed in class. We fixed ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the proportion of true positive labels for eac‍‍‍‍‍‍‍‍‍‍‍‍‍h group, and we set the PPV values to be equal across both groups. The pink and blue points show the observed FPR and FNR rates for the male and female groups. The lines represent the possible combinations of FPR and FNR for each group, with the PPV for the male group fixed at the lower PPV that we observed for the female group. Given this, we can see that if we wanted the male FPR to be the same as the female FPR, we would need to increase the false negative rate around 35% so that the blue point would shift right onto the blue line. This tells us that we would need to make the model for males much less accurate than it is now to make the models “equal” in their FPR.\n\n\nConcluding Discussion\nUltimately, this model didn’t exhibit that much bias. Even though the male group had a higher PPV, the accuracy, FPRs, and FNRs were similar across the two groups. There are many people that might be interested in this model, like people doing credit risk assessment, recruiters and employers, or banks. These institutions would all have an interest in predicting how much income an individual might make. At a large scale, this model might be slightly more accurate for predicting male income. This could potentially have a negative impact on females because they might be more likely to be denied credit or a job if their income is predicted to be lower than it actually is. This could lead to biased employment practices, unfair denial of services, or housing discrimination. On the positive side, this model could have the impact of streamlining financial services and job-matching or aiding governments in economic research and policy planning with access to income data. I didn’t feel that this model displays ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍problematic ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍bias. The error rates were very similar across groups, but I could see the potential for a model like this to be biased against women, given the systemic wage gap and sexism inherent in our society. Some of the other problems ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍associated with deploying this model, like privacy issues and regulatory compliance. To mitigate these potential impacts, it would be useful to conduct bias audits at a large scale, like the one we did here. It could also be important to ensure that humans, instead of the algorithm, end up having the final say about decisions that might be impacted by the model results."
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "from source import Perceptron\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/palmer-penguins/index.html",
    "href": "posts/palmer-penguins/index.html",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "Abstract\nIn this blog post we are going to train a random forest model to classify penguin species based on three features in the Palmer Penguins dataset. We’ll start with some visualizations to get an understanding of the different features in the dataset, and then we will use a random forest to choose the best three features to train our model on. After training and testing the model, we will explore how well the model did and how it made its classifications using the confusion matrix and plotting decision regions.\n\n\nPreparing the Data\n\n# Read in the data\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\n\nle = LabelEncoder()\ntrain[\"Species\"].replace('Adelie Penguin (Pygoscelis adeliae)','Adelie',inplace=True)\ntrain[\"Species\"].replace('Chinstrap penguin (Pygoscelis antarctica)','Chinstrap',inplace=True)\ntrain[\"Species\"].replace('Gentoo penguin (Pygoscelis papua)','Gentoo',inplace=True)\nle.fit(train[\"Species\"])\n\n# Function to prepare the data \ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1) # Remove unnecessary columns\n  df = df[df[\"Sex\"] != \".\"]                                                                                 \n  df = df.dropna() # Drop NAs                                                                                          \n  y = le.transform(df[\"Species\"]) # Encode the species column                                                                      \n  df = df.drop([\"Species\"], axis = 1)                                                                       \n  df = pd.get_dummies(df) # One-hot encode the boolean columns                                                                \n  return df, y\n\nX_train, y_train = prepare_data(train)\n\nIn the above code, we processed the dataset to prepare it for analysis as follows:\n\nRemove the unnecessary columns and NAs\nPrepare the qualitative columns for analysis by encoding them as quantitative columns\n\nEncode the species column\n\nEach species is assigned a number\n\n“One-hot encode” the boolean columns\n\nThe get_dummies function converts these columns to 0-1 instead of True/False\n\n\n\n\n\narray([1, 1, 2, 2, 1, 0, 0, 1, 2, 1, 0, 1, 0, 1, 1, 2, 0, 2, 2, 2, 2, 0,\n       0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 2, 2, 1, 1, 2, 2, 1, 0,\n       0, 2, 2, 1, 2, 2, 1, 2, 0, 0, 2, 2, 0, 1, 2, 2, 1, 2, 1, 2, 2, 2,\n       0, 0, 0, 2, 2, 2, 0, 1, 2, 2, 2, 0, 0, 2, 0, 0, 2, 0, 0, 0, 1, 0,\n       0, 1, 0, 0, 0, 1, 0, 0, 2, 2, 0, 0, 2, 0, 2, 1, 0, 2, 2, 1, 2, 2,\n       2, 0, 2, 0, 0, 0, 1, 0, 2, 2, 0, 2, 2, 1, 2, 0, 0, 1, 2, 2, 1, 0,\n       2, 0, 1, 2, 0, 0, 2, 0, 2, 1, 0, 0, 2, 1, 0, 2, 0, 2, 0, 1, 0, 0,\n       0, 2, 2, 2, 0, 0, 2, 0, 2, 1, 1, 0, 1, 2, 0, 0, 1, 1, 0, 0, 1, 0,\n       0, 2, 1, 2, 1, 0, 0, 0, 1, 0, 2, 2, 2, 2, 1, 1, 1, 2, 2, 2, 0, 1,\n       0, 0, 1, 0, 0, 0, 0, 0, 1, 2, 2, 2, 0, 2, 2, 1, 0, 2, 0, 0, 2, 0,\n       2, 0, 2, 0, 2, 2, 2, 2, 0, 2, 1, 0, 2, 1, 1, 0, 2, 1, 0, 0, 0, 1,\n       0, 1, 0, 0, 2, 1, 0, 0, 0, 2, 0, 0, 0, 1])\n\n\n\n\nVisualizing the data\nNow, we need to figure out which features best distinguish each species so that we can train a model to predict the species based on the features. We’ll start by looking at visualizations of some of the features in the dataset in order to get an idea of which ones will work well for classification.\n\nsns.set_theme()\n# Plot the culmen depth and flipper length \nsns.relplot(\n    data=train,\n    x=\"Flipper Length (mm)\", y=\"Culmen Depth (mm)\",\n    hue=\"Species\", style=\"Species\"\n).set(title=\"Penguin Species by Culmen Depth and Flipper Length\")\n\n\n\n\nFigure 1: culmen length and flipper depth of each penguin species\n\n\n\n\n\n# Plot the distribution of each species on each of the 3 islands \nax = sns.countplot(data=train, x=\"Island\", hue=\"Species\")\nax.set(xlabel=\"Island\", ylabel=\"Number of Penguins\", title=\"Species Distribution Across Islands\")\nax.set_xticklabels(ax.get_xticklabels(), rotation=30)\n\n\n\n\nFigure 2: species distribution across each island\n\n\n\n\n\n\n\n\n\n\n\n\n\nCulmen Length (mm)\n\n\nSpecies\n\n\n\n\n\nAdelie\n38.970588\n\n\nChinstrap\n48.826316\n\n\nGentoo\n47.073196\n\n\n\n\n\nFigure 3: mean culmen length by species\n\n\nOur goal is to find three features that will allow us to train a model which will be 100% accurate in predicting the species of a penguin in the training set. We can ascertain some information about the top scored features from these plots that will be helpful to determine if these features will work.\nFirst, as we can see in Figure 1, the Gentoo penguin has different culmen and flipper measurements than the Chinstrap and Adelie penguins. Specifically, the Gentoo penguin seems to have a larger flipper length and a shallower culmen depth than the Chinstrap and Adelie penguins. This relationship could be used to identify the Gentoo penguins, but we still need to find a way to distinguish between the Chinstrap and Adelie penguins.\nFigure 2 provides a way to do this; the Chinstrap penguins are only found on Dream Island. We can also see that the Adelie penguins are found on all three islands, and the Gentoo penguin is only found on Biscoe Island. In the summary table, we can see that there is also a difference in culmen length between the species. The Chinstrap and Gentoo penguins have similar average culmen lengths, but the Adelie penguins have much shorter culmens on average.\nUsing some combination of these features, we should be able to accurately predict the species of any penguin in the training set. We will use tree-based feautre selection to determine which of the features we will use to train our model.\n\n\nOptimizing model parameters\nWe will be using a random forest to choose the best features to train our model on. We’ll also use a random forest to execute our classification. First, let’s understand what a random forest is:\n\nRandom Forest\n\nRandom forests are an ensemble learning method. That is, they combine predictions from multiple models to make a final prediction. Random forests use multiple decision trees to make a prediction. Each tree is trained on a different subset of the training set, so different trees can learn different patterns in the data. The trees use a process of “voting” to determine the prediction–whichever species is predicted by the most trees is the final prediction.\n\n\n\nChoosing the best features\n\n# Fit a model to find the best features in the training set\ntree_selector = ExtraTreesClassifier(n_estimators=100, random_state=42)\ntree_selector.fit(X_train, y_train)\nfeature_importances = tree_selector.feature_importances_\n\n# Create DataFrame with the feature and its importance score\nfeature_scores = pd.DataFrame({\"Feature\": X_train.columns, \"Importance\": feature_importances})\nfeature_scores = feature_scores.sort_values(by=\"Importance\", ascending=False)\n\n\n\nTop 3 Features:\n               Feature  Importance\n0   Culmen Length (mm)    0.166972\n2  Flipper Length (mm)    0.139796\n6        Island_Biscoe    0.132696\n\n\nWhen we use the ExtraTreesClassifier function, we are building a random forest from the training set. A random subset of features is used for each tree, then the best features are selected based on an importance score. The importance score is calculated using an impurity score for each feature that captures how well a feature is able to partition the dataset. These scores are then normalized to create the importance score. We can see from the feature selector that the culmen length, flipper length, and the island are the most important features in the dataset. So, we will use these features to train our model.\n\n\nChoosing maximum tree depth\nOne of the parameters for any tree method is the maximum depth at which the tree can go. That is, the number of partitions of the dataset allowed in each tree. In order to find the optimal maximum depth, we will use cross validation to test a range of potential maximum depths:\n\n# The columns that we want to include in our analysis -- based on the highest-scoring features\ncols = [\"Flipper Length (mm)\", \"Culmen Length (mm)\", \"Island_Biscoe\", \"Island_Dream\",\"Island_Torgersen\"]\n\ndepths = range(1, 21)\nmean_scores = []\n\n# Loop through potential depths and use cross validation to score each depth\nfor depth in depths:\n    dt = RandomForestClassifier(max_depth=depth, random_state=42)\n    scores = cross_val_score(dt, X_train[cols], y_train, cv=5, scoring='accuracy')\n    mean_scores.append(np.mean(scores))\n\n# Find the depth with the highest scoring validation\nbest_depth = depths[np.argmax(mean_scores)]\nbest_score = max(mean_scores)\n\n\n\nBest max tree depth: 7\nBest cross-validated accuracy: 0.9843891402714933\n\n\nBased on the cross-validation, a random forest with maximum depth 7 is the best option for our model.\n\n\n\nFitting the model\nNow that we know which features we want to use and have done some work to optimize the parameters for our random forest, we’re ready to train a random forest on our training set.\n\n# Initialize the model and fit it to the train set\ndt_model = RandomForestClassifier(max_depth=5, random_state=42)\ndt_model.fit(X_train[cols], y_train)\n\n# Score the model accuracy to see how well it did \ntrain_accuracy = dt_model.score(X_train[cols], y_train)\n\n\n\nTraining Accuracy: 0.9961\n\n\nThis random forest works pretty well! It was 99% accurate on our testing set. Now that we’ve trained this random forest tree, we can test it on the test dataset.\n\n# Read in the test data\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\ntest[\"Species\"].replace('Adelie Penguin (Pygoscelis adeliae)','Adelie',inplace=True)\ntest[\"Species\"].replace('Chinstrap penguin (Pygoscelis antarctica)','Chinstrap',inplace=True)\ntest[\"Species\"].replace('Gentoo penguin (Pygoscelis papua)','Gentoo',inplace=True)\n\nX_test, y_test = prepare_data(test)\ntest_acc = dt_model.score(X_test[cols], y_test)\n\n\n\nTesting Accuracy: 0.9853\n\n\n\n\nUnderstanding the results\nLet’s get a better understanding of what it’s actually doing. First, we can use the confusion matrix to look at how all of the penguins were classified.\n\ny_test_pred = dt_model.predict(X_test[cols])\nconfusion_matrix(y_test, y_test_pred)\n\narray([[31,  0,  0],\n       [ 0, 11,  0],\n       [ 1,  0, 25]])\n\n\nWhat does this matrix mean? We can understand it using each species as follows:\n\n\nThere were 31 Adelie penguin(s) who were classified as Adelie penguin(s).\nThere were 0 Adelie penguin(s) who were classified as Chinstrap penguin(s).\nThere were 0 Adelie penguin(s) who were classified as Gentoo penguin(s).\nThere were 0 Chinstrap penguin(s) who were classified as Adelie penguin(s).\nThere were 11 Chinstrap penguin(s) who were classified as Chinstrap penguin(s).\nThere were 0 Chinstrap penguin(s) who were classified as Gentoo penguin(s).\nThere were 1 Gentoo penguin(s) who were classified as Adelie penguin(s).\nThere were 0 Gentoo penguin(s) who were classified as Chinstrap penguin(s).\nThere were 25 Gentoo penguin(s) who were classified as Gentoo penguin(s).\n\n\nThis tells us that our model only misclassified one penguin – a Gentoo classified as an Adelie. We can plot the decision regions for each island to get an understanding of why this happened.\n\n\nPlotting Code\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Gentoo\", \"Chinstrap\", \"Adelie\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\n\n\n\n\n\n\n\n\n\n\nThese plots show us how our model made its classifications. On the far left, we can see that for the Biscoe Island data, the model was able to correctly distinguish between the Adelie (blue) and Gentoo (red) penguins using the flipper length. Moving to the far right, on Torgensen Island, there are only Adelie penguins, so the model was able to classify all of them correctly. In the middle plot we can see that the penguins on Dream Island were harder to classify – we can see where the model classified a Chinstrap (green) penguin as an Adelie penguin (blue) because it had a shorter culmen than the other Chinstrap penguins, causing its point to be within the blue region.\n\n\nDiscussion\nWe can see that our random forest was a very accurate model. It only misclassified one of the penguins, giving us 98.5% testing accuracy. We can see from the decision regions that the random forest was able to learn patterns within each island fairly well, even creating some accurate non-linear decision regions. In order to get more accuracy for the Dream island data, we would probably need to use another classification method that could learn the differences between the Chinstrap and Adelie penguins on Dream island. Or, going back to our initial visualizations, we could use an ensemble learning method that first uses one model to separate one species from the other two using the quantitative measurements and another that uses a qualitative feature like the island to distinguish between penguins with similar physical measurements."
  },
  {
    "objectID": "posts/automated-decision-systems/index.html",
    "href": "posts/automated-decision-systems/index.html",
    "title": "Automated Decison Systems",
    "section": "",
    "text": "Abstract\nIn this blog post we’ll be looking at data from a bank. We’ll start with some data exploration and visualization to get a better understanding of the features in the dataset. Our ultimate goal here is to create a decision-making system that will determine whether a prospective borrower should be offered a loan. We will use logistic regression to assign weights to different features in the dataset, which we will then use to give each prospective borrower a “score”. This score will be used to determine whether the borrower will prospective receive a loan based on a threshold that will optimize profits for the bank. After creating this decision-making system, we’ll test it out on a test dataset and do some exploration into who our system predicted would be able to repay their loans, and who it predicted wouldn’t.\n\n# Read in the data\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/train.csv\"\ndf_train = pd.read_csv(url)\n\n\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_home_ownership\nperson_emp_length\nloan_intent\nloan_grade\nloan_amnt\nloan_int_rate\nloan_status\nloan_percent_income\ncb_person_default_on_file\ncb_person_cred_hist_length\n\n\n\n\n0\n25\n43200\nRENT\nNaN\nVENTURE\nB\n1200\n9.91\n0\n0.03\nN\n4\n\n\n1\n27\n98000\nRENT\n3.0\nEDUCATION\nC\n11750\n13.47\n0\n0.12\nY\n6\n\n\n2\n22\n36996\nRENT\n5.0\nEDUCATION\nA\n10000\n7.51\n0\n0.27\nN\n4\n\n\n3\n24\n26000\nRENT\n2.0\nMEDICAL\nC\n1325\n12.87\n1\n0.05\nN\n4\n\n\n4\n29\n53004\nMORTGAGE\n2.0\nHOMEIMPROVEMENT\nA\n15000\n9.63\n0\n0.28\nN\n10\n\n\n\n\n\n\n\n\n\nExploring The Data\nLet’s start with some visualizations to explore patterns in this dataset:\n\n\nPlotting Code\n# Filter out rows with employment length greater than 25 years\ndf_train = df_train[df_train['person_emp_length'] &lt;= 25]\n\n# Bucket the employment length into 5-year chunks\nbins = range(0, 25 + 5, 5)\nlabels = [\"0 to 5 years\", \"5 to 10 years\", \"10 to 15 years\", \"15 to 20 years\", \"20 to 25 years\"]\n\ndf_train['emp_length_bucket'] = pd.cut(\n    df_train['person_emp_length'],\n    bins=bins,\n    right=True,\n    labels=labels,\n    include_lowest=True\n)\n\n# Create a Seaborn plot using histplot with multiple=\"fill\" for normalized (proportional) stacked bars\nplt.figure(figsize=(10, 6))\nsns.histplot(\n    data=df_train,\n    x='emp_length_bucket',\n    hue='loan_intent',\n    multiple='fill',  # normalizes each bar to sum to 1 (proportions)\n    shrink=0.8, \n    palette=\"Set2\"\n)\nplt.title(\"Loan Intent Proportions by Employment Length\")\nplt.xlabel(\"Employment Length\")\nplt.ylabel(\"Proportion\")\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nFigure 1: Loan intent by length of employment\n\n\n\n\n\n\nPlotting Code\n# Plot the distribution of interest rates of each of the home ownership statuses \nsns.boxplot(data=df_train, x=\"person_home_ownership\", y=\"loan_int_rate\", palette=\"Set2\")\nplt.xlabel(\"Home Ownership Status\")\nplt.ylabel(\"Loan Interest Rate\")\n\n\nText(0, 0.5, 'Loan Interest Rate')\nFigure 2: Interest rate distribution by home ownership status\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncredit_history_length\n\n\nloan_grade\n\n\n\n\n\nA\n5.7\n\n\nB\n5.8\n\n\nC\n5.9\n\n\nD\n5.8\n\n\nE\n5.8\n\n\nF\n6.1\n\n\nG\n6.6\n\n\n\n\n\nFigure 3: mean credit history length by loan grade\n\n\nWe’re looking here to understand some of the patterns in this dataset.\nFirst, in Figure 1, we can see that borrowers’ intentions for their loans change a bit as the length of their employment increases. Specifically, after 20 years of employment people stop borrowing to pay for education, which makes sense. People also start taking out more loans for venture and medical uses as their employment goes on. It looks like the proportion of loans that are intended to be used for home improvement, personal reasons, and debt consolidation remain fairly constant across length of employment.\nMoving on to Figure 2, we can see that borrowers with different home ownership statuses (renting, owning, mortgaging or other), have differences in the interest rate they might receive on their loan. Borrowers who rent their home seem more likely to have an interest rate of over 20%, and we can see that borrowers who own their homes rarely see interests rates over 20%. Borrowers who rent and mortgage their homes have more variability in the interest rates they might see, as well.\nFinally, we can see in Figure 3 that loan grades have differences in average length of credit of the borrower. It seems that borrowers with higher loan grades have longer credit histories, on average.\nWe can see that we should be able to get some information about whether a borrower will pay back a loan from some of these features.\n\nPrepping the Data\n\n# Encode the qualitative variables quantitatively\ndef preprocess(df):\n    \"\"\"\n    Function to preprocess the data\n    \"\"\"\n    le_home = LabelEncoder()\n    df[\"person_home_ownership\"] = le_home.fit_transform(df[\"person_home_ownership\"])\n\n    le_intent = LabelEncoder()\n    df[\"loan_intent\"] = le_intent.fit_transform(df[\"loan_intent\"])\n    intent_mapping = dict(zip(le_intent.transform(le_intent.classes_), le_intent.classes_))\n\n    df = pd.get_dummies(df, columns=['cb_person_default_on_file'], prefix='default')\n    df = df.dropna()\n\n    # Change the interest rate column to a percentage\n    df[\"loan_int_rate\"] = df[\"loan_int_rate\"]/100\n\n    #Filter out columns with weird values\n    df = df[(df[\"person_age\"] &lt; 100) & (df[\"person_income\"] &gt; 1000) &  (df[\"loan_int_rate\"] &gt; 0)]\n\n    # Separate our the target variable\n    y = df[\"loan_status\"]\n\n    return df, y, intent_mapping\n\ndf_train_clean, y_train, intent_map = preprocess(df_train)\n\nWe will use this function prepare our datasets for analysis. We’re encoding the qualitative columns for home ownership and loan intent as integers, one-hot encoding the historical default true/false column as 0s and 1s, separating the target variable\n\n\n\nBuilding A Model\n\nChoosing Features\nFirst, let’s figure out which features we want to use for the model. We’ll generate a few new features that we can use to better understand the relationships between our features: - Credit history ratio: \\(\\frac{\\text{credit history length}}{\\text{age}}\\) - Employment stability: \\(\\frac{\\text{employment length}}{\\text{age}}\\) - Income stability: \\(\\text{income}*\\text{employment length}\\)\nWe’ll use cross validation to test a few combinations of features: 1. Employment stability, income, and percentage of income that the loan is 2. Loan intent, credit history ratio, home ownership status 3. income stability, loan interest rate, age\n\n# Create new features \ndef new_features(df):\n    df[\"credit_history_ratio\"] = round(df[\"cb_person_cred_hist_length\"]/df[\"person_age\"],2)\n    df[\"employment_stability\"] = round(df[\"person_emp_length\"]/df[\"person_age\"], 2)\n    df[\"income_stability\"] = round(df[\"person_income\"]*df[\"person_emp_length\"],2)\n    return df\ndf_train_clean = new_features(df_train_clean)\n\n\n# Define three different sets of features\nfeature_sets = {\n    \"Set 1\": [\"employment_stability\", \"person_income\", \"loan_percent_income\"],\n    \"Set 2\": [\"loan_intent\", \"credit_history_ratio\", \"person_home_ownership\"],\n    \"Set 3\": [\"income_stability\", \"loan_int_rate\", \"person_age\"]\n}\n\nmodel = LogisticRegression()\n\n# Evaluate each feature set\nresults = {}\nfor set_name, features in feature_sets.items():\n    X_train = df_train_clean[features]\n    X_train = StandardScaler().fit_transform(X_train)  # Standardize features \n    \n    scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')\n    mean_accuracy = np.mean(scores)\n    results[set_name] = mean_accuracy\n    print(f\"{set_name}: Mean Accuracy = {mean_accuracy:.4f}\")\n\nSet 1: Mean Accuracy = 0.8246\nSet 2: Mean Accuracy = 0.7850\nSet 3: Mean Accuracy = 0.8095\n\n\n\n\nEstimating weights\nAfter fitting an LR to each of the feature sets and using cross validation to test how accurate the models are, we can see that our first set of features, which includes employment stability, income, and percentage of income that the loan is has the highest average accuracy. Now, we’ll fit an LR model to these features to obtain our vector of weights that we’ll use in our scoring function.\n\nweights_model = LogisticRegression()\nX_train = df_train_clean[feature_sets[\"Set 1\"]]\nX_train = StandardScaler().fit_transform(X_train)\nweights_model.fit(X_train, y_train)\n\nfor feature, coef in zip(feature_sets[\"Set 1\"], weights_model.coef_[0]):\n    print(f\"{feature}: {coef:.4f}\")\n\nemployment_stability: -0.1482\nperson_income: -0.4953\nloan_percent_income: 0.7925\n\n\nNow we can use these coefficients in our as the values in our weight vector when we create a scoring function, which we’ll do next.\n\n\n\nCreating the score function\nOur next step is to give each borrower a score, using the features and weights that we decided on above. We’ll add a “score” column to the train set that has the result of our score function:\n\\(\\text{score} = -0.1482 * x_1 - 0.4953 * x_2 + 0.7925 * x_3\\)\nwhere\n\\(x_1\\) = employment_stability\n\\(x_2\\) = person_income\n\\(x_3\\) = loan_percent_income\n\ndef score_dataframe(df):\n    \"\"\"\n    Compute the score for each borrower using the weights \n    determined by the logistic regression\n\n    inputs:\n    df: Pandas DataFrame with all features used to generate weights\n    and loan status, amount, and interest rate for profit calculations\n    \"\"\"\n    df_scored = df.copy()\n    df_scored[[\"employment_stability\", \"person_income\", \"loan_percent_income\"]] = StandardScaler().fit_transform(df_scored[[\"employment_stability\", \"person_income\", \"loan_percent_income\"]])\n    df_scored[\"score\"] =  round(-0.1486 * df_scored[\"employment_stability\"] - 0.6252 * df_scored[\"person_income\"] + 0.7925 * df_scored[\"loan_percent_income\"],2)\n    return df_scored\n\nNow, we will write a function that will compute the expected profit from each prospective borrower. If a prospective borrower has a score greater than the threshold, our system will predict that this borrower will default on their loan. So, we will not give them a loan, making their profit contribution 0. For users that we predict will not default on their loan, we have two cases. If they repay their loan, their profit will be determined by:\n\\(\\text{profit = (loan amount)} * (1 + 0.25* \\text{(loan interest rate)})^{10} - \\text{(loan amount)}\\)\nIf the borrowed does not repay their loan, the bank will “profit” according to this equation:\n\\(\\text{profit = (loan amount)} * (1 + 0.25* \\text{(loan interest rate)})^{3} - 1.7 * \\text{(loan amount)}\\)\nThis function will apply the profit equation to each borrower based on their predicted loan status, and we will calculate the expected profit per borrower by dividing by the number of prospective borrowers.\n\ndef compute_profit(threshold, df):\n    \"\"\"\n    Compute the profit that the bank would have made if they had\n    used the predictions determined by our score function\n    and threshold\n\n    inputs:\n    threshold: integer value for score threshold \n    df: Pandas DataFrame with loan amount, interest rate, actual loan status, and score from \n    scoring function\n    \"\"\"\n    df[\"pred_default\"] = df[\"score\"].apply(lambda x: 1 if x &gt; threshold else 0)\n    \n    # Calculate potential profit for repayment or defaulting \n    df[\"repaid_profit\"] = round(df[\"loan_amnt\"] * (1 + 0.25 * df[\"loan_int_rate\"])**10 - df[\"loan_amnt\"],2)\n    df[\"defaulted_profit\"] = round(df[\"loan_amnt\"] * (1 + 0.25 * df[\"loan_int_rate\"])**3 - 1.7 * df[\"loan_amnt\"],2)\n    \n    # Profit based on actual loan status versus predicted status\n    df[\"profit\"] = 0  # Default to zero profit\n    df.loc[(df[\"loan_status\"] == 0) & (df[\"pred_default\"] == 0), \"profit\"] = df[\"repaid_profit\"]  # Successful repayment\n    df.loc[(df[\"loan_status\"] == 1) & (df[\"pred_default\"] == 0), \"profit\"] = df[\"defaulted_profit\"]  # Incorrectly predicted as good loan, default\n    df.loc[(df[\"loan_status\"] == 1) & (df[\"pred_default\"] == 1), \"profit\"] = 0  # Correctly predicted as default, no profit\n    df.loc[(df[\"loan_status\"] == 0) & (df[\"pred_default\"] == 1), \"profit\"] = 0  # Incorrectly predicted as default, no profit\n\n    return sum(df[\"profit\"]) / len(df)\n\n\n# Find optimal threshold\nscored_df = score_dataframe(df_train_clean)\nthresholds = np.arange(0,3,0.05)\nprofits = [compute_profit(t, scored_df) for t in thresholds]\n\n# Find best threshold\noptimal_threshold = thresholds[np.argmax(profits)]\noptimal_profit = max(profits)\nprint(f\"Optimal Threshold for Maximum Profit: {optimal_threshold:.4f}\")\nprint(f\"Maximum profit per prospective borrower: {round(optimal_profit,2):.4f}\")\n\n\n# Plot profit vs. threshold\nsns.set_style(\"whitegrid\")\nsns.set_palette(\"Set2\")\n\nplt.figure(figsize=(8, 5))\nplt.scatter(optimal_threshold, optimal_profit, color=sns.color_palette(\"Set2\")[1], label=f'Optimal t={optimal_threshold:.4f}', zorder=2)\nsns.lineplot(x=thresholds, y=profits, label=\"Profit\", zorder=1)\nplt.xlabel(\"Threshold\")\nplt.ylabel(\"Profit per borrower\")\nplt.title(\"Threshold vs Profit\")\nplt.legend()\nplt.show()\n\nOptimal Threshold for Maximum Profit: 1.1500\nMaximum profit per prospective borrower: 1316.4800\n\n\n\n\n\n\n\n\n\n\n\nEvaluating from the bank’s perspective\nNow, we want to look at how this decision making system works on a test dataset. We’ll apply all of the functions we used on the training set with the test set, and see what the profit per borrower will look like using the threshold from above, 1.15.\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/test.csv\"\ndf_test = pd.read_csv(url)\ndf_test, y_test, intent_map = preprocess(df_test)\ndf_test = new_features(df_test)\n    \nscored_test_df = score_dataframe(df_test)\nprint(\"profit per prospective borrower:\", round(compute_profit(1.15, scored_test_df),2))\n\nprofit per prospective borrower: 1230.42\n\n\nIt looks like we ended up with a fairly similar profit per borrower on the test set to what we got from the train set, $1316 per borrower on the test set and $1230 on the train set. From the bank’s perspective, this seems positive. If they are making, on average, $1230 per borrower, they would make around $6,000,000 off of this dataset of borrowers.\n\n\nEvaluating from the borrower’s perspective\nNow, let’s try to get a better understanding of who in the test dataset we are predicting is going to default on their loans, and who we are predicting is going to repay them. We’ll look at the following questions:\n\nIs it more difficult for people in certain age groups to access credit under your proposed system?\nIs it more difficult for people to get loans in order to pay for medical expenses? How does this compare with the actual rate of default in that 3. group? What about people seeking loans for business ventures or education?\nHow does a person’s income level impact the ease with which they can access credit under your decision system?\n\n\nQuestion 1\n\n# Split the borrower ages into 4 buckets and see \nscored_test_df[\"age_group\"] = pd.qcut(scored_test_df[\"person_age\"], q=4, labels=[\"Young\", \"Middle-aged\", \"Older\", \"Senior\"])\nage_access = scored_test_df.groupby(\"age_group\")[\"pred_default\"].mean()\nage_access\n\nage_group\nYoung          0.171756\nMiddle-aged    0.121554\nOlder          0.142263\nSenior         0.134460\nName: pred_default, dtype: float64\n\n\nThese probabilities are the mean of the prediction column, so they tell us the rate at which a 1 is predicted in each of the age buckets using our decision making system. That is, they tell us the loan rejection rate for each age group. We can see that younger people are mich more likely to be rejected for a loan than middle-aged and older people are, with a 17% rejection rate as opposed to a 12-14% rejection rate for older groups.\n\n\nQuestion 2\n\n# Calculate rejection rates and actual default rates for loan intent\nloan_intent_access = scored_test_df.groupby(\"loan_intent\")[\"pred_default\"].mean()\nloan_intent_default_rates = scored_test_df.groupby(\"loan_intent\")[\"loan_status\"].mean()\n\nloan_intent_df = pd.DataFrame({\n    \"Loan Intent (Decoded)\": loan_intent_access.index.map(intent_map),\n    \"Rejection Rate\": loan_intent_access.values,\n    \"Actual Default Rate\": loan_intent_default_rates.values\n})\n\n\n\n\n\n\n\n\n\n\nLoan Intent (Decoded)\nRejection Rate\nActual Default Rate\n\n\n\n\n0\nDEBTCONSOLIDATION\n0.150442\n0.287611\n\n\n1\nEDUCATION\n0.150510\n0.167517\n\n\n2\nHOMEIMPROVEMENT\n0.095779\n0.250000\n\n\n3\nMEDICAL\n0.164958\n0.284250\n\n\n4\nPERSONAL\n0.146293\n0.220441\n\n\n5\nVENTURE\n0.127593\n0.146266\n\n\n\n\n\n\n\nWe’re interested in the loans that are intended for medical uses. We can see that medical loans have the highest rate of rejection from our system (20% of medical loan requests are rejected), but they also have one of the highest rates of actual default (28%). Venture and business loans have lower rates of rejection and their actual default rates are closer to their rejection rates.\n\n\nQuestion 3\n\nscored_test_df[\"income_group\"] = pd.qcut(scored_test_df[\"person_income\"], q=4, labels=[\"Low\", \"Lower-middle\", \"Upper-middle\", \"High\"])\nincome_access = scored_test_df.groupby(\"income_group\")[\"pred_default\"].mean()\n\n\n\nincome_group\nLow             0.342638\nLower-middle    0.168044\nUpper-middle    0.052274\nHigh            0.004370\nName: pred_default, dtype: float64\n\n\nHere, we are again looking at the loan rejection rates, this time by income group. It is clear that people with lower incomes are much more likely to be rejected using our decision making system (41% of the time), as opposed to high income people who are only rejected 0.8% of the time with our system.\n\n\n\nWrite and Reflect\nWe found from our model and evaluation that when we choose a threshold that optimizes the bank’s profit, it can result in certain groups having a harder time obtaining a loan. We saw that age, income, and loan intent groups have differences rejection rates all with our decision making system. We can also see that optimizing profits for a bank requires turning a way a significant chunk of loan applications, because the reality is that many people do end up defaulting on their loans. In our decision-making process, we only considered 3 features of a person’s application, but we found that these features correlated with certain groups having a harder time receiving a loan. This is a great example of a feature that is implicitly encoding information about another category. For example, a person’s income could be correlated with what they are planning to use the loan for. Even though loan intent wasn’t one of the features of our score function, we could see that different loan intents ended up with different rates of rejection in our system. So, is this system fair? We’ll explore that by answering this question: Considering that people seeking loans for medical expense have high rates of default, is it fair that it is more difficult for them to obtain access to credit?\nFirst, let’s define fairness. I will define fairness in deciding who should get a loan of a combination of how much the person needs the loan and how likely it is that they will pay the loan back. If we are only basing fairness on how much the person needs the loan, we would likely say that people who need loans for medical reasons should all receive their loans. But, is this fair to the bank? They would end up losing a lot of money because people who take out loans for medical expenses often default on them. If we are only basing fairness on how likely it is that they will pay the loan back, most people seeking medical loans probably shouldn’t get them, because they do often default. But, in this case, very few people would receive medical loans, which seems unfair because they may be able to pay them back, even if people haven’t historically paid medical loans back at a high rate. There are also some other confounding variables to consider, like whether people seeking medical loans are more likely to come from a historically marginalized group that might have a harder time proving they can pay a loan back or a smaller income on average than other groups? It may not be fair for these people to be subject to high rejection rates just because of their identity.\nUltimately, Looking at the rate of rejection versus the true rate of default for medical loans, I think it’s fair that it may be more difficult for people to access these loans. The rate of default on these loans is much higher than the rate of default for loans with other intended purposes, so it seems that people who are seeking these loans may need to provide additional proof that they would be able to pay the loan back. It’s fair for the bank to be skeptical. However, we can see that in our system, 80% of people seeking medical loans are being accepted. So it isn’t the case that it’s impossible for these people, who probably really need the loan, to obtain it. Therefore, I think it’s fair that it is more difficult for people seeking medical loans to obtain access to credit given that they default much more frequently."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Implementing Logistic Regression\n\n\n\n\n\nImplementing Logistic Regression with Gradient Descent\n\n\n\n\n\nApr 2, 2025\n\n\nCarly McAdam\n\n\n\n\n\n\n\n\n\n\n\n\nLimits of the Quantitative Approach to Bias and Fairness\n\n\n\n\n\nEssay: Limits of the Quantitative Approach to Bias and Fairness\n\n\n\n\n\nMar 16, 2025\n\n\nCarly McAdam\n\n\n\n\n\n\n\n\n\n\n\n\nReplication Study\n\n\n\n\n\nReplication Study: Dissecting racial bias in an algorithm used to manage the health of populations\n\n\n\n\n\nMar 12, 2025\n\n\nCarly McAdam\n\n\n\n\n\n\n\n\n\n\n\n\nAuditing Bias\n\n\n\n\n\nAuditing for gender bias in income predictions\n\n\n\n\n\nMar 6, 2025\n\n\nCarly McAdam\n\n\n\n\n\n\n\n\n\n\n\n\nAutomated Decison Systems\n\n\n\n\n\nDesign and Impact of Automated Decision Systems\n\n\n\n\n\nFeb 27, 2025\n\n\nCarly McAdam\n\n\n\n\n\n\n\n\n\n\n\n\nClassifying Palmer Penguins\n\n\n\n\n\nClassifying Palmer Penguins\n\n\n\n\n\nFeb 13, 2025\n\n\nCarly McAdam\n\n\n\n\n\n\n\n\n\n\n\n\nSecond Post\n\n\n\n\n\nA new blog post that I just made!\n\n\n\n\n\nMar 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\n\n\n\n\n\n\nTimnit Gebru\n\n\n\n\n\nA new blog post that I just made!\n\n\n\n\n\nMar 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\n\n\n\n\n\n\nHello Blog\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  }
]