[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is Carly’s CSCI 451 blog"
  },
  {
    "objectID": "posts/new-new-test-post/index.html",
    "href": "posts/new-new-test-post/index.html",
    "title": "Timnit Gebru",
    "section": "",
    "text": "from source import Perceptron\np = Perceptron()\n\nI did it!!\nnot implemented\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-new-test-post/index.html#math",
    "href": "posts/new-new-test-post/index.html#math",
    "title": "Timnit Gebru",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/new-test-post/index.html",
    "href": "posts/new-test-post/index.html",
    "title": "Second Post",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-test-post/index.html#math",
    "href": "posts/new-test-post/index.html#math",
    "title": "Second Post",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "from source import Perceptron\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/auditing-bias/index.html",
    "href": "posts/auditing-bias/index.html",
    "title": "Auditing Bias",
    "section": "",
    "text": "Abstract\nIn this blog post we’ll be looking at census data and trying to predict whether a person will have an income over $50,000. We will not use a persons’ sex to predict their income. After training and implementing this algorithm, we will audit our model for gender bias. We will see that although the model doesn’t appear to perform too differently across male and female groups, there are some differences in the model performance, which we will explore further and discuss.\n\n\nPulling and Prepping the Data\n\n# Read in the data\n\nSTATE = \"CA\"\n\ndata_source = ACSDataSource(survey_year='2018', \n                            horizon='1-Year', \n                            survey='person')\n\nacs_data = data_source.get_data(states=[STATE], download=True)\nacs_data=acs_data[['PINCP', 'AGEP', 'SCHL', 'MAR', 'RELP', 'DIS', 'ESP', 'CIT', 'MIG', 'MIL', 'ANC', 'NATIVITY', 'DEAR', 'DEYE', 'DREM', 'SEX', 'RAC1P', 'ESR']]\n\nacs_data.head()\n\n\n\n\n\n\n\n\nPINCP\nAGEP\nSCHL\nMAR\nRELP\nDIS\nESP\nCIT\nMIG\nMIL\nANC\nNATIVITY\nDEAR\nDEYE\nDREM\nSEX\nRAC1P\nESR\n\n\n\n\n0\n48500.0\n30\n14.0\n1\n16\n2\nNaN\n1\n3.0\n4.0\n1\n1\n2\n2\n2.0\n1\n8\n6.0\n\n\n1\n0.0\n18\n14.0\n5\n17\n2\nNaN\n1\n1.0\n4.0\n1\n1\n2\n2\n2.0\n2\n1\n6.0\n\n\n2\n13100.0\n69\n17.0\n1\n17\n1\nNaN\n1\n1.0\n2.0\n2\n1\n2\n2\n2.0\n1\n9\n6.0\n\n\n3\n0.0\n25\n1.0\n5\n17\n1\nNaN\n1\n1.0\n4.0\n1\n1\n1\n2\n1.0\n1\n1\n6.0\n\n\n4\n0.0\n31\n18.0\n5\n16\n2\nNaN\n1\n1.0\n4.0\n1\n1\n2\n2\n2.0\n2\n1\n6.0\n\n\n\n\n\n\n\n\nfeatures_to_use = ['AGEP', 'SCHL', 'MAR', 'RELP']\n\nFirst, we’re reading in the census data. We’re also going to select some features that we’re going to use for our mode. We’ll be predicting income, so I chose some features that I think will be relevant – age, education level, marriage status, and relationship to “head of household”. Now, ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍we’ll ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍construct ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍a ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍BasicProblem that expresses that we are going to use the features we specified above to predict if a person’s income (“PINCP”) is over $50,000.\n\nIncomeProblem = BasicProblem(\n    features=features_to_use,\n    target='PINCP',\n    target_transform=lambda x: x &gt; 50000,\n    group='SEX',\n    preprocess=lambda x: x,\n    postprocess=lambda x: np.nan_to_num(x, -1),\n)\nfeatures, label, group = IncomeProblem.df_to_numpy(acs_data)\n\n\n\nBasic Descriptives\nBefore we get into creating our model, let’s get to know our dataset so that we understand the demographics that we’re working with. We’ll split our dataset into train and test sets.\n\nX_train, X_test, y_train, y_test, group_train, group_test = train_test_split(\n    features, label, group, test_size=0.2, random_state=0)\n\nThis is what our dataset looks like. It has all of the features that we’re going to use to train our model, and it has a “group” column where 1 represents a male and 2 represents a female. The “label” column, which we will isolate as our target variable, is a boolean (true for a person with income &gt; $50,000, false otherwise).\n\n\n\n\n\n\n\n\n\nAGEP\nSCHL\nMAR\nRELP\ngroup\nlabel\n\n\n\n\n0\n77.0\n16.0\n2.0\n0.0\n2\nTrue\n\n\n1\n29.0\n20.0\n5.0\n2.0\n1\nFalse\n\n\n2\n60.0\n21.0\n1.0\n0.0\n2\nFalse\n\n\n3\n27.0\n19.0\n5.0\n13.0\n2\nFalse\n\n\n4\n63.0\n23.0\n3.0\n0.0\n1\nTrue\n\n\n\n\n\n\n\nNow, let’s get some information about the data:\n\n\nNumber of individuals in the training set: 303053\n\n\n\n\nNumber of individuals in the training set with income over $50,000: 73963\n\n\n\n\nNumber of males in the training set: 149294\nNumber of females in the training set: 153759\n\n\n\n\nMale proportion w/ income over $50,000: 0.3\nFemale proportion w/ income over $50,000: 0.19\n\n\nWe can see that there are more females than males in the training set, although not by much. It’s almost evenly split. There are also about 10% more males than females have an income of over $50,000. It’s important to understand the breakdown in our dataset because if we’re looking for a model to perform well for both groups, we want there to be enough data in the training set for each group. We also want to be sure that we have people from each group with income above and below the threshold, so that the model can learn patterns about people in each group with each label. Now, let’s look at intersectional trends in the data by breaking out the income groups by sex and education level.\n\n\nPlotting code\n# Compute proportions of positive labels by both SEX and SCHL\nintersectional_counts = df.groupby([\"group\", \"SCHL\"])[\"label\"].mean().reset_index()\n\n# Rename columns for clarity\nintersectional_counts.columns = [\"Sex\", \"Education Level\", \"Proportion with Positive Label\"]\n\n# Create a bar plot\nplt.figure(figsize=(12, 6))\nsns.barplot(\n    data=intersectional_counts, \n    x=\"Education Level\", \n    y=\"Proportion with Positive Label\", \n    hue=\"Sex\"\n)\n\n# Labels and title\nplt.xlabel(\"Education Level (SCHL)\")\nplt.ylabel(\"Proportion with Positive Label (e.g., Income &gt; $50k)\")\nplt.title(\"Proportion of Positive Labels by Sex and Education Level\")\nplt.legend(title=\"Sex\")\n\n# Rotate x-axis labels for better readability\nplt.xticks(rotation=45)\n\n# Show plot\nplt.show()\n\n\n\n\n\n\n\n\n\nHere, we can see that across all education levels, men are more likely to have an income over $50,000 than women are. We can also see that this gap seems to be exacerbated at lower education levels. There is a bigger gap between the proportion of men and proportion of women with income over $50,000 at 15-19 years of education than at the higher education levels of 21-24 years. This is an important example of intersectional trends, because we can see that the income disparity between men and women varies depending on education level.\n\n\nTraining the Model\nNow that we understand our dataset, we’ll train a support vector machine model to predict if an individual’s income is over $50,000. An SVM model aims to find an optimal hyperplane in an N-dimensional space to separate data points into different classes. It is similar to a linear classification model, but it can be in multiple dimensions. The SVM algorithm maximizes the margin between the closest points of different classes. One of the parameters in an SVM model is the regularization term “C”, which is used to balance the misclassification penalties with maximization. If the value of “C” is higher, the model more strictly penalizes misclassifications. Before training our model, we will use cross-validation to find an optimal value of this “C” parameter.\n\n\nCross-val code\nmodel = make_pipeline(StandardScaler(), LinearSVC(dual=False))\nmodel.fit(X_train, y_train)\n\n# Define the pipeline\npipeline = make_pipeline(StandardScaler(), LinearSVC(dual=False, max_iter=5000))\n\n# Define the hyperparameter grid for C\nparam_grid = {'linearsvc__C': [0.001, 0.01, 0.1, 1, 10, 100]}\n\n# Perform Grid Search with Cross-Validation\ngrid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\ngrid_search.fit(X_train, y_train)\n\n# Get the best C value\nbest_C = grid_search.best_params_['linearsvc__C']\nprint(f\"Best C: {best_C}\")\n\n# Best model with optimal C\nbest_model = grid_search.best_estimator_\n\n\nBest C: 0.001\n\n\nBased on our cross-validation, the value of “C” that we should use is 0.001. Now, we’ll create and train our model using a pipeline. The pipeline is used to sequentially apply transformations – here, we are using StandardScaler() to standardize all of the features in the dataset.\n\nmodel = make_pipeline(StandardScaler(), LinearSVC(C=0.001, dual=False))\nmodel.fit(X_train, y_train)\n\nPipeline(steps=[('standardscaler', StandardScaler()),\n                ('linearsvc', LinearSVC(C=0.001, dual=False))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('standardscaler', StandardScaler()),\n                ('linearsvc', LinearSVC(C=0.001, dual=False))])StandardScalerStandardScaler()LinearSVCLinearSVC(C=0.001, dual=False)\n\n\nNow, that we’ve fit our model, we’re going to test it on the test set and then audit the model for gender bias. Recall that we didn’t include sex as one of the features in our model, so we’re interested to see if the model we trained on other features might be implicitly biased by sex.\n\n\nAuditing the Model\n\ny_hat = model.predict(X_test)\n\n\n\nOverall accuracy: 0.8\n\n\n\n\nAccuracy for men: 0.79\n\n\n\n\nAccuracy for women: 0.81\n\n\nAt first glance, we can see that our model is slightly more accurate for women, but it performs fairly similarly across each group when looking at accurate. But, let’s get more precise to understand more about the differences in accuracy between groups. We’ll get the accuracy using the accuracy score function, the PPV using the precision score function, and the true positive, true negative, false negative, and false positive rates from the confusion matrix. We’ll get all of these metrics for the whole test set, as well as the test set filtered for the female and male groups. We will put all of these metrics into a dictionary for easy access when we plot these results next.\n\n\nCode to calculate accuracy and error rates\n# Make predictions\ny_pred = best_model.predict(X_test)\n\n### Overall Measures ###\n# Compute overall accuracy\noverall_accuracy = accuracy_score(y_test, y_pred)\n\n# Compute overall PPV (Positive Predictive Value / Precision)\noverall_ppv = precision_score(y_test, y_pred)\n\n# Compute overall confusion matrix\ntn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n\n# Compute overall False Positive Rate (FPR) and False Negative Rate (FNR)\noverall_fpr = fp / (fp + tn)\noverall_fnr = fn / (fn + tp)\n\n# Print overall metrics\nprint(\"\\n--- Overall Model Performance ---\")\nprint(f\"Overall Accuracy: {overall_accuracy:.2f}\")\nprint(f\"Overall PPV: {overall_ppv:.2f}\")\nprint(f\"Overall False Positive Rate (FPR): {overall_fpr:.2f}\")\nprint(f\"Overall False Negative Rate (FNR): {overall_fnr:.2f}\")\n\n### By-Group Measures ###\nprint(\"\\n--- Subgroup Model Performance ---\")\nsubgroup_results = []\n\nfor sex in [1, 2]:  # 1 = Male, 2 = Female\n    mask = (group_test == sex)\n    \n    # Compute subgroup accuracy\n    subgroup_accuracy = accuracy_score(y_test[mask], y_pred[mask])\n    \n    # Compute subgroup PPV\n    subgroup_ppv = precision_score(y_test[mask], y_pred[mask])\n    \n    # Compute subgroup confusion matrix\n    tn_s, fp_s, fn_s, tp_s = confusion_matrix(y_test[mask], y_pred[mask]).ravel()\n\n    # Compute subgroup FPR and FNR\n    subgroup_fpr = fp_s / (fp_s + tn_s)\n    subgroup_fnr = fn_s / (fn_s + tp_s)\n    \n    group_name = \"Male\" if sex == 1 else \"Female\"\n    print(f\"\\nGroup: {group_name}\")\n    print(f\"  Accuracy: {subgroup_accuracy:.2f}\")\n    print(f\"  PPV: {subgroup_ppv:.2f}\")\n    print(f\"  False Positive Rate (FPR): {subgroup_fpr:.2f}\")\n    print(f\"  False Negative Rate (FNR): {subgroup_fnr:.2f}\")\n\n    subgroup_results.append({\n        \"Sex\": group_name,\n        \"Accuracy\": subgroup_accuracy,\n        \"PPV\": subgroup_ppv,\n        \"FPR\": subgroup_fpr,\n        \"FNR\": subgroup_fnr\n    })\n\n### Bias Measures ###\n# Calibration: The model is considered calibrated if P(Y=1 | S=1) ≈ P(Y=1 | S=2)\ncalibration_diff = abs(y_pred[group_test == 1].mean() - y_pred[group_test == 2].mean())\n\n# Error Rate Balance: FNR and FPR should be similar across groups\nerror_rate_balance = abs(subgroup_results[0][\"FNR\"] - subgroup_results[1][\"FNR\"]) + \\\n                     abs(subgroup_results[0][\"FPR\"] - subgroup_results[1][\"FPR\"])\n\n# Statistical Parity: P(Y=1 | S=1) ≈ P(Y=1 | S=2)\nstatistical_parity_diff = abs(np.mean(y_pred[group_test == 1]) - np.mean(y_pred[group_test == 2]))\n\n# Print Bias Metrics\nprint(\"\\n--- Bias Metrics ---\")\nprint(f\"Calibration Difference: {calibration_diff:.2f}\")\nprint(f\"Error Rate Balance: {error_rate_balance:.2f}\")\nprint(f\"Statistical Parity Difference: {statistical_parity_diff:.2f}\")\n\n\n\n--- Overall Model Performance ---\nOverall Accuracy: 0.80\nOverall PPV: 0.63\nOverall False Positive Rate (FPR): 0.08\nOverall False Negative Rate (FNR): 0.56\n\n--- Subgroup Model Performance ---\n\nGroup: Male\n  Accuracy: 0.79\n  PPV: 0.75\n  False Positive Rate (FPR): 0.06\n  False Negative Rate (FNR): 0.56\n\nGroup: Female\n  Accuracy: 0.81\n  PPV: 0.50\n  False Positive Rate (FPR): 0.10\n  False Negative Rate (FNR): 0.57\n\n--- Bias Metrics ---\nCalibration Difference: 0.01\nError Rate Balance: 0.05\nStatistical Parity Difference: 0.01\n\n\nThese results tell us more information about the difference in model performance between the two groups. We can see that the male group has a lower false positive rate (FPR) than the female group, that is, our model is less likely to incorrectly predict that a man has income over $50,000 when he actually does not. The false negative rate (FNR) is very similar across groups, so we are pretty much equally likely to predict that a male or female does not have income over $50,000 when they actually do. The male group has a higher PPV value than the female group, which we’ll explore further in the plot below. We will reproduce a plot from the COMPAS paper that compares the FPR and FNR rates across groups and plots potential combinations of FPR/FNR rates for each group. The plot also looks at the potential combinations of FPR/FNR rates for the female group if we fix the PPV rate for the female group to the larger PPV value of the male group.\n\n\nCode to reproduce plot from COMPAS paper\n# Extract values for male and female\np_male = np.mean(y_test[group_test == 1])  # Prevalence of male\np_female = np.mean(y_test[group_test == 2])  # Prevalence of female\n\nPPV_female = subgroup_results[1][\"PPV\"] # Observed PPV for females\nPPV_male_fixed = PPV_female # Set PPV_male to PPV_female\n\n# Extract FNR and FPR values for observed points\nfnr_values = [subgroup_results[0][\"FNR\"], subgroup_results[1][\"FNR\"]]\nfpr_values = [subgroup_results[0][\"FPR\"], subgroup_results[1][\"FPR\"]]\nlabels = [subgroup_results[0][\"Sex\"], subgroup_results[1][\"Sex\"]]\n\n# Generate range of FNR values from 0 to 1\nfnr_range = np.linspace(0, 1, 100)\n\n# Compute feasible FPR values for males\nfpr_feasible_male = ((1 - fnr_range) * p_male * (1 - PPV_male_fixed)) / (PPV_male_fixed * (1 - p_male))\n\n# Compute feasible FPR values for females when PPV_female = PPV_male\nfpr_feasible_female = ((1 - fnr_range) * p_female * (1 - PPV_female)) / (PPV_female * (1 - p_female))\n\n# Define range of delta values for shading\ndelta_values = [0.05, 0.1, 0.15]  # Smaller delta -&gt; smaller feasible region\n\nplt.figure(figsize=(10, 6))\n\n# Scatter plot of observed values\nsns.scatterplot(x=fnr_values, y=fpr_values, hue=labels, palette=[\"lightblue\", \"lightpink\"], s=100)\n\n# Plot feasible set for males\nplt.plot(fnr_range, fpr_feasible_male, color=\"lightblue\", label=\"Feasible Set (Male)\")\n\n# Plot feasible set for females when PPV_female = PPV_male\nplt.plot(fnr_range, fpr_feasible_female, color=\"lightpink\", label=\"Feasible Set (Female, PPV=PPV_male)\")\n\n# Add nested shaded regions for varying PPV_female constraints\nfor i, delta in enumerate(delta_values):\n    # PPV_female_upper = PPV_male + delta\n    # PPV_female_lower = PPV_male - delta\n    \n    PPV_male_upper = PPV_female + delta\n    PPV_male_lower = PPV_female - delta\n\n\n    # Compute feasible FPR values for upper and lower PPV bounds\n    fpr_feasible_upper = ((1 - fnr_range) * p_male * (1 - PPV_male_upper)) / (PPV_male_upper * (1 - p_male))\n    fpr_feasible_lower = ((1 - fnr_range) * p_male * (1 - PPV_male_lower)) / (PPV_male_lower * (1 - p_male))\n\n    # Fill region between upper and lower bounds\n    plt.fill_between(fnr_range, fpr_feasible_lower, fpr_feasible_upper, color=\"lightgrey\", alpha=0.3)\n\n# Labels and title\nplt.xlabel(\"False Negative Rate (FNR)\")\nplt.ylabel(\"False Positive Rate (FPR)\")\nplt.title(\"Feasible (FNR, FPR) combinations\")\n\n# Show plot\nplt.show()\n\n\n\n\n\n\n\n\n\nThis chart is a reproduction of the Chouldechova paper that we discussed in class. We fixed ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the proportion of true positive labels for eac‍‍‍‍‍‍‍‍‍‍‍‍‍h group, and we set the PPV values to be equal across both groups. The pink and blue points show the observed FPR and FNR rates for the male and female groups. The lines represent the possible combinations of FPR and FNR for each group, with the PPV for the male group fixed at the lower PPV that we observed for the female group. Given this, we can see that if we wanted the male FPR to be the same as the female FPR, we would need to increase the false negative rate around 35% so that the blue point would shift right onto the blue line. This tells us that we would need to make the model for males much less accurate than it is now to make the models “equal” in their FPR.\n\n\nConcluding Discussion\nUltimately, this model didn’t exhibit that much bias. Even though the male group had a higher PPV, the accuracy, FPRs, and FNRs were similar across the two groups. There are many people that might be interested in this model, like people doing credit risk assessment, recruiters and employers, or banks. These institutions would all have an interest in predicting how much income an individual might make. At a large scale, this model might be slightly more accurate for predicting male income. This could potentially have a negative impact on females because they might be more likely to be denied credit or a job if their income is predicted to be lower than it actually is. This could lead to biased employment practices, unfair denial of services, or housing discrimination. On the positive side, this model could have the impact of streamlining financial services and job-matching or aiding governments in economic research and policy planning with access to income data. I didn’t feel that this model displays ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍problematic ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍bias. The error rates were very similar across groups, but I could see the potential for a model like this to be biased against women, given the systemic wage gap and sexism inherent in our society. Some of the other problems ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍associated with deploying this model, like privacy issues and regulatory compliance. To mitigate these potential impacts, it would be useful to conduct bias audits at a large scale, like the one we did here. It could also be important to ensure that humans, instead of the algorithm, end up having the final say about decisions that might be impacted by the model results."
  },
  {
    "objectID": "posts/palmer-penguins/index.html",
    "href": "posts/palmer-penguins/index.html",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "Abstract\nIn this blog post we are going to train a random forest model to classify penguin species based on three features in the Palmer Penguins dataset. We’ll start with some visualizations to get an understanding of the different features in the dataset, and then we will use a random forest to choose the best three features to train our model on. After training and testing the model, we will explore how well the model did and how it made its classifications using the confusion matrix and plotting decision regions.\n\n\nPreparing the Data\n\n# Read in the data\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\n\nle = LabelEncoder()\ntrain[\"Species\"].replace('Adelie Penguin (Pygoscelis adeliae)','Adelie',inplace=True)\ntrain[\"Species\"].replace('Chinstrap penguin (Pygoscelis antarctica)','Chinstrap',inplace=True)\ntrain[\"Species\"].replace('Gentoo penguin (Pygoscelis papua)','Gentoo',inplace=True)\nle.fit(train[\"Species\"])\n\n# Function to prepare the data \ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1) # Remove unnecessary columns\n  df = df[df[\"Sex\"] != \".\"]                                                                                 \n  df = df.dropna() # Drop NAs                                                                                          \n  y = le.transform(df[\"Species\"]) # Encode the species column                                                                      \n  df = df.drop([\"Species\"], axis = 1)                                                                       \n  df = pd.get_dummies(df) # One-hot encode the boolean columns                                                                \n  return df, y\n\nX_train, y_train = prepare_data(train)\n\nIn the above code, we processed the dataset to prepare it for analysis as follows:\n\nRemove the unnecessary columns and NAs\nPrepare the qualitative columns for analysis by encoding them as quantitative columns\n\nEncode the species column\n\nEach species is assigned a number\n\n“One-hot encode” the boolean columns\n\nThe get_dummies function converts these columns to 0-1 instead of True/False\n\n\n\n\n\narray([1, 1, 2, 2, 1, 0, 0, 1, 2, 1, 0, 1, 0, 1, 1, 2, 0, 2, 2, 2, 2, 0,\n       0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 2, 2, 1, 1, 2, 2, 1, 0,\n       0, 2, 2, 1, 2, 2, 1, 2, 0, 0, 2, 2, 0, 1, 2, 2, 1, 2, 1, 2, 2, 2,\n       0, 0, 0, 2, 2, 2, 0, 1, 2, 2, 2, 0, 0, 2, 0, 0, 2, 0, 0, 0, 1, 0,\n       0, 1, 0, 0, 0, 1, 0, 0, 2, 2, 0, 0, 2, 0, 2, 1, 0, 2, 2, 1, 2, 2,\n       2, 0, 2, 0, 0, 0, 1, 0, 2, 2, 0, 2, 2, 1, 2, 0, 0, 1, 2, 2, 1, 0,\n       2, 0, 1, 2, 0, 0, 2, 0, 2, 1, 0, 0, 2, 1, 0, 2, 0, 2, 0, 1, 0, 0,\n       0, 2, 2, 2, 0, 0, 2, 0, 2, 1, 1, 0, 1, 2, 0, 0, 1, 1, 0, 0, 1, 0,\n       0, 2, 1, 2, 1, 0, 0, 0, 1, 0, 2, 2, 2, 2, 1, 1, 1, 2, 2, 2, 0, 1,\n       0, 0, 1, 0, 0, 0, 0, 0, 1, 2, 2, 2, 0, 2, 2, 1, 0, 2, 0, 0, 2, 0,\n       2, 0, 2, 0, 2, 2, 2, 2, 0, 2, 1, 0, 2, 1, 1, 0, 2, 1, 0, 0, 0, 1,\n       0, 1, 0, 0, 2, 1, 0, 0, 0, 2, 0, 0, 0, 1])\n\n\n\n\nVisualizing the data\nNow, we need to figure out which features best distinguish each species so that we can train a model to predict the species based on the features. We’ll start by looking at visualizations of some of the features in the dataset in order to get an idea of which ones will work well for classification.\n\nsns.set_theme()\n# Plot the culmen depth and flipper length \nsns.relplot(\n    data=train,\n    x=\"Flipper Length (mm)\", y=\"Culmen Depth (mm)\",\n    hue=\"Species\", style=\"Species\"\n).set(title=\"Penguin Species by Culmen Depth and Flipper Length\")\n\n\n\n\nFigure 1: culmen length and flipper depth of each penguin species\n\n\n\n\n\n# Plot the distribution of each species on each of the 3 islands \nax = sns.countplot(data=train, x=\"Island\", hue=\"Species\")\nax.set(xlabel=\"Island\", ylabel=\"Number of Penguins\", title=\"Species Distribution Across Islands\")\nax.set_xticklabels(ax.get_xticklabels(), rotation=30)\n\n\n\n\nFigure 2: species distribution across each island\n\n\n\n\n\n\n\n\n\n\n\n\n\nCulmen Length (mm)\n\n\nSpecies\n\n\n\n\n\nAdelie\n38.970588\n\n\nChinstrap\n48.826316\n\n\nGentoo\n47.073196\n\n\n\n\n\nFigure 3: mean culmen length by species\n\n\nOur goal is to find three features that will allow us to train a model which will be 100% accurate in predicting the species of a penguin in the training set. We can ascertain some information about the top scored features from these plots that will be helpful to determine if these features will work.\nFirst, as we can see in Figure 1, the Gentoo penguin has different culmen and flipper measurements than the Chinstrap and Adelie penguins. Specifically, the Gentoo penguin seems to have a larger flipper length and a shallower culmen depth than the Chinstrap and Adelie penguins. This relationship could be used to identify the Gentoo penguins, but we still need to find a way to distinguish between the Chinstrap and Adelie penguins.\nFigure 2 provides a way to do this; the Chinstrap penguins are only found on Dream Island. We can also see that the Adelie penguins are found on all three islands, and the Gentoo penguin is only found on Biscoe Island. In the summary table, we can see that there is also a difference in culmen length between the species. The Chinstrap and Gentoo penguins have similar average culmen lengths, but the Adelie penguins have much shorter culmens on average.\nUsing some combination of these features, we should be able to accurately predict the species of any penguin in the training set. We will use tree-based feautre selection to determine which of the features we will use to train our model.\n\n\nOptimizing model parameters\nWe will be using a random forest to choose the best features to train our model on. We’ll also use a random forest to execute our classification. First, let’s understand what a random forest is:\n\nRandom Forest\n\nRandom forests are an ensemble learning method. That is, they combine predictions from multiple models to make a final prediction. Random forests use multiple decision trees to make a prediction. Each tree is trained on a different subset of the training set, so different trees can learn different patterns in the data. The trees use a process of “voting” to determine the prediction–whichever species is predicted by the most trees is the final prediction.\n\n\n\nChoosing the best features\n\n# Fit a model to find the best features in the training set\ntree_selector = ExtraTreesClassifier(n_estimators=100, random_state=42)\ntree_selector.fit(X_train, y_train)\nfeature_importances = tree_selector.feature_importances_\n\n# Create DataFrame with the feature and its importance score\nfeature_scores = pd.DataFrame({\"Feature\": X_train.columns, \"Importance\": feature_importances})\nfeature_scores = feature_scores.sort_values(by=\"Importance\", ascending=False)\n\n\n\nTop 3 Features:\n               Feature  Importance\n0   Culmen Length (mm)    0.166972\n2  Flipper Length (mm)    0.139796\n6        Island_Biscoe    0.132696\n\n\nWhen we use the ExtraTreesClassifier function, we are building a random forest from the training set. A random subset of features is used for each tree, then the best features are selected based on an importance score. The importance score is calculated using an impurity score for each feature that captures how well a feature is able to partition the dataset. These scores are then normalized to create the importance score. We can see from the feature selector that the culmen length, flipper length, and the island are the most important features in the dataset. So, we will use these features to train our model.\n\n\nChoosing maximum tree depth\nOne of the parameters for any tree method is the maximum depth at which the tree can go. That is, the number of partitions of the dataset allowed in each tree. In order to find the optimal maximum depth, we will use cross validation to test a range of potential maximum depths:\n\n# The columns that we want to include in our analysis -- based on the highest-scoring features\ncols = [\"Flipper Length (mm)\", \"Culmen Length (mm)\", \"Island_Biscoe\", \"Island_Dream\",\"Island_Torgersen\"]\n\ndepths = range(1, 21)\nmean_scores = []\n\n# Loop through potential depths and use cross validation to score each depth\nfor depth in depths:\n    dt = RandomForestClassifier(max_depth=depth, random_state=42)\n    scores = cross_val_score(dt, X_train[cols], y_train, cv=5, scoring='accuracy')\n    mean_scores.append(np.mean(scores))\n\n# Find the depth with the highest scoring validation\nbest_depth = depths[np.argmax(mean_scores)]\nbest_score = max(mean_scores)\n\n\n\nBest max tree depth: 7\nBest cross-validated accuracy: 0.9843891402714933\n\n\nBased on the cross-validation, a random forest with maximum depth 7 is the best option for our model.\n\n\n\nFitting the model\nNow that we know which features we want to use and have done some work to optimize the parameters for our random forest, we’re ready to train a random forest on our training set.\n\n# Initialize the model and fit it to the train set\ndt_model = RandomForestClassifier(max_depth=5, random_state=42)\ndt_model.fit(X_train[cols], y_train)\n\n# Score the model accuracy to see how well it did \ntrain_accuracy = dt_model.score(X_train[cols], y_train)\n\n\n\nTraining Accuracy: 0.9961\n\n\nThis random forest works pretty well! It was 99% accurate on our testing set. Now that we’ve trained this random forest tree, we can test it on the test dataset.\n\n# Read in the test data\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\ntest[\"Species\"].replace('Adelie Penguin (Pygoscelis adeliae)','Adelie',inplace=True)\ntest[\"Species\"].replace('Chinstrap penguin (Pygoscelis antarctica)','Chinstrap',inplace=True)\ntest[\"Species\"].replace('Gentoo penguin (Pygoscelis papua)','Gentoo',inplace=True)\n\nX_test, y_test = prepare_data(test)\ntest_acc = dt_model.score(X_test[cols], y_test)\n\n\n\nTesting Accuracy: 0.9853\n\n\n\n\nUnderstanding the results\nLet’s get a better understanding of what it’s actually doing. First, we can use the confusion matrix to look at how all of the penguins were classified.\n\ny_test_pred = dt_model.predict(X_test[cols])\nconfusion_matrix(y_test, y_test_pred)\n\narray([[31,  0,  0],\n       [ 0, 11,  0],\n       [ 1,  0, 25]])\n\n\nWhat does this matrix mean? We can understand it using each species as follows:\n\n\nThere were 31 Adelie penguin(s) who were classified as Adelie penguin(s).\nThere were 0 Adelie penguin(s) who were classified as Chinstrap penguin(s).\nThere were 0 Adelie penguin(s) who were classified as Gentoo penguin(s).\nThere were 0 Chinstrap penguin(s) who were classified as Adelie penguin(s).\nThere were 11 Chinstrap penguin(s) who were classified as Chinstrap penguin(s).\nThere were 0 Chinstrap penguin(s) who were classified as Gentoo penguin(s).\nThere were 1 Gentoo penguin(s) who were classified as Adelie penguin(s).\nThere were 0 Gentoo penguin(s) who were classified as Chinstrap penguin(s).\nThere were 25 Gentoo penguin(s) who were classified as Gentoo penguin(s).\n\n\nThis tells us that our model only misclassified one penguin – a Gentoo classified as an Adelie. We can plot the decision regions for each island to get an understanding of why this happened.\n\n\nPlotting Code\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Gentoo\", \"Chinstrap\", \"Adelie\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\n\n\n\n\n\n\n\n\n\n\nThese plots show us how our model made its classifications. On the far left, we can see that for the Biscoe Island data, the model was able to correctly distinguish between the Adelie (blue) and Gentoo (red) penguins using the flipper length. Moving to the far right, on Torgensen Island, there are only Adelie penguins, so the model was able to classify all of them correctly. In the middle plot we can see that the penguins on Dream Island were harder to classify – we can see where the model classified a Chinstrap (green) penguin as an Adelie penguin (blue) because it had a shorter culmen than the other Chinstrap penguins, causing its point to be within the blue region.\n\n\nDiscussion\nWe can see that our random forest was a very accurate model. It only misclassified one of the penguins, giving us 98.5% testing accuracy. We can see from the decision regions that the random forest was able to learn patterns within each island fairly well, even creating some accurate non-linear decision regions. In order to get more accuracy for the Dream island data, we would probably need to use another classification method that could learn the differences between the Chinstrap and Adelie penguins on Dream island. Or, going back to our initial visualizations, we could use an ensemble learning method that first uses one model to separate one species from the other two using the quantitative measurements and another that uses a qualitative feature like the island to distinguish between penguins with similar physical measurements."
  },
  {
    "objectID": "posts/automated-decision-systems/index.html",
    "href": "posts/automated-decision-systems/index.html",
    "title": "Automated Decison Systems",
    "section": "",
    "text": "Abstract\nIn this blog post we’ll be looking at data from a bank. We’ll start with some data exploration and visualization to get a better understanding of the features in the dataset. Our ultimate goal here is to create a decision-making system that will determine whether a prospective borrower should be offered a loan. We will use logistic regression to assign weights to different features in the dataset, which we will then use to give each prospective borrower a “score”. This score will be used to determine whether the borrower will prospective receive a loan based on a threshold that will optimize profits for the bank. After creating this decision-making system, we’ll test it out on a test dataset and do some exploration into who our system predicted would be able to repay their loans, and who it predicted wouldn’t.\n\n# Read in the data\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/train.csv\"\ndf_train = pd.read_csv(url)\n\n\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_home_ownership\nperson_emp_length\nloan_intent\nloan_grade\nloan_amnt\nloan_int_rate\nloan_status\nloan_percent_income\ncb_person_default_on_file\ncb_person_cred_hist_length\n\n\n\n\n0\n25\n43200\nRENT\nNaN\nVENTURE\nB\n1200\n9.91\n0\n0.03\nN\n4\n\n\n1\n27\n98000\nRENT\n3.0\nEDUCATION\nC\n11750\n13.47\n0\n0.12\nY\n6\n\n\n2\n22\n36996\nRENT\n5.0\nEDUCATION\nA\n10000\n7.51\n0\n0.27\nN\n4\n\n\n3\n24\n26000\nRENT\n2.0\nMEDICAL\nC\n1325\n12.87\n1\n0.05\nN\n4\n\n\n4\n29\n53004\nMORTGAGE\n2.0\nHOMEIMPROVEMENT\nA\n15000\n9.63\n0\n0.28\nN\n10\n\n\n\n\n\n\n\n\n\nExploring The Data\nLet’s start with some visualizations to explore patterns in this dataset:\n\n\nPlotting Code\n# Filter out rows with employment length greater than 25 years\ndf_train = df_train[df_train['person_emp_length'] &lt;= 25]\n\n# Bucket the employment length into 5-year chunks\nbins = range(0, 25 + 5, 5)\nlabels = [\"0 to 5 years\", \"5 to 10 years\", \"10 to 15 years\", \"15 to 20 years\", \"20 to 25 years\"]\n\ndf_train['emp_length_bucket'] = pd.cut(\n    df_train['person_emp_length'],\n    bins=bins,\n    right=True,\n    labels=labels,\n    include_lowest=True\n)\n\n# Create a Seaborn plot using histplot with multiple=\"fill\" for normalized (proportional) stacked bars\nplt.figure(figsize=(10, 6))\nsns.histplot(\n    data=df_train,\n    x='emp_length_bucket',\n    hue='loan_intent',\n    multiple='fill',  # normalizes each bar to sum to 1 (proportions)\n    shrink=0.8, \n    palette=\"Set2\"\n)\nplt.title(\"Loan Intent Proportions by Employment Length\")\nplt.xlabel(\"Employment Length\")\nplt.ylabel(\"Proportion\")\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nFigure 1: Loan intent by length of employment\n\n\n\n\n\n\nPlotting Code\n# Plot the distribution of interest rates of each of the home ownership statuses \nsns.boxplot(data=df_train, x=\"person_home_ownership\", y=\"loan_int_rate\", palette=\"Set2\")\nplt.xlabel(\"Home Ownership Status\")\nplt.ylabel(\"Loan Interest Rate\")\n\n\nText(0, 0.5, 'Loan Interest Rate')\nFigure 2: Interest rate distribution by home ownership status\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncredit_history_length\n\n\nloan_grade\n\n\n\n\n\nA\n5.7\n\n\nB\n5.8\n\n\nC\n5.9\n\n\nD\n5.8\n\n\nE\n5.8\n\n\nF\n6.1\n\n\nG\n6.6\n\n\n\n\n\nFigure 3: mean credit history length by loan grade\n\n\nWe’re looking here to understand some of the patterns in this dataset.\nFirst, in Figure 1, we can see that borrowers’ intentions for their loans change a bit as the length of their employment increases. Specifically, after 20 years of employment people stop borrowing to pay for education, which makes sense. People also start taking out more loans for venture and medical uses as their employment goes on. It looks like the proportion of loans that are intended to be used for home improvement, personal reasons, and debt consolidation remain fairly constant across length of employment.\nMoving on to Figure 2, we can see that borrowers with different home ownership statuses (renting, owning, mortgaging or other), have differences in the interest rate they might receive on their loan. Borrowers who rent their home seem more likely to have an interest rate of over 20%, and we can see that borrowers who own their homes rarely see interests rates over 20%. Borrowers who rent and mortgage their homes have more variability in the interest rates they might see, as well.\nFinally, we can see in Figure 3 that loan grades have differences in average length of credit of the borrower. It seems that borrowers with higher loan grades have longer credit histories, on average.\nWe can see that we should be able to get some information about whether a borrower will pay back a loan from some of these features.\n\nPrepping the Data\n\n# Encode the qualitative variables quantitatively\ndef preprocess(df):\n    \"\"\"\n    Function to preprocess the data\n    \"\"\"\n    le_home = LabelEncoder()\n    df[\"person_home_ownership\"] = le_home.fit_transform(df[\"person_home_ownership\"])\n\n    le_intent = LabelEncoder()\n    df[\"loan_intent\"] = le_intent.fit_transform(df[\"loan_intent\"])\n    intent_mapping = dict(zip(le_intent.transform(le_intent.classes_), le_intent.classes_))\n\n    df = pd.get_dummies(df, columns=['cb_person_default_on_file'], prefix='default')\n    df = df.dropna()\n\n    # Change the interest rate column to a percentage\n    df[\"loan_int_rate\"] = df[\"loan_int_rate\"]/100\n\n    #Filter out columns with weird values\n    df = df[(df[\"person_age\"] &lt; 100) & (df[\"person_income\"] &gt; 1000) &  (df[\"loan_int_rate\"] &gt; 0)]\n\n    # Separate our the target variable\n    y = df[\"loan_status\"]\n\n    return df, y, intent_mapping\n\ndf_train_clean, y_train, intent_map = preprocess(df_train)\n\nWe will use this function prepare our datasets for analysis. We’re encoding the qualitative columns for home ownership and loan intent as integers, one-hot encoding the historical default true/false column as 0s and 1s, separating the target variable\n\n\n\nBuilding A Model\n\nChoosing Features\nFirst, let’s figure out which features we want to use for the model. We’ll generate a few new features that we can use to better understand the relationships between our features: - Credit history ratio: \\(\\frac{\\text{credit history length}}{\\text{age}}\\) - Employment stability: \\(\\frac{\\text{employment length}}{\\text{age}}\\) - Income stability: \\(\\text{income}*\\text{employment length}\\)\nWe’ll use cross validation to test a few combinations of features: 1. Employment stability, income, and percentage of income that the loan is 2. Loan intent, credit history ratio, home ownership status 3. income stability, loan interest rate, age\n\n# Create new features \ndef new_features(df):\n    df[\"credit_history_ratio\"] = round(df[\"cb_person_cred_hist_length\"]/df[\"person_age\"],2)\n    df[\"employment_stability\"] = round(df[\"person_emp_length\"]/df[\"person_age\"], 2)\n    df[\"income_stability\"] = round(df[\"person_income\"]*df[\"person_emp_length\"],2)\n    return df\ndf_train_clean = new_features(df_train_clean)\n\n\n# Define three different sets of features\nfeature_sets = {\n    \"Set 1\": [\"employment_stability\", \"person_income\", \"loan_percent_income\"],\n    \"Set 2\": [\"loan_intent\", \"credit_history_ratio\", \"person_home_ownership\"],\n    \"Set 3\": [\"income_stability\", \"loan_int_rate\", \"person_age\"]\n}\n\nmodel = LogisticRegression()\n\n# Evaluate each feature set\nresults = {}\nfor set_name, features in feature_sets.items():\n    X_train = df_train_clean[features]\n    X_train = StandardScaler().fit_transform(X_train)  # Standardize features \n    \n    scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')\n    mean_accuracy = np.mean(scores)\n    results[set_name] = mean_accuracy\n    print(f\"{set_name}: Mean Accuracy = {mean_accuracy:.4f}\")\n\nSet 1: Mean Accuracy = 0.8246\nSet 2: Mean Accuracy = 0.7850\nSet 3: Mean Accuracy = 0.8095\n\n\n\n\nEstimating weights\nAfter fitting an LR to each of the feature sets and using cross validation to test how accurate the models are, we can see that our first set of features, which includes employment stability, income, and percentage of income that the loan is has the highest average accuracy. Now, we’ll fit an LR model to these features to obtain our vector of weights that we’ll use in our scoring function.\n\nweights_model = LogisticRegression()\nX_train = df_train_clean[feature_sets[\"Set 1\"]]\nX_train = StandardScaler().fit_transform(X_train)\nweights_model.fit(X_train, y_train)\n\nfor feature, coef in zip(feature_sets[\"Set 1\"], weights_model.coef_[0]):\n    print(f\"{feature}: {coef:.4f}\")\n\nemployment_stability: -0.1482\nperson_income: -0.4953\nloan_percent_income: 0.7925\n\n\nNow we can use these coefficients in our as the values in our weight vector when we create a scoring function, which we’ll do next.\n\n\n\nCreating the score function\nOur next step is to give each borrower a score, using the features and weights that we decided on above. We’ll add a “score” column to the train set that has the result of our score function:\n\\(\\text{score} = -0.1482 * x_1 - 0.4953 * x_2 + 0.7925 * x_3\\)\nwhere\n\\(x_1\\) = employment_stability\n\\(x_2\\) = person_income\n\\(x_3\\) = loan_percent_income\n\ndef score_dataframe(df):\n    \"\"\"\n    Compute the score for each borrower using the weights \n    determined by the logistic regression\n\n    inputs:\n    df: Pandas DataFrame with all features used to generate weights\n    and loan status, amount, and interest rate for profit calculations\n    \"\"\"\n    df_scored = df.copy()\n    df_scored[[\"employment_stability\", \"person_income\", \"loan_percent_income\"]] = StandardScaler().fit_transform(df_scored[[\"employment_stability\", \"person_income\", \"loan_percent_income\"]])\n    df_scored[\"score\"] =  round(-0.1486 * df_scored[\"employment_stability\"] - 0.6252 * df_scored[\"person_income\"] + 0.7925 * df_scored[\"loan_percent_income\"],2)\n    return df_scored\n\nNow, we will write a function that will compute the expected profit from each prospective borrower. If a prospective borrower has a score greater than the threshold, our system will predict that this borrower will default on their loan. So, we will not give them a loan, making their profit contribution 0. For users that we predict will not default on their loan, we have two cases. If they repay their loan, their profit will be determined by:\n\\(\\text{profit = (loan amount)} * (1 + 0.25* \\text{(loan interest rate)})^{10} - \\text{(loan amount)}\\)\nIf the borrowed does not repay their loan, the bank will “profit” according to this equation:\n\\(\\text{profit = (loan amount)} * (1 + 0.25* \\text{(loan interest rate)})^{3} - 1.7 * \\text{(loan amount)}\\)\nThis function will apply the profit equation to each borrower based on their predicted loan status, and we will calculate the expected profit per borrower by dividing by the number of prospective borrowers.\n\ndef compute_profit(threshold, df):\n    \"\"\"\n    Compute the profit that the bank would have made if they had\n    used the predictions determined by our score function\n    and threshold\n\n    inputs:\n    threshold: integer value for score threshold \n    df: Pandas DataFrame with loan amount, interest rate, actual loan status, and score from \n    scoring function\n    \"\"\"\n    df[\"pred_default\"] = df[\"score\"].apply(lambda x: 1 if x &gt; threshold else 0)\n    \n    # Calculate potential profit for repayment or defaulting \n    df[\"repaid_profit\"] = round(df[\"loan_amnt\"] * (1 + 0.25 * df[\"loan_int_rate\"])**10 - df[\"loan_amnt\"],2)\n    df[\"defaulted_profit\"] = round(df[\"loan_amnt\"] * (1 + 0.25 * df[\"loan_int_rate\"])**3 - 1.7 * df[\"loan_amnt\"],2)\n    \n    # Profit based on actual loan status versus predicted status\n    df[\"profit\"] = 0  # Default to zero profit\n    df.loc[(df[\"loan_status\"] == 0) & (df[\"pred_default\"] == 0), \"profit\"] = df[\"repaid_profit\"]  # Successful repayment\n    df.loc[(df[\"loan_status\"] == 1) & (df[\"pred_default\"] == 0), \"profit\"] = df[\"defaulted_profit\"]  # Incorrectly predicted as good loan, default\n    df.loc[(df[\"loan_status\"] == 1) & (df[\"pred_default\"] == 1), \"profit\"] = 0  # Correctly predicted as default, no profit\n    df.loc[(df[\"loan_status\"] == 0) & (df[\"pred_default\"] == 1), \"profit\"] = 0  # Incorrectly predicted as default, no profit\n\n    return sum(df[\"profit\"]) / len(df)\n\n\n# Find optimal threshold\nscored_df = score_dataframe(df_train_clean)\nthresholds = np.arange(0,3,0.05)\nprofits = [compute_profit(t, scored_df) for t in thresholds]\n\n# Find best threshold\noptimal_threshold = thresholds[np.argmax(profits)]\noptimal_profit = max(profits)\nprint(f\"Optimal Threshold for Maximum Profit: {optimal_threshold:.4f}\")\nprint(f\"Maximum profit per prospective borrower: {round(optimal_profit,2):.4f}\")\n\n\n# Plot profit vs. threshold\nsns.set_style(\"whitegrid\")\nsns.set_palette(\"Set2\")\n\nplt.figure(figsize=(8, 5))\nplt.scatter(optimal_threshold, optimal_profit, color=sns.color_palette(\"Set2\")[1], label=f'Optimal t={optimal_threshold:.4f}', zorder=2)\nsns.lineplot(x=thresholds, y=profits, label=\"Profit\", zorder=1)\nplt.xlabel(\"Threshold\")\nplt.ylabel(\"Profit per borrower\")\nplt.title(\"Threshold vs Profit\")\nplt.legend()\nplt.show()\n\nOptimal Threshold for Maximum Profit: 1.1500\nMaximum profit per prospective borrower: 1316.4800\n\n\n\n\n\n\n\n\n\n\n\nEvaluating from the bank’s perspective\nNow, we want to look at how this decision making system works on a test dataset. We’ll apply all of the functions we used on the training set with the test set, and see what the profit per borrower will look like using the threshold from above, 1.15.\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/test.csv\"\ndf_test = pd.read_csv(url)\ndf_test, y_test, intent_map = preprocess(df_test)\ndf_test = new_features(df_test)\n    \nscored_test_df = score_dataframe(df_test)\nprint(\"profit per prospective borrower:\", round(compute_profit(1.15, scored_test_df),2))\n\nprofit per prospective borrower: 1230.42\n\n\nIt looks like we ended up with a fairly similar profit per borrower on the test set to what we got from the train set, $1316 per borrower on the test set and $1230 on the train set. From the bank’s perspective, this seems positive. If they are making, on average, $1230 per borrower, they would make around $6,000,000 off of this dataset of borrowers.\n\n\nEvaluating from the borrower’s perspective\nNow, let’s try to get a better understanding of who in the test dataset we are predicting is going to default on their loans, and who we are predicting is going to repay them. We’ll look at the following questions:\n\nIs it more difficult for people in certain age groups to access credit under your proposed system?\nIs it more difficult for people to get loans in order to pay for medical expenses? How does this compare with the actual rate of default in that 3. group? What about people seeking loans for business ventures or education?\nHow does a person’s income level impact the ease with which they can access credit under your decision system?\n\n\nQuestion 1\n\n# Split the borrower ages into 4 buckets and see \nscored_test_df[\"age_group\"] = pd.qcut(scored_test_df[\"person_age\"], q=4, labels=[\"Young\", \"Middle-aged\", \"Older\", \"Senior\"])\nage_access = scored_test_df.groupby(\"age_group\")[\"pred_default\"].mean()\nage_access\n\nage_group\nYoung          0.171756\nMiddle-aged    0.121554\nOlder          0.142263\nSenior         0.134460\nName: pred_default, dtype: float64\n\n\nThese probabilities are the mean of the prediction column, so they tell us the rate at which a 1 is predicted in each of the age buckets using our decision making system. That is, they tell us the loan rejection rate for each age group. We can see that younger people are mich more likely to be rejected for a loan than middle-aged and older people are, with a 17% rejection rate as opposed to a 12-14% rejection rate for older groups.\n\n\nQuestion 2\n\n# Calculate rejection rates and actual default rates for loan intent\nloan_intent_access = scored_test_df.groupby(\"loan_intent\")[\"pred_default\"].mean()\nloan_intent_default_rates = scored_test_df.groupby(\"loan_intent\")[\"loan_status\"].mean()\n\nloan_intent_df = pd.DataFrame({\n    \"Loan Intent (Decoded)\": loan_intent_access.index.map(intent_map),\n    \"Rejection Rate\": loan_intent_access.values,\n    \"Actual Default Rate\": loan_intent_default_rates.values\n})\n\n\n\n\n\n\n\n\n\n\nLoan Intent (Decoded)\nRejection Rate\nActual Default Rate\n\n\n\n\n0\nDEBTCONSOLIDATION\n0.150442\n0.287611\n\n\n1\nEDUCATION\n0.150510\n0.167517\n\n\n2\nHOMEIMPROVEMENT\n0.095779\n0.250000\n\n\n3\nMEDICAL\n0.164958\n0.284250\n\n\n4\nPERSONAL\n0.146293\n0.220441\n\n\n5\nVENTURE\n0.127593\n0.146266\n\n\n\n\n\n\n\nWe’re interested in the loans that are intended for medical uses. We can see that medical loans have the highest rate of rejection from our system (20% of medical loan requests are rejected), but they also have one of the highest rates of actual default (28%). Venture and business loans have lower rates of rejection and their actual default rates are closer to their rejection rates.\n\n\nQuestion 3\n\nscored_test_df[\"income_group\"] = pd.qcut(scored_test_df[\"person_income\"], q=4, labels=[\"Low\", \"Lower-middle\", \"Upper-middle\", \"High\"])\nincome_access = scored_test_df.groupby(\"income_group\")[\"pred_default\"].mean()\n\n\n\nincome_group\nLow             0.342638\nLower-middle    0.168044\nUpper-middle    0.052274\nHigh            0.004370\nName: pred_default, dtype: float64\n\n\nHere, we are again looking at the loan rejection rates, this time by income group. It is clear that people with lower incomes are much more likely to be rejected using our decision making system (41% of the time), as opposed to high income people who are only rejected 0.8% of the time with our system.\n\n\n\nWrite and Reflect\nWe found from our model and evaluation that when we choose a threshold that optimizes the bank’s profit, it can result in certain groups having a harder time obtaining a loan. We saw that age, income, and loan intent groups have differences rejection rates all with our decision making system. We can also see that optimizing profits for a bank requires turning a way a significant chunk of loan applications, because the reality is that many people do end up defaulting on their loans. In our decision-making process, we only considered 3 features of a person’s application, but we found that these features correlated with certain groups having a harder time receiving a loan. This is a great example of a feature that is implicitly encoding information about another category. For example, a person’s income could be correlated with what they are planning to use the loan for. Even though loan intent wasn’t one of the features of our score function, we could see that different loan intents ended up with different rates of rejection in our system. So, is this system fair? We’ll explore that by answering this question: Considering that people seeking loans for medical expense have high rates of default, is it fair that it is more difficult for them to obtain access to credit?\nFirst, let’s define fairness. I will define fairness in deciding who should get a loan of a combination of how much the person needs the loan and how likely it is that they will pay the loan back. If we are only basing fairness on how much the person needs the loan, we would likely say that people who need loans for medical reasons should all receive their loans. But, is this fair to the bank? They would end up losing a lot of money because people who take out loans for medical expenses often default on them. If we are only basing fairness on how likely it is that they will pay the loan back, most people seeking medical loans probably shouldn’t get them, because they do often default. But, in this case, very few people would receive medical loans, which seems unfair because they may be able to pay them back, even if people haven’t historically paid medical loans back at a high rate. There are also some other confounding variables to consider, like whether people seeking medical loans are more likely to come from a historically marginalized group that might have a harder time proving they can pay a loan back or a smaller income on average than other groups? It may not be fair for these people to be subject to high rejection rates just because of their identity.\nUltimately, Looking at the rate of rejection versus the true rate of default for medical loans, I think it’s fair that it may be more difficult for people to access these loans. The rate of default on these loans is much higher than the rate of default for loans with other intended purposes, so it seems that people who are seeking these loans may need to provide additional proof that they would be able to pay the loan back. It’s fair for the bank to be skeptical. However, we can see that in our system, 80% of people seeking medical loans are being accepted. So it isn’t the case that it’s impossible for these people, who probably really need the loan, to obtain it. Therefore, I think it’s fair that it is more difficult for people seeking medical loans to obtain access to credit given that they default much more frequently."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Replication Study\n\n\n\n\n\nReplication Study: Dissecting racial bias in an algorithm used to manage the health of populations\n\n\n\n\n\nMar 12, 2025\n\n\nCarly McAdam\n\n\n\n\n\n\n\n\n\n\n\n\nAuditing Bias\n\n\n\n\n\nAuditing for gender bias in income predictions\n\n\n\n\n\nMar 6, 2025\n\n\nCarly McAdam\n\n\n\n\n\n\n\n\n\n\n\n\nAutomated Decison Systems\n\n\n\n\n\nDesign and Impact of Automated Decision Systems\n\n\n\n\n\nFeb 27, 2025\n\n\nCarly McAdam\n\n\n\n\n\n\n\n\n\n\n\n\nClassifying Palmer Penguins\n\n\n\n\n\nClassifying Palmer Penguins\n\n\n\n\n\nFeb 13, 2025\n\n\nCarly McAdam\n\n\n\n\n\n\n\n\n\n\n\n\nSecond Post\n\n\n\n\n\nA new blog post that I just made!\n\n\n\n\n\nMar 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\n\n\n\n\n\n\nTimnit Gebru\n\n\n\n\n\nA new blog post that I just made!\n\n\n\n\n\nMar 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\n\n\n\n\n\n\nHello Blog\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/replication-study/index.html",
    "href": "posts/replication-study/index.html",
    "title": "Replication Study",
    "section": "",
    "text": "Abstract\nIn this blog post, we will be replicating a study published by Obermeyer et al. in 2019. They found evidence of racial bias in one widely used algorithm. We will visualize how Black patients are often not flagged for extra care even when their white counterparts with the same health conditions are flagged. We will see that Black patients are labeled as lower risk because healthcare costs are used as a proxy for healthcare needs, and less money is spent on Black patients, making it seem like they have less need for extra care. We will quantify this result using a linear regression, and discuss which ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍statistical ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍discrimination ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍criteria describes the bias we found in this algorithm.\nFirst, we’ll read in and take a look at the dataset.\n\n# Read in the data\nurl = \"https://gitlab.com/labsysmed/dissecting-bias/-/raw/master/data/data_new.csv?inline=false\"\ndf = pd.read_csv(url)\n\n\n\n\n\n\n\n\n\n\nrisk_score_t\nprogram_enrolled_t\ncost_t\ncost_avoidable_t\nbps_mean_t\nghba1c_mean_t\nhct_mean_t\ncre_mean_t\nldl_mean_t\nrace\n...\ntrig_min-high_tm1\ntrig_min-normal_tm1\ntrig_mean-low_tm1\ntrig_mean-high_tm1\ntrig_mean-normal_tm1\ntrig_max-low_tm1\ntrig_max-high_tm1\ntrig_max-normal_tm1\ngagne_sum_tm1\ngagne_sum_t\n\n\n\n\n0\n1.987430\n0\n1200.0\n0.0\nNaN\n5.4\nNaN\n1.110000\n194.0\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n7.677934\n0\n2600.0\n0.0\n119.0\n5.5\n40.4\n0.860000\n93.0\nwhite\n...\n0\n1\n0\n0\n1\n0\n0\n1\n4\n3\n\n\n2\n0.407678\n0\n500.0\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n3\n0.798369\n0\n1300.0\n0.0\n117.0\nNaN\nNaN\nNaN\nNaN\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n4\n17.513165\n0\n1100.0\n0.0\n116.0\nNaN\n34.1\n1.303333\n53.0\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n\n\n\n\n5 rows × 160 columns\n\n\n\nWe will be particularly focused on these columns: - risk_score_t ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍is ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍algorithm’s ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍risk ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍score ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍assigned ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍to ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍a ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍given ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍patient. - cost_t ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍is ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍patient’s ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍medical ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍costs ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍in ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍study ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍period. - race ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍is ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍patient’s ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍self-reported ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍race. ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍The ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍authors ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍filtered ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍data ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍to ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍include ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍only ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍white ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍and ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍black ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍patients. - gagne_sum_t ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍is ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍total ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍number ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍of ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍chronic ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍illnesses ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍presented ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍by ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍patient ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍during ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍study ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍period.\n\n\nReproducing Fig. 1\nFirst, we’ll look at some visualizations to get an understanding of our dataset. The first plot we will reproduce from the paper will look at risk score percentiles against mean number of active chronic conditions within each percentile. To make this plot, we’ll start by calculating the percentiles for the risk scores.\n\n# Calculate risk score percentiles\ndf[\"risk_percentile\"] = pd.qcut(df[\"risk_score_t\"], 100, labels=False)\n\nNext, we’ll create a subplot for males and females to further split the data. To plot the risk scores and number of chronic conditions, we will iterate through each race in the dataset (just white and black) and take the mean number of chronic conditions for each risk score percentile. We will use different color and marker for each race, blue circles for black patients and green x’s for white patients.\n\n\nPlotting code for risk score vs. number chronic conditions\n# Define markers for race on our plots\nmarkers = {\"white\": \"x\", \"black\": \"o\"}\ncolors = {\"white\": \"green\", \"black\": \"blue\"}\n\nsns.set_style(\"whitegrid\")\n\n# Create subplots for male (0) and female (1) patients\nfig, axes = plt.subplots(1, 2, figsize=(12, 6), sharey=True)\n\nfor i, female in enumerate([0, 1]):  # 0 = Male, 1 = Female\n    ax = axes[i]\n    # Iterate through the races in the df (black, white)\n    for race in df[\"race\"].unique():\n        subset = df[(df[\"dem_female\"] == female) & (df[\"race\"] == race)]\n        grouped = subset.groupby(\"risk_percentile\")[\"gagne_sum_t\"].mean().reset_index()\n        sns.scatterplot(\n            data=grouped, \n            x=\"gagne_sum_t\", \n            y=\"risk_percentile\", \n            marker=markers[race], \n            color = colors[race],\n            label=f\"{race.capitalize()} Patients\", \n            ax=ax\n        )\n    ax.set_title(f\"{'Female' if female == 1 else 'Male'} Patients\")\n    ax.set_xlabel(\"Mean Chronic Conditions\")\n    if i == 0:\n        ax.set_ylabel(\"Risk Score Percentile\")\n    ax.legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nLet’s take a look at this plot to understand what might happen in this scenario: Patient A is black and Patient B is white and both patients have the same chronic illnesses. We can see from the plot that across both genders, it seems that Patient A and Patient B are not equally likely to be referred to the high-risk care management program. The blue points, which represent black patients, consistently have lower risk scores than green points that have at the same number of chronic illnesses. This would make it less likely for these black patients to be referred to a high-risk care management program because they are being scored as lower risk than white patients with the same illnesses.\n\n\nReproducing Fig. 3\nNow, we’ll reproduce another figure from the paper that should give us some more insight into why Black patients are being given lower risk scores than white patients with the same number of chronic conditions. First, we’ll group across risk percentile and race to take the mean total medical expenditure for each race at each risk percentile. Then, we’ll group by number of chronic conditions and race so that we can take the mean total medical expenditure for each race at each number of chronic conditions.\n\nrisk_grouped = df.groupby([\"risk_percentile\", \"race\"])[\"cost_t\"].mean().reset_index()\nchronic_illness_grouped = df.groupby([\"gagne_sum_t\", \"race\"])[\"cost_t\"].mean().reset_index()\n\nNext, we’ll make two subplots – one that plots total medical expenditure by risk score percentile and one that plots total medical expenditure by number of chronic conditions. Again, we will use blue circles for black patients and green x’s for white patients.\n\n\nPlotting code for medical expenditure vs. number chronic conditions and risk score\n# Create the figure with two panels\nfig, axes = plt.subplots(1, 2, figsize=(12, 6), sharey=True)\n\n# Left panel: Mean expenditure vs. Risk Score Percentile\nfor race in df[\"race\"].unique():\n    subset = risk_grouped[risk_grouped[\"race\"] == race]\n    sns.scatterplot(\n        data=subset,\n        x=\"risk_percentile\",\n        y=\"cost_t\",\n        marker=markers[race],  \n        color=colors[race],  \n        label=f\"{race.capitalize()} Patients\",\n        ax=axes[0]\n    )\n\naxes[0].set_title(\"Medical Expenditure vs. Risk Score Percentile\")\naxes[0].set_xlabel(\"Risk Score Percentile\")\naxes[0].set_ylabel(\"Mean Medical Expenditure ($)\")\naxes[0].set_ylim(1000, 10000)\naxes[0].legend()\naxes[0].set_yscale(\"log\")\n\n# Right panel: Medical Expenditure vs. Number of Chronic Conditions\nfor race in df[\"race\"].unique():\n    subset = chronic_illness_grouped[chronic_illness_grouped[\"race\"] == race]\n    sns.scatterplot(\n        data=subset,\n        x=\"gagne_sum_t\",\n        y=\"cost_t\",\n        marker=markers[race],  \n        color=colors[race],  \n        label=f\"{race.capitalize()} Patients\",\n        ax=axes[1]\n    )\n\naxes[1].set_title(\"Medical Expenditure vs. Chronic Conditions\")\naxes[1].set_xlabel(\"Number of Chronic Conditions\")\naxes[1].set_ylabel(\"\")\naxes[1].set_ylim(1000, 100000)\naxes[1].legend()\naxes[1].set_yscale(\"log\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nIn this plot, we can get an understanding of how total medical expenditures are correlated with risk score and the number of chronic illnesses. It seems from the left panel that there is more variation in the mean medical expenditures for black patients by risk score than there is for white patients. Although we see many blue points (black patients) below green points with the same risk score, we also see blue points above green points with the same risk score. There is an interesting trend in the right panel – it appears that white patients tend to have more medical expenditures than blank patients with the same number of chronic conditions when the number of conditions is less than 5, but as the number of conditions increases, black patients tend to have more medical expenditures than white patients with the same number of conditions. Considering that the vast majority of patients in this dataset have fewer than 5 chronic conditions, this dataset seems to show black patients, on average, having fewer medical expenditures than white patients with the same number of chronic conditions.\n\n\nModeling Cost Disparity\nNow that we’ve gotten an understanding of the dataset and what’s going on with bias here, let’s make a model so that we can quantify it more accurately.We’ll start with prepping the data.\n\nData Prep\n\n\nPercentage of patients in DF with 5 or fewer chronic conditions: 0.96 %\n\n\nGiven that the vast majority of people in the dataset have fewer than 5 chronic conditions, I think it makes sense to just focus on these people in our analysis. We will take the log of the medical expenditures because they have a large range.\n\n# Take the log of the cost column (also remove costs that are 0)\ndf = df[df[\"cost_t\"] &gt; 0]\ndf[\"log_cost\"] = np.log(df[\"cost_t\"])\n\nNext we’ll encode the race dummy as an 0-1 variable, with 1 for black individuals and 0 for white individuals.\n\n# Encode the race variable as an integer \ndf[\"race_dummy\"] = (df[\"race\"] == \"black\").astype(int)\n\nLastly, we will separate out our features (race and number of chronic conditions) from our target variable (total medical expenditure).\n\n# Separate predictor and target variables\nX = df[[\"race_dummy\", \"gagne_sum_t\"]]\ny = df[\"log_cost\"]\n\n\n\nModeling\nWe will fit a linear regression model with polynomial features in the number of active chronic conditions to account for the nonlinearity in the relationship between number of active chronic conditions that is apparent from the left hand panel in the above plot. We’ll first define a function to add polynomial features to the dataset.\n\ndef add_polynomial_features(X, degree):\n  X_ = X.copy()\n  for j in range(1, degree):\n    X_[f\"poly_{j}\"] = X_[\"gagne_sum_t\"]**j\n  return X_\n\nNext, we will use this function to loop through polynomial features from \\(1\\) to \\(11\\) and regularization strengths \\(10^k\\) from \\(k = -4, -3, ..., 3, 4\\). The regularization strength is a parameter that describes how much the training process should prefer “simple” models with small values of the weights \\(w\\) as opposed to a more complex model. At each degree-regularization combination, we’ll train a model and compute the cross-validated mean squared error of the model. We will take the combination with the smallest mean squared error and use that degree-regularization combination to train the model we will use for our analysis.\n\n# Ranges of degrees and ks for regularization\ndegrees = range(1, 12)      \nks = range(-4, 4)    \n\nresults = []\n\nfor degree in degrees:\n    # Add polynomial features to X_base (note: for degree == 1, nothing extra is added)\n    X_poly = add_polynomial_features(X, degree)\n    \n    # Loop through each regularization strength\n    for k in ks:\n        alpha = 10**k\n        # Create a Ridge regression model with the given alpha\n        model = Ridge(alpha=alpha)\n        \n        # Use cross-validation to estimate the mean squared error.\n        # Note: cross_val_score returns negative MSE when scoring is set to \"neg_mean_squared_error\"\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\"ignore\")\n            scores = cross_val_score(model, X_poly, y, cv=5, scoring=\"neg_mean_squared_error\")\n        mse = -np.mean(scores)\n            \n        # Save the combination and results\n        results.append({\n            \"degree\": degree,\n            \"alpha\": alpha,\n            \"cv_mse\": mse\n        })\nresults_df = pd.DataFrame(results)\nresults_df = results_df.sort_values(\"cv_mse\")\n\n# Identify the best (lowest CV MSE) combination\nbest = round(results_df.iloc[0],2)\n\n\n\n\nBest combination:\ndegree    10.00\nalpha      1.00\ncv_mse     1.51\nName: 76, dtype: float64\n\n\nSo, we can see that our optimal parameter combination is \\(\\alpha = 1\\) with a degree of \\(10\\). Next let’s fit a model to the entire dataset using these parameters.\n\n# Fit a model on the entire dataset \nX_poly = add_polynomial_features(X, 10)\n\n# Create a Ridge regression model with the alpha=1\nmodel = Ridge(alpha=1)\n\n# Fit the model on the entire dataset to extract coefficients\nmodel.fit(X_poly, y)\n\n# Find the coefficient corresponding to race_dummy\ncoef_index = list(X_poly.columns).index(\"race_dummy\")\nrace_coef = model.coef_[coef_index]\nexp_race_coef = np.exp(race_coef)\n\n\n\n\nBlack coefficient: -0.27\n\n\nBased on these results, we can see that the coefficient associated with the Black individuals in the dataset is \\(w_b = -0.27\\). We can calculate \\(e^{w_b} = e^{-0.27} = 0.76\\). So, percentage of cost incurred by a Black patient in comparison to an equally sick white patient, is around 76%. This supports the argument that was made in the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍Obermeyer paper because it suggests that less money is spent on Black patients who have the same level of need as white patients.\n\n\n\nAbstract and Discussion\nAfter replicating the study done in the Obermeyer paper, we can see that because less money is spent on Black patients’ healthcare, they are often classified as lower risk than white patients who have the same chronic conditions. This has concerning implications, as it seems like this algorithm was making it harder for Black patients to access care than white patients, even if the Black patients are sicker than the white patients.\nIn terms of the formal ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍statistical ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍discrimination ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍criteria ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍discussed in Chapter 3 of ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍ ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍Barocas, Hardt, and Narayanan, the separation criterion best describes the bias that Obermeyer et al. found in their paper. This is because separation would require that the risk score be conditionally independent of race given true health need. That is, if two individuals have the same underlying health status, their risk scores should be statistically indistinguishable across racial groups. But, our results show that this isn’t true when we use this algorithm. Our results showed that Black patients with the same chronic conditions as white patients are assigned lower risk scores because the algorithm uses healthcare expenditures as a proxy for health need. Even though the algorithm may be able to predict health care costs effectively, these effective predictions are not an accurate proxy for health need, which makes the algorithm violate the separation criterion. Even though the model may be accurate, it is not necessarily unbiased.\nI learned a lot from replicating this study. I think I went into it with an understanding of the importance of accuracy and equality in a model, but without considering the implications of the model. Even though this model may have been accurate, the context in which it was being applied made it so that it was biased against Black individuals. Using one feature as a proxy for another feature seems tricky, and requires a lot of auditing to ensure that it doesn’t create bias."
  }
]