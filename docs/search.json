[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is Carly’s CSCI 451 blog"
  },
  {
    "objectID": "posts/new-new-test-post/index.html",
    "href": "posts/new-new-test-post/index.html",
    "title": "Timnit Gebru",
    "section": "",
    "text": "from source import Perceptron\np = Perceptron()\n\nI did it!!\nnot implemented\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-new-test-post/index.html#math",
    "href": "posts/new-new-test-post/index.html#math",
    "title": "Timnit Gebru",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/palmer-penguins/index.html",
    "href": "posts/palmer-penguins/index.html",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "Abstract\nIn this blog post we are going to train a random forest model to classify penguin species based on three features in the Palmer Penguins dataset. We’ll start with some visualizations to get an understanding of the different features in the dataset, and then we will use a random forest to choose the best three features to train our model on. After training and testing the model, we will explore how well the model did and how it made its classifications using the confusion matrix and plotting decision regions.\n\n\nPreparing the Data\n\n# Read in the data\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\n\nle = LabelEncoder()\ntrain[\"Species\"].replace('Adelie Penguin (Pygoscelis adeliae)','Adelie',inplace=True)\ntrain[\"Species\"].replace('Chinstrap penguin (Pygoscelis antarctica)','Chinstrap',inplace=True)\ntrain[\"Species\"].replace('Gentoo penguin (Pygoscelis papua)','Gentoo',inplace=True)\nle.fit(train[\"Species\"])\n\n# Function to prepare the data \ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1) # Remove unnecessary columns\n  df = df[df[\"Sex\"] != \".\"]                                                                                 \n  df = df.dropna() # Drop NAs                                                                                          \n  y = le.transform(df[\"Species\"]) # Encode the species column                                                                      \n  df = df.drop([\"Species\"], axis = 1)                                                                       \n  df = pd.get_dummies(df) # One-hot encode the boolean columns                                                                \n  return df, y\n\nX_train, y_train = prepare_data(train)\n\nIn the above code, we processed the dataset to prepare it for analysis as follows: * Remove the unnecessary columns and NAs * Prepare the qualitative columns for analysis by encoding them as quantitative columns + Encode the species column - Each species is assigned a number + “One-hot encode” the boolean columns - The get_dummies function converts these columns to 0-1 instead of True/False\n\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nStage_Adult, 1 Egg Stage\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\n0\n40.9\n16.6\n187.0\n3200.0\n9.08458\n-24.54903\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n1\n49.0\n19.5\n210.0\n3950.0\n9.53262\n-24.66867\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n2\n50.0\n15.2\n218.0\n5700.0\n8.25540\n-25.40075\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n3\n45.8\n14.6\n210.0\n4200.0\n7.79958\n-25.62618\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n4\n51.0\n18.8\n203.0\n4100.0\n9.23196\n-24.17282\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n\n\n\n\n\n\n\nVisualizing the data\nNow, we need to figure out which features best distinguish each species so that we can train a model to predict the species based on the features. We’ll start by looking at visualizations of some of the features in the dataset in order to get an idea of which ones will work well for classification.\n\nsns.set_theme()\n# Plot the culmen depth and flipper length \nsns.relplot(\n    data=train,\n    x=\"Flipper Length (mm)\", y=\"Culmen Depth (mm)\",\n    hue=\"Species\", style=\"Species\"\n).set(title=\"Penguin Species by Culmen Depth and Flipper Length\")\n\n\n\n\nFigure 1: culmen length and flipper depth of each penguin species\n\n\n\n\n\n# Plot the distribution of each species on each of the 3 islands \nax = sns.countplot(data=train, x=\"Island\", hue=\"Species\")\nax.set(xlabel=\"Island\", ylabel=\"Number of Penguins\", title=\"Species Distribution Across Islands\")\nax.set_xticklabels(ax.get_xticklabels(), rotation=30)\n\n\n\n\nFigure 2: species distribution across each island\n\n\n\n\n\n\n\n\n\n\n\n\n\nCulmen Length (mm)\n\n\nSpecies\n\n\n\n\n\nAdelie\n38.970588\n\n\nChinstrap\n48.826316\n\n\nGentoo\n47.073196\n\n\n\n\n\nFigure 3: mean culmen length by species\n\n\nOur goal is to find three features that will allow us to train a model which will be 100% accurate in predicting the species of a penguin in the training set. We can ascertain some information about the top scored features from these plots that will be helpful to determine if these features will work.\nFirst, as we can see in Figure 1, the Gentoo penguin has different culmen and flipper measurements than the Chinstrap and Adelie penguins. Specifically, the Gentoo penguin seems to have a larger flipper length and a shallower culmen depth than the Chinstrap and Adelie penguins. This relationship could be used to identify the Gentoo penguins, but we still need to find a way to distinguish between the Chinstrap and Adelie penguins.\nFigure 2 provides a way to do this; the Chinstrap penguins are only found on Dream Island. We can also see that the Adelie penguins are found on all three islands, and the Gentoo penguin is only found on Biscoe Island. In the summary table, we can see that there is also a difference in culmen length between the species. The Chinstrap and Gentoo penguins have similar average culmen lengths, but the Adelie penguins have much shorter culmens on average.\nUsing some combination of these features, we should be able to accurately predict the species of any penguin in the training set. We will use tree-based feautre selection to determine which of the features we will use to train our model.\n\n\nOptimizing model parameters\nWe will be using a random forest to choose the best features to train our model on. We’ll also use a random forest to execute our classification. First, let’s understand what a random forest is:\n\nRandom Forest\n\nRandom forests are an ensemble learning method. That is, they combine predictions from multiple models to make a final prediction. Random forests use multiple decision trees to make a prediction. Each tree is trained on a different subset of the training set, so different trees can learn different patterns in the data. The trees use a process of “voting” to determine the prediction–whichever species is predicted by the most trees is the final prediction.\n\n\n\nChoosing the best features\n\n# Fit a model to find the best features in the training set\ntree_selector = ExtraTreesClassifier(n_estimators=100, random_state=42)\ntree_selector.fit(X_train, y_train)\nfeature_importances = tree_selector.feature_importances_\n\n# Create DataFrame with the feature and its importance score\nfeature_scores = pd.DataFrame({\"Feature\": X_train.columns, \"Importance\": feature_importances})\nfeature_scores = feature_scores.sort_values(by=\"Importance\", ascending=False)\n\n\n\nTop 3 Features:\n               Feature  Importance\n0   Culmen Length (mm)    0.166972\n2  Flipper Length (mm)    0.139796\n6        Island_Biscoe    0.132696\n\n\nWhen we use the ExtraTreesClassifier function, we are building a random forest from the training set. A random subset of features is used for each tree, then the best features are selected based on an importance score. The importance score is calculated using an impurity score for each feature that captures how well a feature is able to partition the dataset. These scores are then normalized to create the importance score. We can see from the feature selector that the culmen length, flipper length, and the island are the most important features in the dataset. So, we will use these features to train our model.\n\n\nChoosing maximum tree depth\nOne of the parameters for any tree method is the maximum depth at which the tree can go. That is, the number of partitions of the dataset allowed in each tree. In order to find the optimal maximum depth, we will use cross validation to test a range of potential maximum depths:\n\n# The columns that we want to include in our analysis -- based on the highest-scoring features\ncols = [\"Flipper Length (mm)\", \"Culmen Length (mm)\", \"Island_Biscoe\", \"Island_Dream\",\"Island_Torgersen\"]\n\ndepths = range(1, 21)\nmean_scores = []\n\n# Loop through potential depths and use cross validation to score each depth\nfor depth in depths:\n    dt = RandomForestClassifier(max_depth=depth, random_state=42)\n    scores = cross_val_score(dt, X_train[cols], y_train, cv=5, scoring='accuracy')\n    mean_scores.append(np.mean(scores))\n\n# Find the depth with the highest scoring validation\nbest_depth = depths[np.argmax(mean_scores)]\nbest_score = max(mean_scores)\n\n\n\nBest max tree depth: 7\nBest cross-validated accuracy: 0.9843891402714933\n\n\nBased on the cross-validation, a random forest with maximum depth 7 is the best option for our model.\n\n\n\nFitting the model\nNow that we know which features we want to use and have done some work to optimize the parameters for our random forest, we’re ready to train a random forest on our training set.\n\n# Initialize the model and fit it to the train set\ndt_model = RandomForestClassifier(max_depth=5, random_state=42)\ndt_model.fit(X_train[cols], y_train)\n\n# Score the model accuracy to see how well it did \ntrain_accuracy = dt_model.score(X_train[cols], y_train)\n\n\n\nTraining Accuracy: 0.9961\n\n\nThis random forest works pretty well! It was 99% accurate on our testing set. Now that we’ve trained this random forest tree, we can test it on the test dataset.\n\n# Read in the test data\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\ntest[\"Species\"].replace('Adelie Penguin (Pygoscelis adeliae)','Adelie',inplace=True)\ntest[\"Species\"].replace('Chinstrap penguin (Pygoscelis antarctica)','Chinstrap',inplace=True)\ntest[\"Species\"].replace('Gentoo penguin (Pygoscelis papua)','Gentoo',inplace=True)\n\nX_test, y_test = prepare_data(test)\ntest_acc = dt_model.score(X_test[cols], y_test)\n\n\n\nTesting Accuracy: 0.9853\n\n\n\n\nUnderstanding the results\nLet’s get a better understanding of what it’s actually doing. First, we can use the confusion matrix to look at how all of the penguins were classified.\n\ny_test_pred = dt_model.predict(X_test[cols])\nconfusion_matrix(y_test, y_test_pred)\n\narray([[31,  0,  0],\n       [ 0, 11,  0],\n       [ 1,  0, 25]])\n\n\nWhat does this matrix mean? We can understand it using each species as follows:\n\n\nThere were 31 Adelie penguin(s) who were classified as Adelie penguin(s).\nThere were 0 Adelie penguin(s) who were classified as Chinstrap penguin(s).\nThere were 0 Adelie penguin(s) who were classified as Gentoo penguin(s).\nThere were 0 Chinstrap penguin(s) who were classified as Adelie penguin(s).\nThere were 11 Chinstrap penguin(s) who were classified as Chinstrap penguin(s).\nThere were 0 Chinstrap penguin(s) who were classified as Gentoo penguin(s).\nThere were 1 Gentoo penguin(s) who were classified as Adelie penguin(s).\nThere were 0 Gentoo penguin(s) who were classified as Chinstrap penguin(s).\nThere were 25 Gentoo penguin(s) who were classified as Gentoo penguin(s).\n\n\nThis tells us that our model only misclassified one penguin – a Gentoo classified as an Adelie. We can plot the decision regions for each island to get an understanding of why this happened.\n\n\nPlotting Code\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Gentoo\", \"Chinstrap\", \"Adelie\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\n\n\n\n\n\n\n\n\n\n\nThese plots show us how our model made its classifications. On the far left, we can see that for the Biscoe Island data, the model was able to correctly distinguish between the Adelie (blue) and Gentoo (red) penguins using the flipper length. Moving to the far right, on Torgensen Island, there are only Adelie penguins, so the model was able to classify all of them correctly. In the middle plot we can see that the penguins on Dream Island were harder to classify – we can see where the model classified a Chinstrap (green) penguin as an Adelie penguin (blue) because it had a shorter culmen than the other Chinstrap penguins, causing its point to be within the blue region.\n\n\nDiscussion\nWe can see that our random forest was a very accurate model. It only misclassified one of the penguins, giving us 98.5% testing accuracy. We can see from the decision regions that the random forest was able to learn patterns within each island fairly well, even creating some accurate non-linear decision regions. In order to get more accuracy for the Dream island data, we would probably need to use another classification method that could learn the differences between the Chinstrap and Adelie penguins on Dream island. Or, going back to our initial visualizations, we could use an ensemble learning method that first uses one model to separate one species from the other two using the quantitative measurements and another that uses a qualitative feature like the island to distinguish between penguins with similar physical measurements."
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "from source import Perceptron\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/new-test-post/index.html",
    "href": "posts/new-test-post/index.html",
    "title": "Second Post",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-test-post/index.html#math",
    "href": "posts/new-test-post/index.html#math",
    "title": "Second Post",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Classifying Palmer Penguins\n\n\n\n\n\nClassifying Palmer Penguins\n\n\n\n\n\nFeb 13, 2025\n\n\nCarly McAdam\n\n\n\n\n\n\n\n\n\n\n\n\nSecond Post\n\n\n\n\n\nA new blog post that I just made!\n\n\n\n\n\nMar 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\n\n\n\n\n\n\nTimnit Gebru\n\n\n\n\n\nA new blog post that I just made!\n\n\n\n\n\nMar 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\n\n\n\n\n\n\nHello Blog\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  }
]