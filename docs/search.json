[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is Carly’s CSCI 451 blog"
  },
  {
    "objectID": "posts/new-new-test-post/index.html",
    "href": "posts/new-new-test-post/index.html",
    "title": "Timnit Gebru",
    "section": "",
    "text": "from source import Perceptron\np = Perceptron()\n\nI did it!!\nnot implemented\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-new-test-post/index.html#math",
    "href": "posts/new-new-test-post/index.html#math",
    "title": "Timnit Gebru",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/new-test-post/index.html",
    "href": "posts/new-test-post/index.html",
    "title": "Second Post",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-test-post/index.html#math",
    "href": "posts/new-test-post/index.html#math",
    "title": "Second Post",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/implementing-logistic-regression/index.html",
    "href": "posts/implementing-logistic-regression/index.html",
    "title": "Implementing Logistic Regression",
    "section": "",
    "text": "Abstract\nFind the source code here\nIn this post, we will explore a custom logistic regression method, the details of which can be seen in the source code (link above). We will perform some experiments using our logistic regression model, starting with a vanilla gradient descent method. This first experiment will be an example of how the method works at its simplest. We will do a second experiment with the momentum parameter that will highlight the impact that momentum can have on the time it takes for gradient descent to converge. Next, we’ll look at the potential for overfitting by testing our model on an overparameterized dataset and generalizing the results to a test dataset. Finally, we’ll test out our model on a real dataset to predict whether Pima Indians have diabetes.\n\n\nExperiments\n\nGenerating Experimental Data\n\ndef classification_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    y = 1.0*y\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    \n    return X, y\n\nX, y = classification_data(noise = 0.5)\n\n\n\nExperiments\n\nVanilla Gradient Descent\nWe’ll start with our most simple experiment. We want to look at a case with a small value for \\(\\alpha\\) and with \\(\\beta = 0\\) and show that this method of descending down the gradient to minimize the loss function will converge to a ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍weight ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍vector ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍w that looks visually correct. That is, when we plot the decision boundary we should see that this algorithm is able to split the data fairly well. We also want to show that as we do more iterations, the loss should be decreasing monotonically, which means that it is only decreasing and never increases as we do more iterations. We’ll plot both the decision boundary and the loss function after implementing the gradient descent method to check that these criteria have been met. First, we will generate some data using the above function and train it using a gradient descent loop.\n\n# generate classification data\nX, y = classification_data(n_points = 500, noise=0.5, p_dims=2)\n\n# create model and optimizer\nmodel = LogisticRegression()\noptimizer = GradientDescentOptimizer(model)\n\n# track loss over time\nlosses_vanilla = []\n\n# run vanilla gradient descent loop (train the model)\nfor _ in range(100):\n    loss_vanilla = model.loss(X, y).item()\n    losses_vanilla.append(loss_vanilla)\n    optimizer.step(X, y, alpha=0.1, beta=0.0)\n\n\n\nPlotting code for decision boundaries and loss\n# plot the loss over iterations\nplt.plot(losses_vanilla)\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Logistic Loss\")\nplt.title(\"Loss Over Time (Vanilla Gradient Descent)\")\nplt.grid(True)\nplt.show()\n\n# plot decision boundary with data\ndef plot_decision_boundary(model, X, y):\n    x_min, x_max = X[:,0].min() - 0.5, X[:,0].max() + 0.5\n    y_min, y_max = X[:,1].min() - 0.5, X[:,1].max() + 0.5\n    xx, yy = torch.meshgrid(torch.linspace(x_min, x_max, 100),\n                            torch.linspace(y_min, y_max, 100),\n                            indexing='xy')\n    \n    # add bias column of ones\n    grid = torch.cat((xx.reshape(-1,1), yy.reshape(-1,1), torch.ones((xx.numel(), 1))), dim=1)\n    preds = model.predict(grid).reshape(xx.shape)\n\n    plt.contourf(xx, yy, preds, alpha=0.5, cmap=\"RdBu\")\n    plt.scatter(X[:,0], X[:,1], c=y, cmap=\"RdBu\", edgecolor='k')\n    plt.title(\"Decision Boundary (Logistic Regression)\")\n    plt.show()\n\nplot_decision_boundary(model, X, y)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLooking at these plots, we can see that both of the criteria we described above are met by the vanilla gradient descent model. The loss function is only decreasing as we do more iterations of the gradient descent process, and we can see that the decision boundary generated by the model was able to split the data fairly well, although not perfectly. This makes sense because our dataset has noise and we can see from the plot that it would be impossible to separate this data using a linear decision boundary.\n\n\nBenefits ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍of ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍momentum\nNow, we’ll take a look at what this process looks like on the same data with a different \\(\\beta\\) (momentum) value. Gradient descent with momentum can converge to the correct weights in fewer iterations than it does without momentum (\\(\\beta = 0\\), as seen above with vanilla gradient descent).\n\n# run momentum gradient descent loop (train the model)\nlosses_momentum = []\nfor _ in range(100):\n    loss_momentum = model.loss(X, y).item()\n    losses_momentum.append(loss_momentum)\n    optimizer.step(X, y, alpha=0.1, beta=0.9)\n\n\n# plot the loss over iterations for momentum\nplt.plot(losses_vanilla, label=\"Vanilla GD (β = 0)\")\nplt.plot(losses_momentum, label=\"Momentum GD (β = 0.9)\")\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Logistic Loss\")\nplt.title(\"Loss vs Iteration: Vanilla vs Momentum\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nIn the plot we can see that when we add momentum to the gradient descent method (orange line), the loss is minimized more quickly than the vanilla gradient descent process (blue line). Thus, momentum can be a beneficial addition to the gradient descent process.\n\n\nOverfitting\nNow, we’ll take a look at an example of overfitting. We will generate two datasets, both with more dimensions (features) than the number points. Then, we’ll train a model on the train set that achieves 100% accurate for this train set and we will see how this model performs on the test set.\n\n# generate data\nX_train, y_train = classification_data(n_points = 50, noise=0.5, p_dims=100)\nX_test, y_test = classification_data(n_points = 50, noise=0.5, p_dims=100)\n\n\n# initialize model\nmodel = LogisticRegression()\nopt = GradientDescentOptimizer(model)\n\n# train model\nfor _ in range(300):\n    opt.step(X_train, y_train, alpha=0.1, beta=0.5)\n\n# evaluate model on train set\ntrain_preds = model.predict(X_train)\ntrain_acc = (train_preds == y_train).float().mean().item()\n\n\n\nTrain Accuracy: 100.00%\n\n\nNow that we have a model that performs with 100% accuracy on the train set, let’s test it using the test set.\n\n# evaluate model on train set\ntest_preds = model.predict(X_test)\ntest_acc = (test_preds == y_test).float().mean().item()\n\n\n\nTest Accuracy: 86.00%\n\n\nAs we can see from the lower testing accuracy than training accuracy, this is an example of overfitting. We created an overparameterized classification problem by making more features than points in the dataset. So, although the model was able to fit the training data perfectly, it was fitting a model that was too complex to generalize well. This is why we saw a much lower accuracy on the train set – the model learned patterns in the train data that didn’t actually apply to the data more generally.\n\n\nPerformance ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍on ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍empirical ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍data\nNow, we’ll test out this model on a real dataset. We’ll use a dataset called the Pima Indians Diabetes Dataset that originates from the National Institute of Diabetes and Digestive and Kidney Diseases and is available through the UCI Machine Learning Repository. ​The dataset comprises medical records for 768 female patients of Pima Indian heritage, aged 21 and above. Each record includes 8 numerical attributes related to medical measurements and personal information, along with a binary outcome indicating the presence or absence of diabetes. First, we’ll load and process the data so that we can use it with our custom logistic regression model. We need to normalize the features and add a bias column before train-test splitting.\n\n# load and preprocess the dataset\nurl = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\ncolumn_names = [\n    \"Pregnancies\", \"Glucose\", \"BloodPressure\", \"SkinThickness\", \"Insulin\",\n    \"BMI\", \"DiabetesPedigreeFunction\", \"Age\", \"Outcome\"\n]\ndf = pd.read_csv(url, names=column_names)\n\n\n\n\n\n\n\n\nPregnancies\nGlucose\nBloodPressure\nSkinThickness\nInsulin\nBMI\nDiabetesPedigreeFunction\nAge\nOutcome\n\n\n\n\n0\n6\n148\n72\n35\n0\n33.6\n0.627\n50\n1\n\n\n1\n1\n85\n66\n29\n0\n26.6\n0.351\n31\n0\n\n\n2\n8\n183\n64\n0\n0\n23.3\n0.672\n32\n1\n\n\n3\n1\n89\n66\n23\n94\n28.1\n0.167\n21\n0\n\n\n4\n0\n137\n40\n35\n168\n43.1\n2.288\n33\n1\n\n\n\n\n\n\n\n\n# normalize features\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\n\n\n# add bias column -- ensures compatibility with our custom model!\nX = torch.tensor(X, dtype=torch.float32)\nX = torch.cat([X, torch.ones((X.shape[0], 1))], dim=1)\ny = torch.tensor(y, dtype=torch.float32)\n\n/var/folders/23/gcqcls_x0glfvcgh87xqjsjr0000gn/T/ipykernel_90083/16192820.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  y = torch.tensor(y, dtype=torch.float32)\n\n\n\n# split off 20% of the data for the test set\nX_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# split remaining 80% into train (60%) and val (20%)\nX_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25, random_state=42)\n\nNow, we’ll train the model using the same method that we did above.\n\ndef train_model(X_train, y_train, X_val, y_val, alpha=0.1, beta=0.0, steps=100):\n    model = LogisticRegression()\n    opt = GradientDescentOptimizer(model)\n    train_losses = []\n    val_losses = []\n\n    for _ in range(steps):\n        # train model on the training set\n        train_losses.append(model.loss(X_train, y_train).item())\n        val_losses.append(model.loss(X_val, y_val).item())\n        opt.step(X_train, y_train, alpha=alpha, beta=beta)\n\n    return model, train_losses, val_losses\n\n# train model without momentum\nmodel_vanilla, losses_train_vanilla, losses_val_vanilla = train_model(X_train, y_train, X_val, y_val, beta=0.0)\n\n# train model with momentum\nmodel_momentum, losses_train_momentum, losses_val_momentum = train_model(X_train, y_train, X_val, y_val, beta=0.9)\n\nNow that we’ve trained the model two different ways (on both the train and validation sets) let’s take a the training ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍and ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍validation ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍loss ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍over the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍iterations, both with and without momentum.‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍\n\nimport matplotlib.pyplot as plt\n\nplt.plot(losses_train_vanilla, label=\"Train Loss (β=0.0)\", linestyle='-')\nplt.plot(losses_val_vanilla, label=\"Val Loss (β=0.0)\", linestyle='--')\nplt.plot(losses_train_momentum, label=\"Train Loss (β=0.9)\", linestyle='-')\nplt.plot(losses_val_momentum, label=\"Val Loss (β=0.9)\", linestyle='--')\n\nplt.xlabel(\"Iterations\")\nplt.ylabel(\"Loss\")\nplt.title(\"Training & Validation Loss\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nWe can see from the plot above that both the training a validation loss were minimized more quickly with momentum than without, which is expected given our above experiment with momentum. Now, let’s use the test set to see how accurate our model is on this dataset.\n\ndef evaluate(model, X, y):\n    preds = model.predict(X)\n    acc = (preds == y).float().mean().item()\n    loss = model.loss(X, y).item()\n    return acc, loss\n\ntest_acc, test_loss = evaluate(model_momentum, X_test, y_test)\n\nTest Accuracy: 89.00%\nTest Loss: 0.22\n\n\nOur model is fairly accurate – 89% on the test set.\n\n\n\n\nDiscussion\nThis process gave us a better understanding of how our logistic regression model works, and some of its limitations. We used vanilla gradient descent to look at how the gradient descent process works, both how decision boundaries are made and how the loss decreases as we do more iterations. We then gained an understanding the impact that the momentum parameter can have on the speed at which gradient descent converges by adding momentum to the vanilla gradient descent process, which made the loss minimize faster than without the momentum parameter. We also explored what can happen if we overfit the model to the training dataset – significantly decreased accuracy on the test set. Finally, we saw that the model was fairly accurate on a real world example and we were able to predict with ~90% accuracy whether Pima Indians have diabetes or not."
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "from source import Perceptron\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/auditing-bias/index.html",
    "href": "posts/auditing-bias/index.html",
    "title": "Auditing Bias",
    "section": "",
    "text": "Abstract\nIn this blog post we’ll be looking at census data and trying to predict whether a person will have an income over $50,000. We will not use a persons’ sex to predict their income. After training and implementing this algorithm, we will audit our model for gender bias. We will see that although the model doesn’t appear to perform too differently across male and female groups, there are some differences in the model performance, which we will explore further and discuss.\n\n\nPulling and Prepping the Data\n\n# Read in the data\n\nSTATE = \"CA\"\n\ndata_source = ACSDataSource(survey_year='2018', \n                            horizon='1-Year', \n                            survey='person')\n\nacs_data = data_source.get_data(states=[STATE], download=True)\nacs_data=acs_data[['PINCP', 'AGEP', 'SCHL', 'MAR', 'RELP', 'DIS', 'ESP', 'CIT', 'MIG', 'MIL', 'ANC', 'NATIVITY', 'DEAR', 'DEYE', 'DREM', 'SEX', 'RAC1P', 'ESR']]\n\nacs_data.head()\n\n\n\n\n\n\n\n\nPINCP\nAGEP\nSCHL\nMAR\nRELP\nDIS\nESP\nCIT\nMIG\nMIL\nANC\nNATIVITY\nDEAR\nDEYE\nDREM\nSEX\nRAC1P\nESR\n\n\n\n\n0\n48500.0\n30\n14.0\n1\n16\n2\nNaN\n1\n3.0\n4.0\n1\n1\n2\n2\n2.0\n1\n8\n6.0\n\n\n1\n0.0\n18\n14.0\n5\n17\n2\nNaN\n1\n1.0\n4.0\n1\n1\n2\n2\n2.0\n2\n1\n6.0\n\n\n2\n13100.0\n69\n17.0\n1\n17\n1\nNaN\n1\n1.0\n2.0\n2\n1\n2\n2\n2.0\n1\n9\n6.0\n\n\n3\n0.0\n25\n1.0\n5\n17\n1\nNaN\n1\n1.0\n4.0\n1\n1\n1\n2\n1.0\n1\n1\n6.0\n\n\n4\n0.0\n31\n18.0\n5\n16\n2\nNaN\n1\n1.0\n4.0\n1\n1\n2\n2\n2.0\n2\n1\n6.0\n\n\n\n\n\n\n\n\nfeatures_to_use = ['AGEP', 'SCHL', 'MAR', 'RELP']\n\nFirst, we’re reading in the census data. We’re also going to select some features that we’re going to use for our mode. We’ll be predicting income, so I chose some features that I think will be relevant – age, education level, marriage status, and relationship to “head of household”. Now, ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍we’ll ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍construct ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍a ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍BasicProblem that expresses that we are going to use the features we specified above to predict if a person’s income (“PINCP”) is over $50,000.\n\nIncomeProblem = BasicProblem(\n    features=features_to_use,\n    target='PINCP',\n    target_transform=lambda x: x &gt; 50000,\n    group='SEX',\n    preprocess=lambda x: x,\n    postprocess=lambda x: np.nan_to_num(x, -1),\n)\nfeatures, label, group = IncomeProblem.df_to_numpy(acs_data)\n\n\n\nBasic Descriptives\nBefore we get into creating our model, let’s get to know our dataset so that we understand the demographics that we’re working with. We’ll split our dataset into train and test sets.\n\nX_train, X_test, y_train, y_test, group_train, group_test = train_test_split(\n    features, label, group, test_size=0.2, random_state=0)\n\nThis is what our dataset looks like. It has all of the features that we’re going to use to train our model, and it has a “group” column where 1 represents a male and 2 represents a female. The “label” column, which we will isolate as our target variable, is a boolean (true for a person with income &gt; $50,000, false otherwise).\n\n\n\n\n\n\n\n\n\nAGEP\nSCHL\nMAR\nRELP\ngroup\nlabel\n\n\n\n\n0\n77.0\n16.0\n2.0\n0.0\n2\nTrue\n\n\n1\n29.0\n20.0\n5.0\n2.0\n1\nFalse\n\n\n2\n60.0\n21.0\n1.0\n0.0\n2\nFalse\n\n\n3\n27.0\n19.0\n5.0\n13.0\n2\nFalse\n\n\n4\n63.0\n23.0\n3.0\n0.0\n1\nTrue\n\n\n\n\n\n\n\nNow, let’s get some information about the data:\n\n\nNumber of individuals in the training set: 303053\n\n\n\n\nNumber of individuals in the training set with income over $50,000: 73963\n\n\n\n\nNumber of males in the training set: 149294\nNumber of females in the training set: 153759\n\n\n\n\nMale proportion w/ income over $50,000: 0.3\nFemale proportion w/ income over $50,000: 0.19\n\n\nWe can see that there are more females than males in the training set, although not by much. It’s almost evenly split. There are also about 10% more males than females have an income of over $50,000. It’s important to understand the breakdown in our dataset because if we’re looking for a model to perform well for both groups, we want there to be enough data in the training set for each group. We also want to be sure that we have people from each group with income above and below the threshold, so that the model can learn patterns about people in each group with each label. Now, let’s look at intersectional trends in the data by breaking out the income groups by sex and education level.\n\n\nPlotting code\n# Compute proportions of positive labels by both SEX and SCHL\nintersectional_counts = df.groupby([\"group\", \"SCHL\"])[\"label\"].mean().reset_index()\n\n# Rename columns for clarity\nintersectional_counts.columns = [\"Sex\", \"Education Level\", \"Proportion with Positive Label\"]\n\n# Create a bar plot\nplt.figure(figsize=(12, 6))\nsns.barplot(\n    data=intersectional_counts, \n    x=\"Education Level\", \n    y=\"Proportion with Positive Label\", \n    hue=\"Sex\"\n)\n\n# Labels and title\nplt.xlabel(\"Education Level (SCHL)\")\nplt.ylabel(\"Proportion with Positive Label (e.g., Income &gt; $50k)\")\nplt.title(\"Proportion of Positive Labels by Sex and Education Level\")\nplt.legend(title=\"Sex\")\n\n# Rotate x-axis labels for better readability\nplt.xticks(rotation=45)\n\n# Show plot\nplt.show()\n\n\n\n\n\n\n\n\n\nHere, we can see that across all education levels, men are more likely to have an income over $50,000 than women are. We can also see that this gap seems to be exacerbated at lower education levels. There is a bigger gap between the proportion of men and proportion of women with income over $50,000 at 15-19 years of education than at the higher education levels of 21-24 years. This is an important example of intersectional trends, because we can see that the income disparity between men and women varies depending on education level.\n\n\nTraining the Model\nNow that we understand our dataset, we’ll train a support vector machine model to predict if an individual’s income is over $50,000. An SVM model aims to find an optimal hyperplane in an N-dimensional space to separate data points into different classes. It is similar to a linear classification model, but it can be in multiple dimensions. The SVM algorithm maximizes the margin between the closest points of different classes. One of the parameters in an SVM model is the regularization term “C”, which is used to balance the misclassification penalties with maximization. If the value of “C” is higher, the model more strictly penalizes misclassifications. Before training our model, we will use cross-validation to find an optimal value of this “C” parameter.\n\n\nCross-val code\nmodel = make_pipeline(StandardScaler(), LinearSVC(dual=False))\nmodel.fit(X_train, y_train)\n\n# Define the pipeline\npipeline = make_pipeline(StandardScaler(), LinearSVC(dual=False, max_iter=5000))\n\n# Define the hyperparameter grid for C\nparam_grid = {'linearsvc__C': [0.001, 0.01, 0.1, 1, 10, 100]}\n\n# Perform Grid Search with Cross-Validation\ngrid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\ngrid_search.fit(X_train, y_train)\n\n# Get the best C value\nbest_C = grid_search.best_params_['linearsvc__C']\nprint(f\"Best C: {best_C}\")\n\n# Best model with optimal C\nbest_model = grid_search.best_estimator_\n\n\nBest C: 0.001\n\n\nBased on our cross-validation, the value of “C” that we should use is 0.001. Now, we’ll create and train our model using a pipeline. The pipeline is used to sequentially apply transformations – here, we are using StandardScaler() to standardize all of the features in the dataset.\n\nmodel = make_pipeline(StandardScaler(), LinearSVC(C=0.001, dual=False))\nmodel.fit(X_train, y_train)\n\nPipeline(steps=[('standardscaler', StandardScaler()),\n                ('linearsvc', LinearSVC(C=0.001, dual=False))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('standardscaler', StandardScaler()),\n                ('linearsvc', LinearSVC(C=0.001, dual=False))])StandardScalerStandardScaler()LinearSVCLinearSVC(C=0.001, dual=False)\n\n\nNow, that we’ve fit our model, we’re going to test it on the test set and then audit the model for gender bias. Recall that we didn’t include sex as one of the features in our model, so we’re interested to see if the model we trained on other features might be implicitly biased by sex.\n\n\nAuditing the Model\n\ny_hat = model.predict(X_test)\n\n\n\nOverall accuracy: 0.8\n\n\n\n\nAccuracy for men: 0.79\n\n\n\n\nAccuracy for women: 0.81\n\n\nAt first glance, we can see that our model is slightly more accurate for women, but it performs fairly similarly across each group when looking at accurate. But, let’s get more precise to understand more about the differences in accuracy between groups. We’ll get the accuracy using the accuracy score function, the PPV using the precision score function, and the true positive, true negative, false negative, and false positive rates from the confusion matrix. We’ll get all of these metrics for the whole test set, as well as the test set filtered for the female and male groups. We will put all of these metrics into a dictionary for easy access when we plot these results next.\n\n\nCode to calculate accuracy and error rates\n# Make predictions\ny_pred = best_model.predict(X_test)\n\n### Overall Measures ###\n# Compute overall accuracy\noverall_accuracy = accuracy_score(y_test, y_pred)\n\n# Compute overall PPV (Positive Predictive Value / Precision)\noverall_ppv = precision_score(y_test, y_pred)\n\n# Compute overall confusion matrix\ntn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n\n# Compute overall False Positive Rate (FPR) and False Negative Rate (FNR)\noverall_fpr = fp / (fp + tn)\noverall_fnr = fn / (fn + tp)\n\n# Print overall metrics\nprint(\"\\n--- Overall Model Performance ---\")\nprint(f\"Overall Accuracy: {overall_accuracy:.2f}\")\nprint(f\"Overall PPV: {overall_ppv:.2f}\")\nprint(f\"Overall False Positive Rate (FPR): {overall_fpr:.2f}\")\nprint(f\"Overall False Negative Rate (FNR): {overall_fnr:.2f}\")\n\n### By-Group Measures ###\nprint(\"\\n--- Subgroup Model Performance ---\")\nsubgroup_results = []\n\nfor sex in [1, 2]:  # 1 = Male, 2 = Female\n    mask = (group_test == sex)\n    \n    # Compute subgroup accuracy\n    subgroup_accuracy = accuracy_score(y_test[mask], y_pred[mask])\n    \n    # Compute subgroup PPV\n    subgroup_ppv = precision_score(y_test[mask], y_pred[mask])\n    \n    # Compute subgroup confusion matrix\n    tn_s, fp_s, fn_s, tp_s = confusion_matrix(y_test[mask], y_pred[mask]).ravel()\n\n    # Compute subgroup FPR and FNR\n    subgroup_fpr = fp_s / (fp_s + tn_s)\n    subgroup_fnr = fn_s / (fn_s + tp_s)\n    \n    group_name = \"Male\" if sex == 1 else \"Female\"\n    print(f\"\\nGroup: {group_name}\")\n    print(f\"  Accuracy: {subgroup_accuracy:.2f}\")\n    print(f\"  PPV: {subgroup_ppv:.2f}\")\n    print(f\"  False Positive Rate (FPR): {subgroup_fpr:.2f}\")\n    print(f\"  False Negative Rate (FNR): {subgroup_fnr:.2f}\")\n\n    subgroup_results.append({\n        \"Sex\": group_name,\n        \"Accuracy\": subgroup_accuracy,\n        \"PPV\": subgroup_ppv,\n        \"FPR\": subgroup_fpr,\n        \"FNR\": subgroup_fnr\n    })\n\n### Bias Measures ###\n# Calibration: The model is considered calibrated if P(Y=1 | S=1) ≈ P(Y=1 | S=2)\ncalibration_diff = abs(y_pred[group_test == 1].mean() - y_pred[group_test == 2].mean())\n\n# Error Rate Balance: FNR and FPR should be similar across groups\nerror_rate_balance = abs(subgroup_results[0][\"FNR\"] - subgroup_results[1][\"FNR\"]) + \\\n                     abs(subgroup_results[0][\"FPR\"] - subgroup_results[1][\"FPR\"])\n\n# Statistical Parity: P(Y=1 | S=1) ≈ P(Y=1 | S=2)\nstatistical_parity_diff = abs(np.mean(y_pred[group_test == 1]) - np.mean(y_pred[group_test == 2]))\n\n# Print Bias Metrics\nprint(\"\\n--- Bias Metrics ---\")\nprint(f\"Calibration Difference: {calibration_diff:.2f}\")\nprint(f\"Error Rate Balance: {error_rate_balance:.2f}\")\nprint(f\"Statistical Parity Difference: {statistical_parity_diff:.2f}\")\n\n\n\n--- Overall Model Performance ---\nOverall Accuracy: 0.80\nOverall PPV: 0.63\nOverall False Positive Rate (FPR): 0.08\nOverall False Negative Rate (FNR): 0.56\n\n--- Subgroup Model Performance ---\n\nGroup: Male\n  Accuracy: 0.79\n  PPV: 0.75\n  False Positive Rate (FPR): 0.06\n  False Negative Rate (FNR): 0.56\n\nGroup: Female\n  Accuracy: 0.81\n  PPV: 0.50\n  False Positive Rate (FPR): 0.10\n  False Negative Rate (FNR): 0.57\n\n--- Bias Metrics ---\nCalibration Difference: 0.01\nError Rate Balance: 0.05\nStatistical Parity Difference: 0.01\n\n\nThese results tell us more information about the difference in model performance between the two groups. We can see that the male group has a lower false positive rate (FPR) than the female group, that is, our model is less likely to incorrectly predict that a man has income over $50,000 when he actually does not. The false negative rate (FNR) is very similar across groups, so we are pretty much equally likely to predict that a male or female does not have income over $50,000 when they actually do. The male group has a higher PPV value than the female group, which we’ll explore further in the plot below. We will reproduce a plot from the COMPAS paper that compares the FPR and FNR rates across groups and plots potential combinations of FPR/FNR rates for each group. The plot also looks at the potential combinations of FPR/FNR rates for the female group if we fix the PPV rate for the female group to the larger PPV value of the male group.\n\n\nCode to reproduce plot from COMPAS paper\n# Extract values for male and female\np_male = np.mean(y_test[group_test == 1])  # Prevalence of male\np_female = np.mean(y_test[group_test == 2])  # Prevalence of female\n\nPPV_female = subgroup_results[1][\"PPV\"] # Observed PPV for females\nPPV_male_fixed = PPV_female # Set PPV_male to PPV_female\n\n# Extract FNR and FPR values for observed points\nfnr_values = [subgroup_results[0][\"FNR\"], subgroup_results[1][\"FNR\"]]\nfpr_values = [subgroup_results[0][\"FPR\"], subgroup_results[1][\"FPR\"]]\nlabels = [subgroup_results[0][\"Sex\"], subgroup_results[1][\"Sex\"]]\n\n# Generate range of FNR values from 0 to 1\nfnr_range = np.linspace(0, 1, 100)\n\n# Compute feasible FPR values for males\nfpr_feasible_male = ((1 - fnr_range) * p_male * (1 - PPV_male_fixed)) / (PPV_male_fixed * (1 - p_male))\n\n# Compute feasible FPR values for females when PPV_female = PPV_male\nfpr_feasible_female = ((1 - fnr_range) * p_female * (1 - PPV_female)) / (PPV_female * (1 - p_female))\n\n# Define range of delta values for shading\ndelta_values = [0.05, 0.1, 0.15]  # Smaller delta -&gt; smaller feasible region\n\nplt.figure(figsize=(10, 6))\n\n# Scatter plot of observed values\nsns.scatterplot(x=fnr_values, y=fpr_values, hue=labels, palette=[\"lightblue\", \"lightpink\"], s=100)\n\n# Plot feasible set for males\nplt.plot(fnr_range, fpr_feasible_male, color=\"lightblue\", label=\"Feasible Set (Male)\")\n\n# Plot feasible set for females when PPV_female = PPV_male\nplt.plot(fnr_range, fpr_feasible_female, color=\"lightpink\", label=\"Feasible Set (Female, PPV=PPV_male)\")\n\n# Add nested shaded regions for varying PPV_female constraints\nfor i, delta in enumerate(delta_values):\n    # PPV_female_upper = PPV_male + delta\n    # PPV_female_lower = PPV_male - delta\n    \n    PPV_male_upper = PPV_female + delta\n    PPV_male_lower = PPV_female - delta\n\n\n    # Compute feasible FPR values for upper and lower PPV bounds\n    fpr_feasible_upper = ((1 - fnr_range) * p_male * (1 - PPV_male_upper)) / (PPV_male_upper * (1 - p_male))\n    fpr_feasible_lower = ((1 - fnr_range) * p_male * (1 - PPV_male_lower)) / (PPV_male_lower * (1 - p_male))\n\n    # Fill region between upper and lower bounds\n    plt.fill_between(fnr_range, fpr_feasible_lower, fpr_feasible_upper, color=\"lightgrey\", alpha=0.3)\n\n# Labels and title\nplt.xlabel(\"False Negative Rate (FNR)\")\nplt.ylabel(\"False Positive Rate (FPR)\")\nplt.title(\"Feasible (FNR, FPR) combinations\")\n\n# Show plot\nplt.show()\n\n\n\n\n\n\n\n\n\nThis chart is a reproduction of the Chouldechova paper that we discussed in class. We fixed ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the proportion of true positive labels for eac‍‍‍‍‍‍‍‍‍‍‍‍‍h group, and we set the PPV values to be equal across both groups. The pink and blue points show the observed FPR and FNR rates for the male and female groups. The lines represent the possible combinations of FPR and FNR for each group, with the PPV for the male group fixed at the lower PPV that we observed for the female group. Given this, we can see that if we wanted the male FPR to be the same as the female FPR, we would need to increase the false negative rate around 35% so that the blue point would shift right onto the blue line. This tells us that we would need to make the model for males much less accurate than it is now to make the models “equal” in their FPR.\n\n\nConcluding Discussion\nUltimately, this model didn’t exhibit that much bias. Even though the male group had a higher PPV, the accuracy, FPRs, and FNRs were similar across the two groups. There are many people that might be interested in this model, like people doing credit risk assessment, recruiters and employers, or banks. These institutions would all have an interest in predicting how much income an individual might make. At a large scale, this model might be slightly more accurate for predicting male income. This could potentially have a negative impact on females because they might be more likely to be denied credit or a job if their income is predicted to be lower than it actually is. This could lead to biased employment practices, unfair denial of services, or housing discrimination. On the positive side, this model could have the impact of streamlining financial services and job-matching or aiding governments in economic research and policy planning with access to income data. I didn’t feel that this model displays ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍problematic ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍bias. The error rates were very similar across groups, but I could see the potential for a model like this to be biased against women, given the systemic wage gap and sexism inherent in our society. Some of the other problems ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍associated with deploying this model, like privacy issues and regulatory compliance. To mitigate these potential impacts, it would be useful to conduct bias audits at a large scale, like the one we did here. It could also be important to ensure that humans, instead of the algorithm, end up having the final say about decisions that might be impacted by the model results."
  },
  {
    "objectID": "posts/replication-study/index.html",
    "href": "posts/replication-study/index.html",
    "title": "Replication Study",
    "section": "",
    "text": "Abstract\nIn this blog post, we will be replicating a study published by Obermeyer et al. in 2019. They found evidence of racial bias in one widely used algorithm. We will visualize how Black patients are often not flagged for extra care even when their white counterparts with the same health conditions are flagged. We will see that Black patients are labeled as lower risk because healthcare costs are used as a proxy for healthcare needs, and less money is spent on Black patients, making it seem like they have less need for extra care. We will quantify this result using a linear regression, and discuss which ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍statistical ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍discrimination ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍criteria describes the bias we found in this algorithm.\nFirst, we’ll read in and take a look at the dataset.\n\n# Read in the data\nurl = \"https://gitlab.com/labsysmed/dissecting-bias/-/raw/master/data/data_new.csv?inline=false\"\ndf = pd.read_csv(url)\n\n\n\n\n\n\n\n\n\n\nrisk_score_t\nprogram_enrolled_t\ncost_t\ncost_avoidable_t\nbps_mean_t\nghba1c_mean_t\nhct_mean_t\ncre_mean_t\nldl_mean_t\nrace\n...\ntrig_min-high_tm1\ntrig_min-normal_tm1\ntrig_mean-low_tm1\ntrig_mean-high_tm1\ntrig_mean-normal_tm1\ntrig_max-low_tm1\ntrig_max-high_tm1\ntrig_max-normal_tm1\ngagne_sum_tm1\ngagne_sum_t\n\n\n\n\n0\n1.987430\n0\n1200.0\n0.0\nNaN\n5.4\nNaN\n1.110000\n194.0\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n7.677934\n0\n2600.0\n0.0\n119.0\n5.5\n40.4\n0.860000\n93.0\nwhite\n...\n0\n1\n0\n0\n1\n0\n0\n1\n4\n3\n\n\n2\n0.407678\n0\n500.0\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n3\n0.798369\n0\n1300.0\n0.0\n117.0\nNaN\nNaN\nNaN\nNaN\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n4\n17.513165\n0\n1100.0\n0.0\n116.0\nNaN\n34.1\n1.303333\n53.0\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n\n\n\n\n5 rows × 160 columns\n\n\n\nWe will be particularly focused on these columns: - risk_score_t ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍is ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍algorithm’s ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍risk ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍score ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍assigned ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍to ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍a ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍given ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍patient. - cost_t ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍is ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍patient’s ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍medical ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍costs ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍in ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍study ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍period. - race ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍is ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍patient’s ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍self-reported ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍race. ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍The ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍authors ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍filtered ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍data ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍to ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍include ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍only ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍white ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍and ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍black ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍patients. - gagne_sum_t ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍is ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍total ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍number ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍of ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍chronic ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍illnesses ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍presented ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍by ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍patient ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍during ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍study ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍period.\n\n\nReproducing Fig. 1\nFirst, we’ll look at some visualizations to get an understanding of our dataset. The first plot we will reproduce from the paper will look at risk score percentiles against mean number of active chronic conditions within each percentile. To make this plot, we’ll start by calculating the percentiles for the risk scores.\n\n# Calculate risk score percentiles\ndf[\"risk_percentile\"] = pd.qcut(df[\"risk_score_t\"], 100, labels=False)\n\nNext, we’ll create a subplot for males and females to further split the data. To plot the risk scores and number of chronic conditions, we will iterate through each race in the dataset (just white and black) and take the mean number of chronic conditions for each risk score percentile. We will use different color and marker for each race, blue circles for black patients and green x’s for white patients.\n\n\nPlotting code for risk score vs. number chronic conditions\n# Define markers for race on our plots\nmarkers = {\"white\": \"x\", \"black\": \"o\"}\ncolors = {\"white\": \"green\", \"black\": \"blue\"}\n\nsns.set_style(\"whitegrid\")\n\n# Create subplots for male (0) and female (1) patients\nfig, axes = plt.subplots(1, 2, figsize=(12, 6), sharey=True)\n\nfor i, female in enumerate([0, 1]):  # 0 = Male, 1 = Female\n    ax = axes[i]\n    # Iterate through the races in the df (black, white)\n    for race in df[\"race\"].unique():\n        subset = df[(df[\"dem_female\"] == female) & (df[\"race\"] == race)]\n        grouped = subset.groupby(\"risk_percentile\")[\"gagne_sum_t\"].mean().reset_index()\n        sns.scatterplot(\n            data=grouped, \n            x=\"gagne_sum_t\", \n            y=\"risk_percentile\", \n            marker=markers[race], \n            color = colors[race],\n            label=f\"{race.capitalize()} Patients\", \n            ax=ax\n        )\n    ax.set_title(f\"{'Female' if female == 1 else 'Male'} Patients\")\n    ax.set_xlabel(\"Mean Chronic Conditions\")\n    if i == 0:\n        ax.set_ylabel(\"Risk Score Percentile\")\n    ax.legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nLet’s take a look at this plot to understand what might happen in this scenario: Patient A is black and Patient B is white and both patients have the same chronic illnesses. We can see from the plot that across both genders, it seems that Patient A and Patient B are not equally likely to be referred to the high-risk care management program. The blue points, which represent black patients, consistently have lower risk scores than green points that have at the same number of chronic illnesses. This would make it less likely for these black patients to be referred to a high-risk care management program because they are being scored as lower risk than white patients with the same illnesses.\n\n\nReproducing Fig. 3\nNow, we’ll reproduce another figure from the paper that should give us some more insight into why Black patients are being given lower risk scores than white patients with the same number of chronic conditions. First, we’ll group across risk percentile and race to take the mean total medical expenditure for each race at each risk percentile. Then, we’ll group by number of chronic conditions and race so that we can take the mean total medical expenditure for each race at each number of chronic conditions.\n\nrisk_grouped = df.groupby([\"risk_percentile\", \"race\"])[\"cost_t\"].mean().reset_index()\nchronic_illness_grouped = df.groupby([\"gagne_sum_t\", \"race\"])[\"cost_t\"].mean().reset_index()\n\nNext, we’ll make two subplots – one that plots total medical expenditure by risk score percentile and one that plots total medical expenditure by number of chronic conditions. Again, we will use blue circles for black patients and green x’s for white patients.\n\n\nPlotting code for medical expenditure vs. number chronic conditions and risk score\n# Create the figure with two panels\nfig, axes = plt.subplots(1, 2, figsize=(12, 6), sharey=True)\n\n# Left panel: Mean expenditure vs. Risk Score Percentile\nfor race in df[\"race\"].unique():\n    subset = risk_grouped[risk_grouped[\"race\"] == race]\n    sns.scatterplot(\n        data=subset,\n        x=\"risk_percentile\",\n        y=\"cost_t\",\n        marker=markers[race],  \n        color=colors[race],  \n        label=f\"{race.capitalize()} Patients\",\n        ax=axes[0]\n    )\n\naxes[0].set_title(\"Medical Expenditure vs. Risk Score Percentile\")\naxes[0].set_xlabel(\"Risk Score Percentile\")\naxes[0].set_ylabel(\"Mean Medical Expenditure ($)\")\naxes[0].set_ylim(1000, 10000)\naxes[0].legend()\naxes[0].set_yscale(\"log\")\n\n# Right panel: Medical Expenditure vs. Number of Chronic Conditions\nfor race in df[\"race\"].unique():\n    subset = chronic_illness_grouped[chronic_illness_grouped[\"race\"] == race]\n    sns.scatterplot(\n        data=subset,\n        x=\"gagne_sum_t\",\n        y=\"cost_t\",\n        marker=markers[race],  \n        color=colors[race],  \n        label=f\"{race.capitalize()} Patients\",\n        ax=axes[1]\n    )\n\naxes[1].set_title(\"Medical Expenditure vs. Chronic Conditions\")\naxes[1].set_xlabel(\"Number of Chronic Conditions\")\naxes[1].set_ylabel(\"\")\naxes[1].set_ylim(1000, 100000)\naxes[1].legend()\naxes[1].set_yscale(\"log\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nIn this plot, we can get an understanding of how total medical expenditures are correlated with risk score and the number of chronic illnesses. It seems from the left panel that there is more variation in the mean medical expenditures for black patients by risk score than there is for white patients. Although we see many blue points (black patients) below green points with the same risk score, we also see blue points above green points with the same risk score. There is an interesting trend in the right panel – it appears that white patients tend to have more medical expenditures than blank patients with the same number of chronic conditions when the number of conditions is less than 5, but as the number of conditions increases, black patients tend to have more medical expenditures than white patients with the same number of conditions. Considering that the vast majority of patients in this dataset have fewer than 5 chronic conditions, this dataset seems to show black patients, on average, having fewer medical expenditures than white patients with the same number of chronic conditions.\n\n\nModeling Cost Disparity\nNow that we’ve gotten an understanding of the dataset and what’s going on with bias here, let’s make a model so that we can quantify it more accurately.We’ll start with prepping the data.\n\nData Prep\n\n\nPercentage of patients in DF with 5 or fewer chronic conditions: 0.96 %\n\n\nGiven that the vast majority of people in the dataset have fewer than 5 chronic conditions, I think it makes sense to just focus on these people in our analysis. We will take the log of the medical expenditures because they have a large range.\n\n# Take the log of the cost column (also remove costs that are 0)\ndf = df[df[\"cost_t\"] &gt; 0]\ndf[\"log_cost\"] = np.log(df[\"cost_t\"])\n\nNext we’ll encode the race dummy as an 0-1 variable, with 1 for black individuals and 0 for white individuals.\n\n# Encode the race variable as an integer \ndf[\"race_dummy\"] = (df[\"race\"] == \"black\").astype(int)\n\nLastly, we will separate out our features (race and number of chronic conditions) from our target variable (total medical expenditure).\n\n# Separate predictor and target variables\nX = df[[\"race_dummy\", \"gagne_sum_t\"]]\ny = df[\"log_cost\"]\n\n\n\nModeling\nWe will fit a linear regression model with polynomial features in the number of active chronic conditions to account for the nonlinearity in the relationship between number of active chronic conditions that is apparent from the left hand panel in the above plot. We’ll first define a function to add polynomial features to the dataset.\n\ndef add_polynomial_features(X, degree):\n  X_ = X.copy()\n  for j in range(1, degree):\n    X_[f\"poly_{j}\"] = X_[\"gagne_sum_t\"]**j\n  return X_\n\nNext, we will use this function to loop through polynomial features from \\(1\\) to \\(11\\) and regularization strengths \\(10^k\\) from \\(k = -4, -3, ..., 3, 4\\). The regularization strength is a parameter that describes how much the training process should prefer “simple” models with small values of the weights \\(w\\) as opposed to a more complex model. At each degree-regularization combination, we’ll train a model and compute the cross-validated mean squared error of the model. We will take the combination with the smallest mean squared error and use that degree-regularization combination to train the model we will use for our analysis.\n\n# Ranges of degrees and ks for regularization\ndegrees = range(1, 12)      \nks = range(-4, 4)    \n\nresults = []\n\nfor degree in degrees:\n    # Add polynomial features to X_base\n    X_poly = add_polynomial_features(X, degree)\n    \n    # Loop through each regularization strength\n    for k in ks:\n        alpha = 10**k\n        # Create a Ridge regression model with the given alpha\n        model = Ridge(alpha=alpha)\n        \n        # Use cross-validation to estimate the mean squared error\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\"ignore\")\n            scores = cross_val_score(model, X_poly, y, cv=5, scoring=\"neg_mean_squared_error\")\n        # cross_val_score returns negative MSE when scoring is set to \"neg_mean_squared_error\" so we negate it for the mean\n        mse = -np.mean(scores)\n            \n        # Save the combination and results\n        results.append({\n            \"degree\": degree,\n            \"alpha\": alpha,\n            \"cv_mse\": mse\n        })\nresults_df = pd.DataFrame(results)\nresults_df = results_df.sort_values(\"cv_mse\")\n\n# Identify the best (lowest CV MSE) combination\nbest = round(results_df.iloc[0],2)\n\n\n\n\nBest combination:\ndegree    10.00\nalpha      1.00\ncv_mse     1.51\nName: 76, dtype: float64\n\n\nSo, we can see that our optimal parameter combination is \\(\\alpha = 1\\) with a degree of \\(10\\). Next let’s fit a model to the entire dataset using these parameters.\n\n# Fit a model on the entire dataset \nX_poly = add_polynomial_features(X, 10)\n\n# Create a Ridge regression model with the alpha=1\nmodel = Ridge(alpha=1)\n\n# Fit the model on the entire dataset to extract coefficients\nmodel.fit(X_poly, y)\n\n# Find the coefficient corresponding to race_dummy\ncoef_index = list(X_poly.columns).index(\"race_dummy\")\nrace_coef = model.coef_[coef_index]\nexp_race_coef = np.exp(race_coef)\n\n\n\n\nBlack coefficient: -0.27\n\n\nBased on these results, we can see that the coefficient associated with the Black individuals in the dataset is \\(w_b = -0.27\\). We can calculate \\(e^{w_b} = e^{-0.27} = 0.76\\). So, percentage of cost incurred by a Black patient in comparison to an equally sick white patient, is around 76%. This supports the argument that was made in the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍Obermeyer paper because it suggests that less money is spent on Black patients who have the same level of need as white patients.\n\n\n\nDiscussion\nAfter replicating the study done in the Obermeyer paper, we can see that because less money is spent on Black patients’ healthcare, they are often classified as lower risk than white patients who have the same chronic conditions. This has concerning implications, as it seems like this algorithm was making it harder for Black patients to access care than white patients, even if the Black patients are sicker than the white patients.\nIn terms of the formal ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍statistical ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍discrimination ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍criteria ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍discussed in Chapter 3 of ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍ ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍Barocas, Hardt, and Narayanan, the separation criterion best describes the bias that Obermeyer et al. found in their paper. This is because separation would require that the risk score be conditionally independent of race given true health need. That is, if two individuals have the same underlying health status, their risk scores should be statistically indistinguishable across racial groups. But, our results show that this isn’t true when we use this algorithm. Our results showed that Black patients with the same chronic conditions as white patients are assigned lower risk scores because the algorithm uses healthcare expenditures as a proxy for health need. Even though the algorithm may be able to predict health care costs effectively, these effective predictions are not an accurate proxy for health need, which makes the algorithm violate the separation criterion. Even though the model may be accurate, it is not necessarily unbiased.\nI learned a lot from replicating this study. I think I went into it with an understanding of the importance of accuracy and equality in a model, but without considering the implications of the model. Even though this model may have been accurate, the context in which it was being applied made it so that it was biased against Black individuals. Using one feature as a proxy for another feature seems tricky, and requires a lot of auditing to ensure that it doesn’t create bias."
  },
  {
    "objectID": "posts/limits-of-quanitative-approaches/index.html",
    "href": "posts/limits-of-quanitative-approaches/index.html",
    "title": "Limits of the Quantitative Approach to Bias and Fairness",
    "section": "",
    "text": "In a 2022 lecture at Princeton University, associate professor of computer science Arvind Narayanan stated: “currently, quantitative methods are primarily used to justify the status quo. I would argue that they do more harm than good” (Narayanan (2022)). As machine learning algorithms become more prevalent in our lives, and are used to aid in high-stakes decisions like prison sentencing, credit assessment, and pricing, it’s essential that we have a way to measure bias and fairness. Narayanan believes that the quantitative methods which we currently use to assess bias and fairness in machine learning are inadequate. In this essay, I will dissect Narayanan’s position on quantitative methods in bias and fairness, discuss some of the benefits of these quantitative methods, discuss some of the limitations of these quantitative methods, and unpack my own views on these methods.\nIn Narayanan’s speech, he discusses some key limitations he believes exists in our current quantitative methods for assessing fairness and bias in machine learning. He uses the COMPAS study that we have worked with in class as an example – he was initially excited about the data that exhibited bias in the COMPAS algorithm, but after further consideration he developed 7 serious limitations of the way quantitative methods are used to study discrimination. Although Narayanan believes that we should continue to use quantitative methods, he thinks these limitations need to be addressed if these methods continue to be used:\n\nWhat counts as evidence of discrimination is a subjective choice\nNull hypotheses allocate the burden of proof unfairly\nCompounding inequality is not detectable by quantitative methods\nSnapshot datasets can hide discrimination\nStatistical controls can mask discrimination\nDifferent fairness metrics can produce conflicting results\nNumbers are the language of policymaking, even when they are misleading\n\nWe’ll unpack these different ideas along the way as we bring in some more sources to highlight both the benefits and drawbacks of using these methods. First, let’s understand some of the ways in which these quantitative methods have been effective, despite their potential limitations.\nIn Fairness and Machine Learning, Barocas, Hardt, and Narayanan outline 3 quantitative definitions of fairness in machine learning: independence, separation, and sufficiency (Barocas, Hardt, and Narayanan (2023)). We’ll focus on independece here, which requires the sensitive characteristic to be statistically independent of the score. In the early 2010s, Amazon created and AI tool to aid their recruiting process by scoring candidates based on their resumes that did not meet this definition of independence across genders (Dastin (2018)). If we defined \\(R\\) as the score function and \\(\\hat{Y} = I\\{R &gt; t\\}\\) thresholds the score at \\(t\\):\n\\(P\\{\\hat{Y} | \\text{male candidate}\\} \\neq P\\{\\hat{Y} | \\text{female candidate}\\}\\)\nThat is, the probability of assigning a male candidate a score over the threshold was not the same as the probability of assigning a female candidate a score over the threshold. In this case, the male candidates were more likely to be assigned a higher score. This happened because the data the model was trained on was resumes submitted to the company over a 10-year period, of which most came from men. If resumes had the word “women” in them; for example, “women’s chess club captain”, these resumes were likely to be penalized. Once Amazon identified that the independence criteria was not being met for this system, the edited the tool for gender-neutrality, and decided to use the scores generated by the tool as one part of the hiring process, but not rule out any candidates based on the AI process. This example highlights some of the benefits of using quantitative methods to assess fairness and bias in machine learning algorithms. Amazon was able to use the quantitative criteria described above to identify a problem with its algorithm, and make changes to both the algorithm itself and the context in which it was used. Even though the methods like using the independence criteria certainly have some limitations, which we will discuss next, they are still able to quickly and objectively identify a problem and prompt developers to address it. Now that we’ve seen an example of the ways in which quantitative methods can be useful, we can start to understand the drawbacks and limitations of them.\nThe ProPublica article on machine bias, which we have looked at a few times in class, which analyzed the COMPAS algorithm for bias, is a great example of some of the concerns that Narayanan raises about quantitative methods to analyze bias (Angwin et al. (2016)). Even though the algorithm had similar accuracy rates across Black and White defendants, people using the algorithm failed to consider disparities in false positive and false negative rates between these groups. Many of the issues that Narayanan raises are at play here. First, there was a difference in what the algorithm’s creators thought counted as “fair” and what ProPublica thought was “fair”. To frame this in language from Fairness and Machine Learning, COMPAS’s creators were looking for the fact that, if \\(Y\\) is an indicator denoting whether or not a defendant is likely to recommit a crime and \\(\\hat{Y}\\) is an indicator denoting whether that defendant actually did recommit:\n\\(P\\{\\hat{Y} = Y| \\text{white defendant}\\} = P\\{\\hat{Y} = Y| \\text{black defendant}\\}\\)\nThat is, that the accuracy of the algorithm is the same for each racial group. In moral, this is the narrow view of equality as defined in Fairness and Machine Learning. But, the ProPublica authors wanted a more rigorous definition of fairness that aligns with the middle view of equality: better equality in false positive rates across racial groups. In more technical terms:\n\\(P\\{Y = 1 | \\hat{Y} = 0, \\text{white defendant}\\} = P\\{Y = 1 | \\hat{Y} = 0, \\text{black defendant}\\}\\)\nFor the algorithm’s creators, similar accuracy rates across racial groups counted as a fair algorithm, but ProPublica argued that the false positive rate for Black defendants was nearly twice that of white defendants – causing unfair differences in how Black defendants were treated. Second, the creators of COMPAS assumed the null hypothesis that their algorithm was fair unless proven otherwise, and their defense was that no direct racial variables were used in the model—shifting the burden of proof to critics like ProPublica. This ignores the reality that historical and structural biases are embedded in the data itself, making it unnecessary for explicit race-based discrimination to still result in biased outcomes​. Third, COMPAS methods of assessing fairness could not detect compounding inequality. They assessed individual risk without considering the structural inequalities that led to higher arrest rates among Black individuals like the fact that areas with high concentrations of Black residents are more likely to be over-policed and over-charged. The COMPAS algorithm treated past arrests and convictions as neutral indicators, failing to account for how past injustice compounds over time​. Fourth, COMPAS only used a snapshot dataset – a dataset that only considered historical arrest records, not defendants’ actual long-term outcomes. Their algorithm did not consider how a Black defendant’s false high-risk classification could lead to longer incarceration times, which might in turn reduce future employment opportunities and reinforces systemic inequality. Fifth, the COMPAS algorithm controlled for factors like criminal history, age, and gender, in order to argue that any disparities across racial groups were due to legitimate risk factors rather than systemic racism. But, these risk factors were actually shaped by systemic racism, making them a proxy for assigning different scores based on race. Sixth, similar to the first issue, using different fairness metrics, in this case predictive accuracy vs. false positive rates, created conflicting results about whether or not the algorithm was fair. Finally, the seventh issue showed up as COMPAS continued to be used in criminal justice decisions because its numerical outputs carried authority – judges, parole officers, and policymakers trusted the algorithm’s scores despite evidence of bias. People often feel that data is unbiased and algorithms can’t be racist, but we can some of the drawbacks of using quantitative methods do assess the fairness of the COMPAS algorithm across racial groups by applying each of Narayanan’s issues with the quantitative methods uses to assess bias in the COMPAS algorithm used.\nUltimately, I’d agree with Narayanan’s claim about quantitative methods for assessing bias. Although quantitative methods are important and we can use them to understand limitations of our machine learning algorithms, there are many other dimensions in algorithmic bias that need to be considered. As discussed in Data Feminism, power dynamics that can exacerbate bias are embedded in all aspects of data collection, analysis, and interpretation of results (D’Ignazio and Klein (2020)). Narayanan claims that the burden of proof serves to favor the status quo, and Data Feminism goes even further as to argue that quantitative methods are not neutral – they are shaped by and continue to hold up existing power structures. In my experience, people often view quantitative methods as unbiased, they can either be “right” or “wrong”, but when we begin to dig deeper into the systemic power imbalances that are embedded into collecting and interpreting data, even outside of machine learning algorithms, we can see that when quantitative methods are used on datasets that reflect biases in our society, the method themselves begin to reinforce these inequalities. We need to use context, history, and lived experiences in addition to quantitative methods in order to truly capture bias in these algorithms – as D’Ignazio and Klein and write, “context is queen” (D’Ignazio and Klein (2020)). This past J-term, I took a class called Questioning Technology, where we discussed algorithmic bias at length. One of the most interesting issues that we discussed, brough up in a book called More Than A Glitch: Confronting Race, Gender, and Ability Bias in Tech was technochauvanism, a bias that considers computational solutions to be superior to all other solutions (Broussard (2023)). I think that this issue is what lies at the center of the entire discussion around machine learning bias, the fact that people are so willing to accept technological solutions as unbiased and correct. So, like Narayanan claims, I think we can continue to use machine learning algorithms and use quantitative methods to assess their fairness. In fact, I think we should, in order to learn how we can improve these systems. But, we can’t come at this from a technochauvanist angle. Instead, we need to approach machine learning with skepticism, bringing our knowledge of the bias intrinsically present in our datasets to our assessment of machine learning algorithms, especially when using them to inform consequential decisions that can have immense impact on peoples’ lives. To put this into context, in the case of the ProPublica study, technochauvanism was certainly at play. The system we are looking at, the criminal justice system, is already known to have a problem with human bias. We know that judges tend to be biased against people of color, even when they don’t do it intentionally. So, when presented with an algorithmic solution, the criminal justice system was immediately inclined to accept this solution as potentially less biased than human judges. The algorithm is assumed not to include racial bias because developers don’t think that it has been exposed to years of systemic racism like humans have. Therefore, people started using this algorithm without much investigation into its “accuracy” besides its accuracy rates. It required skilled researchers to do a in-depth study in order to expose the problems with the algorithm. But, as we can see from the study, peoples’ lives have already been negatively impacted by this algorithm, despite it being called into question now. Instead, of finding these issues after the fact, a more skeptical approach would look like asking the critical questions that the ProPublica researchers asked, but doing so before the algorithm is deployed. Or, using the algorithm first on a very small subset of cases to understand its limitations and impacts before allowing it to be used on any case without restriction. Furthermore, developers should be skeptical of their data– if they are looking to reduce racial bias, they should be take the time to understand the underlying bias that might be present in the variables they are using to train their model. Ultimately, we cannot continue acting with technochauvanist views and accepting algorithms as unbiased without rigorous testing that involves exploring all the types of potential bias that an algorithm can exhibit. These algorithms have the potential to ruin lives, and we must use them with care.\n\n\n\n\nReferences\n\nAngwin, Julia, Jeff Larson, Surya Mattu, and Lauren Kirchner. 2016. “Machine Bias.” ProPublica. https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing.\n\n\nBarocas, Solon, Moritz Hardt, and Arvind Narayanan. 2023. Fairness and Machine Learning: Limitations and Opportunities. Cambridge, Massachusetts: The MIT Press.\n\n\nBroussard, Meredith. 2023. “More Than a Glitch: Confronting Race, Gender, and Ability Bias in Tech. Cambridge, Massachusetts: The MIT Press.\n\n\nD’Ignazio, Catherine, and Lauren Klein. 2020. Data Feminism. Cambridge, Massachusetts: The MIT Press.\n\n\nDastin, Jeffrey. 2018. “Amazon Scraps Secret AI Recruiting Tool That Showed Bias Against Women.” Reuters. https://www.reuters.com/article/us-amazon-com-jobs-automation-insight-idUSKCN1MK08G.\n\n\nNarayanan, Arvind. 2022. “The Limits of the Quantitative Approach to Discrimination.” Speech."
  },
  {
    "objectID": "posts/overfitting-overparameterization-double-descent/index.html",
    "href": "posts/overfitting-overparameterization-double-descent/index.html",
    "title": "Overfitting, Overparameterization, and Double-Descent",
    "section": "",
    "text": "Abstract\nFind the source code here\nIn this blog post, we’ll explore overfitting and double descent. We will use overparameterization to exemplify what it looks like when a model is overfit, and we’ll see how models can learn patterns that are too specific to the training set and don’t generalize well. We will also look at an example of detecting corruption in images that will show us that interpolating the data is not always bad and can sometimes lead to a more accurate model, despite that we may worry about overfitting.\n\n\nPart 0\nWe know that the closed-form solution for ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍least-squares ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍linear ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍regression is:\n\\(\\hat{w} = (X^tX)^{-1}X^ty\\)\nonly if the number of data observations \\(n\\) is larger than the number of features \\(p\\). When \\(p\\) is larger than \\(n\\), the matrix \\(X^tX\\) becomes no longer invertible. When \\(p &gt; n\\), the columns of \\(X\\) are linearly dependent – there are more variables than there are equations to describe their relationships, which makes the columns in the matrix \\(X\\) linearly dependent. This causes \\(X^tX\\) to not be invertible, so the equation for \\(\\hat{w}\\) no longer works.\n\n\nTesting Model on Simple Data\nWe’re going to be using a linear model that inherits from the linear model that we wrote for the previous blog post. See the source code (linked above) for the details. We’ll first try to fit the model to some simple 1-d data.\n\n# Create the nonlinear dataset\nX = torch.tensor(np.linspace(-3, 3, 100).reshape(-1, 1), dtype = torch.float64)\ny = X**4 - 4*X + torch.normal(0, 5, size=X.shape)\n\n# Plot to get an idea of what we're trying to fit\nplt.scatter(X, y, color='darkgrey', label='Data')\n\n\n\n\n\n\n\n\nIn order to fit a model to this nonlinear dataset, we first need to use the RandomFeatures class to generate a set of features. This class creates a random sigmoidal feature map that must be “fit” before use. This process fits progressively ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍more ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍complex linear regression models that have more random features to the same dataset of a small size. Eventually, the complex models will be able to perfectly model the training set but not generalize to the test data. The large number of parameters allows the model to learn‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍ intricate patterns in the test data, but these patterns don’t necessarily generalize to the test data. We’ll do this by feeding the random feature set into our custom linear regression model.\n\n# Generate random nonlinear features\nphi = RandomFeatures(n_features=100)\nphi.fit(X)  \nX_phi = phi.transform(X)  \n\nNow, we’ll feed this feature set into our model:\n\n# Train the model\nmodel = MyLinearRegression()\noptimizer = OverParameterizedLinearRegressionOptimizer(model)\noptimizer.fit(X_phi, y)  \n\n# Predict\ny_pred = model.predict(X_phi)\n\nAnd plot the data versus the predictions from our model:\n\n# Plot predictions vs. true data\nplt.figure(figsize=(8, 5))\nplt.scatter(X.numpy(), y.numpy(), color='gray', label='Data')\nplt.plot(X.numpy(), y_pred, color='crimson', linewidth=2, label='Predictions')\nplt.xlabel(\"X\")\nplt.ylabel(\"y\")\nplt.legend()\nplt.title(\"Overparameterized Linear Regression\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nWe can see that the overparameterized model does a good job of modeling the patterns in the training set, even picking up small nuances that may not necessarily generalize to a larger test set. A more generalizable version of this model might have a red prediction line that matches the general shape of the training data well but doesn’t include the smaller bumps up and down that can be seen in the training data.\n\n\nDouble Descent In Image Corruption Detection\nNow, we’re going to apply this method to a more complex dataset to highlight the difference in performance between the test and training sets with an overparameterized model. We will use an image of a flower and try to predict how many patches of solid pixels are present in a corrupted version of the image.\n\n# Load the dataset of images and plot to see what they look like\ndataset = load_sample_images()     \nX = dataset.images[1]\nX = zoom(X,.2) #decimate resolution\nX = X.sum(axis = 2)\nX = X.max() - X \nX = X / X.max()\nflower = torch.tensor(X, dtype = torch.float64)\n\nfig, ax = plt.subplots(1, 1, figsize=(6, 6))\nax.imshow(flower)\noff = ax.axis(\"off\")\n\n\n\n\n\n\n\n\n\n\nFunction to make corrupted image\n# Function to corrupt the image by adding random blocks of solid colored pixels\ndef corrupted_image(im, mean_patches = 5): \n    n_pixels = im.size()\n    num_pixels_to_corrupt = torch.round(mean_patches*torch.rand(1))\n    num_added = 0\n\n    X = im.clone()\n\n    for _ in torch.arange(num_pixels_to_corrupt.item()): \n        \n        try: \n            x = torch.randint(0, n_pixels[0], (2,))\n\n            x = torch.randint(0, n_pixels[0], (1,))\n            y = torch.randint(0, n_pixels[1], (1,))\n\n            s = torch.randint(5, 10, (1,))\n            \n            patch = torch.zeros((s.item(), s.item()), dtype = torch.float64) + 0.5\n\n            # place patch in base image X\n            X[x:x+s.item(), y:y+s.item()] = patch\n            num_added += 1\n\n            \n        except: \n            pass\n\n    return X, num_added\n\n\n\n# Take a look at the corrupted image\nX, y = corrupted_image(flower, mean_patches = 50)\nfig, ax = plt.subplots(1, 1, figsize=(6, 6))\nax.imshow(X.numpy(), vmin = 0, vmax = 1)\nax.set(title = f\"Corrupted Image: {y} patches\")\noff = plt.gca().axis(\"off\")\n\n\n\n\n\n\n\n\nHere we can see that the image has blocks of solid colored pixels that make the image corrupt. We want to use our model to predict how many of these corrupted blocks are going to be in an image from just the image itself. We will start by creating a dataset of corrupted images.\n\n# Prepare data\nn_samples = 200\nimage_dim = flower.shape\nX = torch.zeros((n_samples, image_dim[0], image_dim[1]), dtype=torch.float64)\ny = torch.zeros(n_samples, dtype=torch.float64)\nfor i in range(n_samples):\n    X[i], y[i] = corrupted_image(flower, mean_patches=100)\n\nX = X.reshape(n_samples, -1) # Reshape the image \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42) # Split into train and test sets\n\nNow, we want to to assess the performance of the overparametrized model. We will use a loop to vary the number of features that we use in the RandomFeatures class to find the number of features that yields the smallest mean squared error.\n\n# Parameters\nmax_features = 200\ntraining_errors = [] # Lists to keep track of errors\ntesting_errors = []\nfeature_counts = list(range(1, max_features + 1)) # Test 1-200 features\n\n# Loop through different numbers of features and get error\n\nfor n_features in feature_counts:\n\n    # Generate ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍features ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍from ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍a ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍data ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍matrix\n    phi = RandomFeatures(n_features=n_features, activation=lambda x: x**2) # Make the random feature map with square activation function\n    phi.fit(X_train) \n    X_train_phi = phi.transform(X_train)\n    X_test_phi = phi.transform(X_test)\n    \n    model = MyLinearRegression() # Initialize the model\n    optimizer = OverParameterizedLinearRegressionOptimizer(model)\n    optimizer.fit(X_train_phi, y_train) # Fit to the training set with the random features from above\n\n    y_train_pred = model.predict(X_train_phi) # Make predictions for train set\n    y_test_pred = model.predict(X_test_phi) # Make predictions for test set\n\n    training_mse = mean_squared_error(y_train.numpy(), y_train_pred.detach().numpy()) # Get MSE for train set\n    testing_mse = mean_squared_error(y_test.numpy(), y_test_pred.detach().numpy()) # Get MSE for test set\n\n    # Add errors to list for plotting \n    training_errors.append(training_mse)\n    testing_errors.append(testing_mse)\n\n\n\nPlotting features and error\n# Interpolation threshold (where n_features = n_samples)\ninterp_threshold = X_train.shape[0]\n\n# Make 2 plots\nplt.figure(figsize=(14, 5))\n\n# Training error\nplt.subplot(1, 2, 1)\nplt.plot(feature_counts, training_errors, 'o', color='gray')\nplt.axvline(interp_threshold, color='black', linewidth=2)\nplt.yscale(\"log\")\nplt.xlabel(\"Number of features\")\nplt.ylabel(\"Mean squared error (training)\")\nplt.title(\"Training Error vs Number of Features\")\n\n# Testing error\nplt.subplot(1, 2, 2)\nplt.plot(feature_counts, testing_errors, 'o', color='darkred')\nplt.axvline(interp_threshold, color='black', linewidth=2)\nplt.yscale(\"log\")\nplt.xlabel(\"Number of features\")\nplt.ylabel(\"Mean squared error (testing)\")\nplt.title(\"Testing Error vs Number of Features\")\n\nplt.tight_layout()\nplt.show()\n\n# Print best result\nbest_idx = np.argmin(testing_errors)\nbest_n_features = feature_counts[best_idx]\nprint(f\"Best testing error achieved with {best_n_features} features.\")\n\n\n\n\n\n\n\n\n\nBest testing error achieved with 187 features.\n\n\nTaking a look at these plots, we can see in the left plot that as we increase the number of features with the test set, the mean squared error immediately drops as we pass the interpolation threshold. This makes sense– as we discussed above, overparameterizing the model on the training set allows the model to predict the training set very well, as it learns complex patterns about the data. On the test set, we can see that when we used 179 features, we had the lowest mean squared error. This is an example of double descent, as we approach the interpolation threshold, the testing error increases, but then begins to decrease as we pass the interpolation threshold. This suggests that ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍interpolating the data is not always bad, as we generalized above. Instead, sometimes this process can lead to reasonable-looking models, as we can see from the image corruption dataset. We were actually able to decrease the testing error to its lowest after passing the interpolation threshold.\n\n\nDiscussion\nWe looked at a few different examples here of how overfitting can come into play in machine learning. We saw that when we used a random feature map to train our linear regression model, our model was able to fit the training data very well and learn complex trends in the data. But, we also discussed how this is not necessarily the best way to train a model, because it doesn’t always generalize well to the test set. But, our impage corruption detection example provided a different result – that our test error actually ended up being lowest when we had passed the interpolation threshold, an example of double-descent. Ultimately, it’s essential to understand the potential causes and ramifications of overfitting, as it is one of the major concerns of machine learning models. But, we also gained another perspective from this blog post that is particularly applicable in deep learning, that sometimes linear interpolation can lead to success in models."
  },
  {
    "objectID": "posts/palmer-penguins/index.html",
    "href": "posts/palmer-penguins/index.html",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "Abstract\nIn this blog post we are going to train a random forest model to classify penguin species based on three features in the Palmer Penguins dataset. We’ll start with some visualizations to get an understanding of the different features in the dataset, and then we will use a random forest to choose the best three features to train our model on. After training and testing the model, we will explore how well the model did and how it made its classifications using the confusion matrix and plotting decision regions.\n\n\nPreparing the Data\n\n# Read in the data\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\n\nle = LabelEncoder()\ntrain[\"Species\"].replace('Adelie Penguin (Pygoscelis adeliae)','Adelie',inplace=True)\ntrain[\"Species\"].replace('Chinstrap penguin (Pygoscelis antarctica)','Chinstrap',inplace=True)\ntrain[\"Species\"].replace('Gentoo penguin (Pygoscelis papua)','Gentoo',inplace=True)\nle.fit(train[\"Species\"])\n\n# Function to prepare the data \ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1) # Remove unnecessary columns\n  df = df[df[\"Sex\"] != \".\"]                                                                                 \n  df = df.dropna() # Drop NAs                                                                                          \n  y = le.transform(df[\"Species\"]) # Encode the species column                                                                      \n  df = df.drop([\"Species\"], axis = 1)                                                                       \n  df = pd.get_dummies(df) # One-hot encode the boolean columns                                                                \n  return df, y\n\nX_train, y_train = prepare_data(train)\n\nIn the above code, we processed the dataset to prepare it for analysis as follows:\n\nRemove the unnecessary columns and NAs\nPrepare the qualitative columns for analysis by encoding them as quantitative columns\n\nEncode the species column\n\nEach species is assigned a number\n\n“One-hot encode” the boolean columns\n\nThe get_dummies function converts these columns to 0-1 instead of True/False\n\n\n\n\n\narray([1, 1, 2, 2, 1, 0, 0, 1, 2, 1, 0, 1, 0, 1, 1, 2, 0, 2, 2, 2, 2, 0,\n       0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 2, 2, 1, 1, 2, 2, 1, 0,\n       0, 2, 2, 1, 2, 2, 1, 2, 0, 0, 2, 2, 0, 1, 2, 2, 1, 2, 1, 2, 2, 2,\n       0, 0, 0, 2, 2, 2, 0, 1, 2, 2, 2, 0, 0, 2, 0, 0, 2, 0, 0, 0, 1, 0,\n       0, 1, 0, 0, 0, 1, 0, 0, 2, 2, 0, 0, 2, 0, 2, 1, 0, 2, 2, 1, 2, 2,\n       2, 0, 2, 0, 0, 0, 1, 0, 2, 2, 0, 2, 2, 1, 2, 0, 0, 1, 2, 2, 1, 0,\n       2, 0, 1, 2, 0, 0, 2, 0, 2, 1, 0, 0, 2, 1, 0, 2, 0, 2, 0, 1, 0, 0,\n       0, 2, 2, 2, 0, 0, 2, 0, 2, 1, 1, 0, 1, 2, 0, 0, 1, 1, 0, 0, 1, 0,\n       0, 2, 1, 2, 1, 0, 0, 0, 1, 0, 2, 2, 2, 2, 1, 1, 1, 2, 2, 2, 0, 1,\n       0, 0, 1, 0, 0, 0, 0, 0, 1, 2, 2, 2, 0, 2, 2, 1, 0, 2, 0, 0, 2, 0,\n       2, 0, 2, 0, 2, 2, 2, 2, 0, 2, 1, 0, 2, 1, 1, 0, 2, 1, 0, 0, 0, 1,\n       0, 1, 0, 0, 2, 1, 0, 0, 0, 2, 0, 0, 0, 1])\n\n\n\n\nVisualizing the data\nNow, we need to figure out which features best distinguish each species so that we can train a model to predict the species based on the features. We’ll start by looking at visualizations of some of the features in the dataset in order to get an idea of which ones will work well for classification.\n\nsns.set_theme()\n# Plot the culmen depth and flipper length \nsns.relplot(\n    data=train,\n    x=\"Flipper Length (mm)\", y=\"Culmen Depth (mm)\",\n    hue=\"Species\", style=\"Species\"\n).set(title=\"Penguin Species by Culmen Depth and Flipper Length\")\n\n\n\n\nFigure 1: culmen length and flipper depth of each penguin species\n\n\n\n\n\n# Plot the distribution of each species on each of the 3 islands \nax = sns.countplot(data=train, x=\"Island\", hue=\"Species\")\nax.set(xlabel=\"Island\", ylabel=\"Number of Penguins\", title=\"Species Distribution Across Islands\")\nax.set_xticklabels(ax.get_xticklabels(), rotation=30)\n\n\n\n\nFigure 2: species distribution across each island\n\n\n\n\n\n\n\n\n\n\n\n\n\nCulmen Length (mm)\n\n\nSpecies\n\n\n\n\n\nAdelie\n38.970588\n\n\nChinstrap\n48.826316\n\n\nGentoo\n47.073196\n\n\n\n\n\nFigure 3: mean culmen length by species\n\n\nOur goal is to find three features that will allow us to train a model which will be 100% accurate in predicting the species of a penguin in the training set. We can ascertain some information about the top scored features from these plots that will be helpful to determine if these features will work.\nFirst, as we can see in Figure 1, the Gentoo penguin has different culmen and flipper measurements than the Chinstrap and Adelie penguins. Specifically, the Gentoo penguin seems to have a larger flipper length and a shallower culmen depth than the Chinstrap and Adelie penguins. This relationship could be used to identify the Gentoo penguins, but we still need to find a way to distinguish between the Chinstrap and Adelie penguins.\nFigure 2 provides a way to do this; the Chinstrap penguins are only found on Dream Island. We can also see that the Adelie penguins are found on all three islands, and the Gentoo penguin is only found on Biscoe Island. In the summary table, we can see that there is also a difference in culmen length between the species. The Chinstrap and Gentoo penguins have similar average culmen lengths, but the Adelie penguins have much shorter culmens on average.\nUsing some combination of these features, we should be able to accurately predict the species of any penguin in the training set. We will use tree-based feautre selection to determine which of the features we will use to train our model.\n\n\nOptimizing model parameters\nWe will be using a random forest to choose the best features to train our model on. We’ll also use a random forest to execute our classification. First, let’s understand what a random forest is:\n\nRandom Forest\n\nRandom forests are an ensemble learning method. That is, they combine predictions from multiple models to make a final prediction. Random forests use multiple decision trees to make a prediction. Each tree is trained on a different subset of the training set, so different trees can learn different patterns in the data. The trees use a process of “voting” to determine the prediction–whichever species is predicted by the most trees is the final prediction.\n\n\n\nChoosing the best features\n\n# Fit a model to find the best features in the training set\ntree_selector = ExtraTreesClassifier(n_estimators=100, random_state=42)\ntree_selector.fit(X_train, y_train)\nfeature_importances = tree_selector.feature_importances_\n\n# Create DataFrame with the feature and its importance score\nfeature_scores = pd.DataFrame({\"Feature\": X_train.columns, \"Importance\": feature_importances})\nfeature_scores = feature_scores.sort_values(by=\"Importance\", ascending=False)\n\n\n\nTop 3 Features:\n               Feature  Importance\n0   Culmen Length (mm)    0.166972\n2  Flipper Length (mm)    0.139796\n6        Island_Biscoe    0.132696\n\n\nWhen we use the ExtraTreesClassifier function, we are building a random forest from the training set. A random subset of features is used for each tree, then the best features are selected based on an importance score. The importance score is calculated using an impurity score for each feature that captures how well a feature is able to partition the dataset. These scores are then normalized to create the importance score. We can see from the feature selector that the culmen length, flipper length, and the island are the most important features in the dataset. So, we will use these features to train our model.\n\n\nChoosing maximum tree depth\nOne of the parameters for any tree method is the maximum depth at which the tree can go. That is, the number of partitions of the dataset allowed in each tree. In order to find the optimal maximum depth, we will use cross validation to test a range of potential maximum depths:\n\n# The columns that we want to include in our analysis -- based on the highest-scoring features\ncols = [\"Flipper Length (mm)\", \"Culmen Length (mm)\", \"Island_Biscoe\", \"Island_Dream\",\"Island_Torgersen\"]\n\ndepths = range(1, 21)\nmean_scores = []\n\n# Loop through potential depths and use cross validation to score each depth\nfor depth in depths:\n    dt = RandomForestClassifier(max_depth=depth, random_state=42)\n    scores = cross_val_score(dt, X_train[cols], y_train, cv=5, scoring='accuracy')\n    mean_scores.append(np.mean(scores))\n\n# Find the depth with the highest scoring validation\nbest_depth = depths[np.argmax(mean_scores)]\nbest_score = max(mean_scores)\n\n\n\nBest max tree depth: 7\nBest cross-validated accuracy: 0.9843891402714933\n\n\nBased on the cross-validation, a random forest with maximum depth 7 is the best option for our model.\n\n\n\nFitting the model\nNow that we know which features we want to use and have done some work to optimize the parameters for our random forest, we’re ready to train a random forest on our training set.\n\n# Initialize the model and fit it to the train set\ndt_model = RandomForestClassifier(max_depth=5, random_state=42)\ndt_model.fit(X_train[cols], y_train)\n\n# Score the model accuracy to see how well it did \ntrain_accuracy = dt_model.score(X_train[cols], y_train)\n\n\n\nTraining Accuracy: 0.9961\n\n\nThis random forest works pretty well! It was 99% accurate on our testing set. Now that we’ve trained this random forest tree, we can test it on the test dataset.\n\n# Read in the test data\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\ntest[\"Species\"].replace('Adelie Penguin (Pygoscelis adeliae)','Adelie',inplace=True)\ntest[\"Species\"].replace('Chinstrap penguin (Pygoscelis antarctica)','Chinstrap',inplace=True)\ntest[\"Species\"].replace('Gentoo penguin (Pygoscelis papua)','Gentoo',inplace=True)\n\nX_test, y_test = prepare_data(test)\ntest_acc = dt_model.score(X_test[cols], y_test)\n\n\n\nTesting Accuracy: 0.9853\n\n\n\n\nUnderstanding the results\nLet’s get a better understanding of what it’s actually doing. First, we can use the confusion matrix to look at how all of the penguins were classified.\n\ny_test_pred = dt_model.predict(X_test[cols])\nconfusion_matrix(y_test, y_test_pred)\n\narray([[31,  0,  0],\n       [ 0, 11,  0],\n       [ 1,  0, 25]])\n\n\nWhat does this matrix mean? We can understand it using each species as follows:\n\n\nThere were 31 Adelie penguin(s) who were classified as Adelie penguin(s).\nThere were 0 Adelie penguin(s) who were classified as Chinstrap penguin(s).\nThere were 0 Adelie penguin(s) who were classified as Gentoo penguin(s).\nThere were 0 Chinstrap penguin(s) who were classified as Adelie penguin(s).\nThere were 11 Chinstrap penguin(s) who were classified as Chinstrap penguin(s).\nThere were 0 Chinstrap penguin(s) who were classified as Gentoo penguin(s).\nThere were 1 Gentoo penguin(s) who were classified as Adelie penguin(s).\nThere were 0 Gentoo penguin(s) who were classified as Chinstrap penguin(s).\nThere were 25 Gentoo penguin(s) who were classified as Gentoo penguin(s).\n\n\nThis tells us that our model only misclassified one penguin – a Gentoo classified as an Adelie. We can plot the decision regions for each island to get an understanding of why this happened.\n\n\nPlotting Code\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Gentoo\", \"Chinstrap\", \"Adelie\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\n\n\n\n\n\n\n\n\n\n\nThese plots show us how our model made its classifications. On the far left, we can see that for the Biscoe Island data, the model was able to correctly distinguish between the Adelie (blue) and Gentoo (red) penguins using the flipper length. Moving to the far right, on Torgensen Island, there are only Adelie penguins, so the model was able to classify all of them correctly. In the middle plot we can see that the penguins on Dream Island were harder to classify – we can see where the model classified a Chinstrap (green) penguin as an Adelie penguin (blue) because it had a shorter culmen than the other Chinstrap penguins, causing its point to be within the blue region.\n\n\nDiscussion\nWe can see that our random forest was a very accurate model. It only misclassified one of the penguins, giving us 98.5% testing accuracy. We can see from the decision regions that the random forest was able to learn patterns within each island fairly well, even creating some accurate non-linear decision regions. In order to get more accuracy for the Dream island data, we would probably need to use another classification method that could learn the differences between the Chinstrap and Adelie penguins on Dream island. Or, going back to our initial visualizations, we could use an ensemble learning method that first uses one model to separate one species from the other two using the quantitative measurements and another that uses a qualitative feature like the island to distinguish between penguins with similar physical measurements."
  },
  {
    "objectID": "posts/automated-decision-systems/index.html",
    "href": "posts/automated-decision-systems/index.html",
    "title": "Automated Decison Systems",
    "section": "",
    "text": "Abstract\nIn this blog post we’ll be looking at data from a bank. We’ll start with some data exploration and visualization to get a better understanding of the features in the dataset. Our ultimate goal here is to create a decision-making system that will determine whether a prospective borrower should be offered a loan. We will use logistic regression to assign weights to different features in the dataset, which we will then use to give each prospective borrower a “score”. This score will be used to determine whether the borrower will prospective receive a loan based on a threshold that will optimize profits for the bank. After creating this decision-making system, we’ll test it out on a test dataset and do some exploration into who our system predicted would be able to repay their loans, and who it predicted wouldn’t.\n\n# Read in the data\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/train.csv\"\ndf_train = pd.read_csv(url)\n\n\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_home_ownership\nperson_emp_length\nloan_intent\nloan_grade\nloan_amnt\nloan_int_rate\nloan_status\nloan_percent_income\ncb_person_default_on_file\ncb_person_cred_hist_length\n\n\n\n\n0\n25\n43200\nRENT\nNaN\nVENTURE\nB\n1200\n9.91\n0\n0.03\nN\n4\n\n\n1\n27\n98000\nRENT\n3.0\nEDUCATION\nC\n11750\n13.47\n0\n0.12\nY\n6\n\n\n2\n22\n36996\nRENT\n5.0\nEDUCATION\nA\n10000\n7.51\n0\n0.27\nN\n4\n\n\n3\n24\n26000\nRENT\n2.0\nMEDICAL\nC\n1325\n12.87\n1\n0.05\nN\n4\n\n\n4\n29\n53004\nMORTGAGE\n2.0\nHOMEIMPROVEMENT\nA\n15000\n9.63\n0\n0.28\nN\n10\n\n\n\n\n\n\n\n\n\nExploring The Data\nLet’s start with some visualizations to explore patterns in this dataset:\n\n\nPlotting Code\n# Filter out rows with employment length greater than 25 years\ndf_train = df_train[df_train['person_emp_length'] &lt;= 25]\n\n# Bucket the employment length into 5-year chunks\nbins = range(0, 25 + 5, 5)\nlabels = [\"0 to 5 years\", \"5 to 10 years\", \"10 to 15 years\", \"15 to 20 years\", \"20 to 25 years\"]\n\ndf_train['emp_length_bucket'] = pd.cut(\n    df_train['person_emp_length'],\n    bins=bins,\n    right=True,\n    labels=labels,\n    include_lowest=True\n)\n\n# Create a Seaborn plot using histplot with multiple=\"fill\" for normalized (proportional) stacked bars\nplt.figure(figsize=(10, 6))\nsns.histplot(\n    data=df_train,\n    x='emp_length_bucket',\n    hue='loan_intent',\n    multiple='fill',  # normalizes each bar to sum to 1 (proportions)\n    shrink=0.8, \n    palette=\"Set2\"\n)\nplt.title(\"Loan Intent Proportions by Employment Length\")\nplt.xlabel(\"Employment Length\")\nplt.ylabel(\"Proportion\")\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nFigure 1: Loan intent by length of employment\n\n\n\n\n\n\nPlotting Code\n# Plot the distribution of interest rates of each of the home ownership statuses \nsns.boxplot(data=df_train, x=\"person_home_ownership\", y=\"loan_int_rate\", palette=\"Set2\")\nplt.xlabel(\"Home Ownership Status\")\nplt.ylabel(\"Loan Interest Rate\")\n\n\nText(0, 0.5, 'Loan Interest Rate')\nFigure 2: Interest rate distribution by home ownership status\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncredit_history_length\n\n\nloan_grade\n\n\n\n\n\nA\n5.7\n\n\nB\n5.8\n\n\nC\n5.9\n\n\nD\n5.8\n\n\nE\n5.8\n\n\nF\n6.1\n\n\nG\n6.6\n\n\n\n\n\nFigure 3: mean credit history length by loan grade\n\n\nWe’re looking here to understand some of the patterns in this dataset.\nFirst, in Figure 1, we can see that borrowers’ intentions for their loans change a bit as the length of their employment increases. Specifically, after 20 years of employment people stop borrowing to pay for education, which makes sense. People also start taking out more loans for venture and medical uses as their employment goes on. It looks like the proportion of loans that are intended to be used for home improvement, personal reasons, and debt consolidation remain fairly constant across length of employment.\nMoving on to Figure 2, we can see that borrowers with different home ownership statuses (renting, owning, mortgaging or other), have differences in the interest rate they might receive on their loan. Borrowers who rent their home seem more likely to have an interest rate of over 20%, and we can see that borrowers who own their homes rarely see interests rates over 20%. Borrowers who rent and mortgage their homes have more variability in the interest rates they might see, as well.\nFinally, we can see in Figure 3 that loan grades have differences in average length of credit of the borrower. It seems that borrowers with higher loan grades have longer credit histories, on average.\nWe can see that we should be able to get some information about whether a borrower will pay back a loan from some of these features.\n\nPrepping the Data\n\n# Encode the qualitative variables quantitatively\ndef preprocess(df):\n    \"\"\"\n    Function to preprocess the data\n    \"\"\"\n    le_home = LabelEncoder()\n    df[\"person_home_ownership\"] = le_home.fit_transform(df[\"person_home_ownership\"])\n\n    le_intent = LabelEncoder()\n    df[\"loan_intent\"] = le_intent.fit_transform(df[\"loan_intent\"])\n    intent_mapping = dict(zip(le_intent.transform(le_intent.classes_), le_intent.classes_))\n\n    df = pd.get_dummies(df, columns=['cb_person_default_on_file'], prefix='default')\n    df = df.dropna()\n\n    # Change the interest rate column to a percentage\n    df[\"loan_int_rate\"] = df[\"loan_int_rate\"]/100\n\n    #Filter out columns with weird values\n    df = df[(df[\"person_age\"] &lt; 100) & (df[\"person_income\"] &gt; 1000) &  (df[\"loan_int_rate\"] &gt; 0)]\n\n    # Separate our the target variable\n    y = df[\"loan_status\"]\n\n    return df, y, intent_mapping\n\ndf_train_clean, y_train, intent_map = preprocess(df_train)\n\nWe will use this function prepare our datasets for analysis. We’re encoding the qualitative columns for home ownership and loan intent as integers, one-hot encoding the historical default true/false column as 0s and 1s, separating the target variable\n\n\n\nBuilding A Model\n\nChoosing Features\nFirst, let’s figure out which features we want to use for the model. We’ll generate a few new features that we can use to better understand the relationships between our features: - Credit history ratio: \\(\\frac{\\text{credit history length}}{\\text{age}}\\) - Employment stability: \\(\\frac{\\text{employment length}}{\\text{age}}\\) - Income stability: \\(\\text{income}*\\text{employment length}\\)\nWe’ll use cross validation to test a few combinations of features: 1. Employment stability, income, and percentage of income that the loan is 2. Loan intent, credit history ratio, home ownership status 3. income stability, loan interest rate, age\n\n# Create new features \ndef new_features(df):\n    df[\"credit_history_ratio\"] = round(df[\"cb_person_cred_hist_length\"]/df[\"person_age\"],2)\n    df[\"employment_stability\"] = round(df[\"person_emp_length\"]/df[\"person_age\"], 2)\n    df[\"income_stability\"] = round(df[\"person_income\"]*df[\"person_emp_length\"],2)\n    return df\ndf_train_clean = new_features(df_train_clean)\n\n\n# Define three different sets of features\nfeature_sets = {\n    \"Set 1\": [\"employment_stability\", \"person_income\", \"loan_percent_income\"],\n    \"Set 2\": [\"loan_intent\", \"credit_history_ratio\", \"person_home_ownership\"],\n    \"Set 3\": [\"income_stability\", \"loan_int_rate\", \"person_age\"]\n}\n\nmodel = LogisticRegression()\n\n# Evaluate each feature set\nresults = {}\nfor set_name, features in feature_sets.items():\n    X_train = df_train_clean[features]\n    X_train = StandardScaler().fit_transform(X_train)  # Standardize features \n    \n    scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')\n    mean_accuracy = np.mean(scores)\n    results[set_name] = mean_accuracy\n    print(f\"{set_name}: Mean Accuracy = {mean_accuracy:.4f}\")\n\nSet 1: Mean Accuracy = 0.8246\nSet 2: Mean Accuracy = 0.7850\nSet 3: Mean Accuracy = 0.8095\n\n\n\n\nEstimating weights\nAfter fitting an LR to each of the feature sets and using cross validation to test how accurate the models are, we can see that our first set of features, which includes employment stability, income, and percentage of income that the loan is has the highest average accuracy. Now, we’ll fit an LR model to these features to obtain our vector of weights that we’ll use in our scoring function.\n\nweights_model = LogisticRegression()\nX_train = df_train_clean[feature_sets[\"Set 1\"]]\nX_train = StandardScaler().fit_transform(X_train)\nweights_model.fit(X_train, y_train)\n\nfor feature, coef in zip(feature_sets[\"Set 1\"], weights_model.coef_[0]):\n    print(f\"{feature}: {coef:.4f}\")\n\nemployment_stability: -0.1482\nperson_income: -0.4953\nloan_percent_income: 0.7925\n\n\nNow we can use these coefficients in our as the values in our weight vector when we create a scoring function, which we’ll do next.\n\n\n\nCreating the score function\nOur next step is to give each borrower a score, using the features and weights that we decided on above. We’ll add a “score” column to the train set that has the result of our score function:\n\\(\\text{score} = -0.1482 * x_1 - 0.4953 * x_2 + 0.7925 * x_3\\)\nwhere\n\\(x_1\\) = employment_stability\n\\(x_2\\) = person_income\n\\(x_3\\) = loan_percent_income\n\ndef score_dataframe(df):\n    \"\"\"\n    Compute the score for each borrower using the weights \n    determined by the logistic regression\n\n    inputs:\n    df: Pandas DataFrame with all features used to generate weights\n    and loan status, amount, and interest rate for profit calculations\n    \"\"\"\n    df_scored = df.copy()\n    df_scored[[\"employment_stability\", \"person_income\", \"loan_percent_income\"]] = StandardScaler().fit_transform(df_scored[[\"employment_stability\", \"person_income\", \"loan_percent_income\"]])\n    df_scored[\"score\"] =  round(-0.1486 * df_scored[\"employment_stability\"] - 0.6252 * df_scored[\"person_income\"] + 0.7925 * df_scored[\"loan_percent_income\"],2)\n    return df_scored\n\nNow, we will write a function that will compute the expected profit from each prospective borrower. If a prospective borrower has a score greater than the threshold, our system will predict that this borrower will default on their loan. So, we will not give them a loan, making their profit contribution 0. For users that we predict will not default on their loan, we have two cases. If they repay their loan, their profit will be determined by:\n\\(\\text{profit = (loan amount)} * (1 + 0.25* \\text{(loan interest rate)})^{10} - \\text{(loan amount)}\\)\nIf the borrowed does not repay their loan, the bank will “profit” according to this equation:\n\\(\\text{profit = (loan amount)} * (1 + 0.25* \\text{(loan interest rate)})^{3} - 1.7 * \\text{(loan amount)}\\)\nThis function will apply the profit equation to each borrower based on their predicted loan status, and we will calculate the expected profit per borrower by dividing by the number of prospective borrowers.\n\ndef compute_profit(threshold, df):\n    \"\"\"\n    Compute the profit that the bank would have made if they had\n    used the predictions determined by our score function\n    and threshold\n\n    inputs:\n    threshold: integer value for score threshold \n    df: Pandas DataFrame with loan amount, interest rate, actual loan status, and score from \n    scoring function\n    \"\"\"\n    df[\"pred_default\"] = df[\"score\"].apply(lambda x: 1 if x &gt; threshold else 0)\n    \n    # Calculate potential profit for repayment or defaulting \n    df[\"repaid_profit\"] = round(df[\"loan_amnt\"] * (1 + 0.25 * df[\"loan_int_rate\"])**10 - df[\"loan_amnt\"],2)\n    df[\"defaulted_profit\"] = round(df[\"loan_amnt\"] * (1 + 0.25 * df[\"loan_int_rate\"])**3 - 1.7 * df[\"loan_amnt\"],2)\n    \n    # Profit based on actual loan status versus predicted status\n    df[\"profit\"] = 0  # Default to zero profit\n    df.loc[(df[\"loan_status\"] == 0) & (df[\"pred_default\"] == 0), \"profit\"] = df[\"repaid_profit\"]  # Successful repayment\n    df.loc[(df[\"loan_status\"] == 1) & (df[\"pred_default\"] == 0), \"profit\"] = df[\"defaulted_profit\"]  # Incorrectly predicted as good loan, default\n    df.loc[(df[\"loan_status\"] == 1) & (df[\"pred_default\"] == 1), \"profit\"] = 0  # Correctly predicted as default, no profit\n    df.loc[(df[\"loan_status\"] == 0) & (df[\"pred_default\"] == 1), \"profit\"] = 0  # Incorrectly predicted as default, no profit\n\n    return sum(df[\"profit\"]) / len(df)\n\n\n# Find optimal threshold\nscored_df = score_dataframe(df_train_clean)\nthresholds = np.arange(0,3,0.05)\nprofits = [compute_profit(t, scored_df) for t in thresholds]\n\n# Find best threshold\noptimal_threshold = thresholds[np.argmax(profits)]\noptimal_profit = max(profits)\nprint(f\"Optimal Threshold for Maximum Profit: {optimal_threshold:.4f}\")\nprint(f\"Maximum profit per prospective borrower: {round(optimal_profit,2):.4f}\")\n\n\n# Plot profit vs. threshold\nsns.set_style(\"whitegrid\")\nsns.set_palette(\"Set2\")\n\nplt.figure(figsize=(8, 5))\nplt.scatter(optimal_threshold, optimal_profit, color=sns.color_palette(\"Set2\")[1], label=f'Optimal t={optimal_threshold:.4f}', zorder=2)\nsns.lineplot(x=thresholds, y=profits, label=\"Profit\", zorder=1)\nplt.xlabel(\"Threshold\")\nplt.ylabel(\"Profit per borrower\")\nplt.title(\"Threshold vs Profit\")\nplt.legend()\nplt.show()\n\nOptimal Threshold for Maximum Profit: 1.1500\nMaximum profit per prospective borrower: 1316.4800\n\n\n\n\n\n\n\n\n\n\n\nEvaluating from the bank’s perspective\nNow, we want to look at how this decision making system works on a test dataset. We’ll apply all of the functions we used on the training set with the test set, and see what the profit per borrower will look like using the threshold from above, 1.15.\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/test.csv\"\ndf_test = pd.read_csv(url)\ndf_test, y_test, intent_map = preprocess(df_test)\ndf_test = new_features(df_test)\n    \nscored_test_df = score_dataframe(df_test)\nprint(\"profit per prospective borrower:\", round(compute_profit(1.15, scored_test_df),2))\n\nprofit per prospective borrower: 1230.42\n\n\nIt looks like we ended up with a fairly similar profit per borrower on the test set to what we got from the train set, $1316 per borrower on the test set and $1230 on the train set. From the bank’s perspective, this seems positive. If they are making, on average, $1230 per borrower, they would make around $6,000,000 off of this dataset of borrowers.\n\n\nEvaluating from the borrower’s perspective\nNow, let’s try to get a better understanding of who in the test dataset we are predicting is going to default on their loans, and who we are predicting is going to repay them. We’ll look at the following questions:\n\nIs it more difficult for people in certain age groups to access credit under your proposed system?\nIs it more difficult for people to get loans in order to pay for medical expenses? How does this compare with the actual rate of default in that 3. group? What about people seeking loans for business ventures or education?\nHow does a person’s income level impact the ease with which they can access credit under your decision system?\n\n\nQuestion 1\n\n# Split the borrower ages into 4 buckets and see \nscored_test_df[\"age_group\"] = pd.qcut(scored_test_df[\"person_age\"], q=4, labels=[\"Young\", \"Middle-aged\", \"Older\", \"Senior\"])\nage_access = scored_test_df.groupby(\"age_group\")[\"pred_default\"].mean()\nage_access\n\nage_group\nYoung          0.171756\nMiddle-aged    0.121554\nOlder          0.142263\nSenior         0.134460\nName: pred_default, dtype: float64\n\n\nThese probabilities are the mean of the prediction column, so they tell us the rate at which a 1 is predicted in each of the age buckets using our decision making system. That is, they tell us the loan rejection rate for each age group. We can see that younger people are mich more likely to be rejected for a loan than middle-aged and older people are, with a 17% rejection rate as opposed to a 12-14% rejection rate for older groups.\n\n\nQuestion 2\n\n# Calculate rejection rates and actual default rates for loan intent\nloan_intent_access = scored_test_df.groupby(\"loan_intent\")[\"pred_default\"].mean()\nloan_intent_default_rates = scored_test_df.groupby(\"loan_intent\")[\"loan_status\"].mean()\n\nloan_intent_df = pd.DataFrame({\n    \"Loan Intent (Decoded)\": loan_intent_access.index.map(intent_map),\n    \"Rejection Rate\": loan_intent_access.values,\n    \"Actual Default Rate\": loan_intent_default_rates.values\n})\n\n\n\n\n\n\n\n\n\n\nLoan Intent (Decoded)\nRejection Rate\nActual Default Rate\n\n\n\n\n0\nDEBTCONSOLIDATION\n0.150442\n0.287611\n\n\n1\nEDUCATION\n0.150510\n0.167517\n\n\n2\nHOMEIMPROVEMENT\n0.095779\n0.250000\n\n\n3\nMEDICAL\n0.164958\n0.284250\n\n\n4\nPERSONAL\n0.146293\n0.220441\n\n\n5\nVENTURE\n0.127593\n0.146266\n\n\n\n\n\n\n\nWe’re interested in the loans that are intended for medical uses. We can see that medical loans have the highest rate of rejection from our system (20% of medical loan requests are rejected), but they also have one of the highest rates of actual default (28%). Venture and business loans have lower rates of rejection and their actual default rates are closer to their rejection rates.\n\n\nQuestion 3\n\nscored_test_df[\"income_group\"] = pd.qcut(scored_test_df[\"person_income\"], q=4, labels=[\"Low\", \"Lower-middle\", \"Upper-middle\", \"High\"])\nincome_access = scored_test_df.groupby(\"income_group\")[\"pred_default\"].mean()\n\n\n\nincome_group\nLow             0.342638\nLower-middle    0.168044\nUpper-middle    0.052274\nHigh            0.004370\nName: pred_default, dtype: float64\n\n\nHere, we are again looking at the loan rejection rates, this time by income group. It is clear that people with lower incomes are much more likely to be rejected using our decision making system (41% of the time), as opposed to high income people who are only rejected 0.8% of the time with our system.\n\n\n\nWrite and Reflect\nWe found from our model and evaluation that when we choose a threshold that optimizes the bank’s profit, it can result in certain groups having a harder time obtaining a loan. We saw that age, income, and loan intent groups have differences rejection rates all with our decision making system. We can also see that optimizing profits for a bank requires turning a way a significant chunk of loan applications, because the reality is that many people do end up defaulting on their loans. In our decision-making process, we only considered 3 features of a person’s application, but we found that these features correlated with certain groups having a harder time receiving a loan. This is a great example of a feature that is implicitly encoding information about another category. For example, a person’s income could be correlated with what they are planning to use the loan for. Even though loan intent wasn’t one of the features of our score function, we could see that different loan intents ended up with different rates of rejection in our system. So, is this system fair? We’ll explore that by answering this question: Considering that people seeking loans for medical expense have high rates of default, is it fair that it is more difficult for them to obtain access to credit?\nFirst, let’s define fairness. I will define fairness in deciding who should get a loan of a combination of how much the person needs the loan and how likely it is that they will pay the loan back. If we are only basing fairness on how much the person needs the loan, we would likely say that people who need loans for medical reasons should all receive their loans. But, is this fair to the bank? They would end up losing a lot of money because people who take out loans for medical expenses often default on them. If we are only basing fairness on how likely it is that they will pay the loan back, most people seeking medical loans probably shouldn’t get them, because they do often default. But, in this case, very few people would receive medical loans, which seems unfair because they may be able to pay them back, even if people haven’t historically paid medical loans back at a high rate. There are also some other confounding variables to consider, like whether people seeking medical loans are more likely to come from a historically marginalized group that might have a harder time proving they can pay a loan back or a smaller income on average than other groups? It may not be fair for these people to be subject to high rejection rates just because of their identity.\nUltimately, Looking at the rate of rejection versus the true rate of default for medical loans, I think it’s fair that it may be more difficult for people to access these loans. The rate of default on these loans is much higher than the rate of default for loans with other intended purposes, so it seems that people who are seeking these loans may need to provide additional proof that they would be able to pay the loan back. It’s fair for the bank to be skeptical. However, we can see that in our system, 80% of people seeking medical loans are being accepted. So it isn’t the case that it’s impossible for these people, who probably really need the loan, to obtain it. Therefore, I think it’s fair that it is more difficult for people seeking medical loans to obtain access to credit given that they default much more frequently."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Final Project- Predicting MLB Pitch Calls\n\n\n\n\n\nCSCI 0451 final project\n\n\n\n\n\nMay 12, 2025\n\n\nCarly McAdam, Chloe Katz, Ryan Mauney\n\n\n\n\n\n\n\n\n\n\n\n\nOverfitting, Overparameterization, and Double-Descent\n\n\n\n\n\nOverfitting, overparameterization, and double-descent examples with logistic regression\n\n\n\n\n\nApr 21, 2025\n\n\nCarly McAdam\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing Logistic Regression\n\n\n\n\n\nImplementing Logistic Regression with Gradient Descent\n\n\n\n\n\nApr 2, 2025\n\n\nCarly McAdam\n\n\n\n\n\n\n\n\n\n\n\n\nLimits of the Quantitative Approach to Bias and Fairness\n\n\n\n\n\nEssay: Limits of the Quantitative Approach to Bias and Fairness\n\n\n\n\n\nMar 16, 2025\n\n\nCarly McAdam\n\n\n\n\n\n\n\n\n\n\n\n\nReplication Study\n\n\n\n\n\nReplication Study: Dissecting racial bias in an algorithm used to manage the health of populations\n\n\n\n\n\nMar 12, 2025\n\n\nCarly McAdam\n\n\n\n\n\n\n\n\n\n\n\n\nAuditing Bias\n\n\n\n\n\nAuditing for gender bias in income predictions\n\n\n\n\n\nMar 6, 2025\n\n\nCarly McAdam\n\n\n\n\n\n\n\n\n\n\n\n\nAutomated Decison Systems\n\n\n\n\n\nDesign and Impact of Automated Decision Systems\n\n\n\n\n\nFeb 27, 2025\n\n\nCarly McAdam\n\n\n\n\n\n\n\n\n\n\n\n\nClassifying Palmer Penguins\n\n\n\n\n\nClassifying Palmer Penguins\n\n\n\n\n\nFeb 13, 2025\n\n\nCarly McAdam\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/final-project/index.html",
    "href": "posts/final-project/index.html",
    "title": "Final Project- Predicting MLB Pitch Calls",
    "section": "",
    "text": "Abstract\nIn our project, we sought to predict whether umpires will call a pitch correctly or not. We used features of the pitch like its velocity and release position and features of the game situation like number of runners on base, pitch count, and inning to train a logistic regression model. We chose these features because we wanted to look for patterns in pitch calls that weren’t related to where the pitch ended up over the plate. We fit a logistic regression model…. [add results here!]\nFind the source code here\n\n\nIntroduction\nThe problem that we want to address in the project is improving fairness in baseball. We want to see if we can detect patterns in incorrect calls to highlight bias towards or against certain player demographics, game situations, or pitch types. This information could help teams understand when pitchers need to be more precise with their pitch locations and help umpires focus on calling pitches extra carefully during situations when they have been found to be less consistent. We hypothesize that there may be other factors impacting an umpires’ decision to call a strike besides just the location of the pitch when it arrives over the plate.\nStatistics are an essential part of modern day baseball, and all MLB teams have extensive infrastructure in data analytics. Furthermore, because the MLB statistics are publicly available, there are many people using statistical and machine learning methods to make predictions about baseball pitches. Specifically, there has been a lot of work in using classification methods to predict pitch types. Glenn Sidle and Hien Tran, for example, used different multi-class classification methods to predict pitch types (Sidle and Tran (2017)). They used post-processing techniques, like ranking the permuted variable delta error of each feature that was calculated in the construction of their random forest, to investigate the importance of each of the features they used. They found pitch count and batter handedness to be some of the most important features. Even though they were asking a different question than we are, it was still useful to see which features impact the pitch type the most, because these features could also impact the umpire calling the pitch. Or, if if pitch type impacts call correctness, these features would also be important. Additionally, they used categorical features like number of outs, inning, score, and time of day in their model, which helped us figure out how we might also want to use features like these in our model.\nIn another paper that we looked at, Jasmine Barbee was predicting the final pitch outcome for an at-bat (Barbee (2020)). That is, whether the last pitch of the at bat was a strike (strikeout), a ball (walk), put in play but an out, or a hit. Barbee did some exploration into features like pitch speed and she was able to predict with reasonable accuracy the outcome of the final pitch using the pitch count, the horizontal and vertical pitch locations, and the start and end speeds of the pitch. Her process highlighted how these features can be used in conjunction with each other to learn information about a pitch without knowing its position over the plate. When we decided which features to use for our model, we used insights we had gained from other peoples’ work in classifying pitches.\nFuthermore, we also looked at some work that was aiming to answer a question similar to the one that we are. Etan Green and David Daniels at Stanford found that when the pitch count is 3-0 the strike zone expands, and when the pitch count is 0-2, the strike zone contracts (Green and Daniels (2014)). We found these results to support our hypothesis that umpires might not call strikes based soley on the location of the pitch, there may be other factors at play. This paper also prompted us to include the pitch count as a feature in our model. We found that most of these papers, even if they didn’t have the exact same research question that we did, mentioned similar motivations in their work that we have: making the game more fair and helping teams optimize their pitching capabilities to put them in a better position in a game.\n\n\nValues Statement\nThis project would potentially be used by MLB teams and umpires. These teams are probably interested in understanding patterns in when pitches are likely to be called incorrectly, and umpires might be interesting in improving their call accuracy with this technology. [include something about potential results of looking for racial bias maybe].\nThis model could help improve baseball by potentially helping umpires learn patterns in when they are calling pitches incorrectly and improving accuracy. It could benefit players and teams because they could gain a better understanding of situations when pitches are less likely to be called correctly. If they’re aware of these situations, they can try to avoid them.\nBut, the ability for a model to find these patterns could potentially take away jobs from umpires in the long run. Automatic ball strike systems (electronic systems that use the ball position to call pitches instead of umpires) are already being used in minor league baseball. If technology is able to detect that umpires call pitches incorrectly with regularity, this may support the argument to replace home plate umpires with machines.\nPersonally, all of our group members are big baseball fans. One of our favorite things about the game is how important statistics have become in baseball and how much analytical work is done to improve teams’ performances. We were all really excited about the opportunity to work with baseball statistics and we think that this technology could, in theory, make baseball more equitable. Even though this work isn’t necessarily improving the world as a whole, a lot of people care about baseball and making the game more fair might improve teams and fans’ experiences with the game.\n\n\nMaterials and Methods\n\nOur Data\nThe data used in this project comes from Major League Baseball (MLB) pitch tracking data, which records information about pitches thrown during games. The data was collected from publicly available sources, specifically MLB’s official pitch tracking systems and data repositories. Each row in the dataset represents a single pitch thrown by a pitcher, including features such as pitch type, velocity, location, the batter’s handedness (left or right), the umpire’s call (strike or ball), and whether the call was correct. The data also includes metadata about the game, the batter, and the pitcher. The dataset was downloaded from Kaggle, and was uploaded there by user Amandaaa Poor.\nWe also scraped a dataset from the MLB statcast API that includes information about each player active during the 2021 season (see code below). This dataset included batter handedness, batter nationality, position, and other information about each batter in the pitches dataset. We merged with this dataset with the pitches dataset so that we would have information about the pitch and batter for each pitch in the dataset.\nThe primary focus of the our analysis was to determine bias which affects whether the umpire will make the correct call on a pitch, with additional breakdowns by batter handedness (left vs. right) and call type (strikes vs balls). The data was processed to handle missing values, standardize features, and generate additional variables like whether a pitch was on the edge of the strike zone. One potential limitation of the data is its inherent bias due to how pitch tracking systems are calibrated in different ballparks, which may affect accuracy. Additionally, the data does not account for environmental variables (such as lighting conditions or weather) that may influence umpire decisions. Since only pitches that were not hit are included in the analysis, the model may not generalize well to situations where the batter makes contact.\n\n\nCode to pull data from the MLB API\nimport statsapi\nimport pandas as pd\n\ndef get_team_ids():\n    teams = statsapi.get('teams', {'sportId': 1})['teams']\n    return [team['id'] for team in teams]\n\ndef get_players_from_team(team_id, season='2021'):\n    roster = statsapi.get('team_roster', {'teamId': team_id, 'season': season})\n    return roster['roster']\n\ndef get_player_info_flat(player_id):\n    person = statsapi.get('person', {'personId': player_id})\n    info = person['people'][0]\n\n    return {\n        'id': info.get('id'),\n        'fullName': info.get('fullName'),\n        'birthDate': info.get('birthDate'),\n        'birthCountry': info.get('birthCountry'),\n        'height': info.get('height'),\n        'weight': info.get('weight'),\n        'primaryPosition': info.get('primaryPosition', {}).get('abbreviation'),\n        'batSide': info.get('batSide', {}).get('code'),\n        'pitchHand': info.get('pitchHand', {}).get('code'),\n        'debutDate': info.get('mlbDebutDate'),\n        'active': info.get('active'),\n    }\n\ndef get_all_players_df(season='2021'):\n    team_ids = get_team_ids()\n    player_data = []\n\n    for team_id in team_ids:\n        players = get_players_from_team(team_id, season)\n        for player in players:\n            player_id = player['person']['id']\n            info = get_player_info_flat(player_id)\n            player_data.append(info)\n\n    df = pd.DataFrame(player_data)\n    return df\n\ndf_players = get_all_players_df()\nprint(df_players.head())\n\n\nWe joined our two dataframes so that we would have information for each pitch about the demographics of the batter that received each pitch in the pitches dataframe.\n\nimport pandas as pd\n\nplayers = pd.read_csv(\"mlb_players_2021.csv\") # Our dataset\npitches = pd.read_csv(\"regseason.csv\") # Dataset from Kaggle\nplayers.rename(columns={\"id\" : \"batter\",\"fullName\" : \"batter_fullName\" , \"birthCountry\" : \"batter_birthCountry\"}, inplace=True)\nfull_df = pd.merge(pitches, players[[\"batter\",\"batter_fullName\",\"batSide\", \"batter_birthCountry\"]], on=\"batter\")\n\nWe added some features to the dataset and did some cleaning to make it easier for us to work with.\n\n# Remove the pitches that were fouled, in play, or swung at\nnot_hit = [\"called_strike\", \"ball\"]\npitches_not_hit = full_df[full_df['description'].isin(not_hit)].copy()\n\n# Create features needed for our analysis\npitches_not_hit[\"strike_zone\"] = pitches_not_hit[\"zone\"] &lt;= 9 # Determine if the pitch was actually in the zone\n\npitches_not_hit[\"called_correctly\"] = (\n    ((pitches_not_hit[\"zone\"] &lt;= 9) & (pitches_not_hit[\"type\"] == \"S\")) | # Pitch is in the zone and is called a strike  \n    ((pitches_not_hit[\"zone\"] &gt; 9) & (pitches_not_hit[\"type\"] == \"B\"))    # Pitch is not in the zone and is called a ball\n) \npitches_not_hit[\"called_correctly\"] = pitches_not_hit[\"called_correctly\"].astype(int) # This will be are target variable -- whether the pitch was called \"correctly\"\n\nOne of the limitations of our dataset was that there was a large class imbalance, there were significantly more pitches called correctly than called incorrectly, which isn’t necessarily a bad thing. It’s good to see that most of the pitches in the MLB are being called correctly, but this might be challenging for our analysis. We talk about how we handled this in the approach section.\n\n\nNumber of pitches called correctly: 357779\n\n\n\n\nNumber of pitches called incorrectly: 32509\n\n\nWe made some initial visualizations of the data to get an understanding of what we are looking for. The colored area is the strike zone. We are looking to predict when pitches that are in the strike zone are called balls or when pitches that aren’t in the strike zone are called strikes. The plot on the right shows us that these incorrectly called pitches are most likely to be near the edge of the strike zone, which makes sense; these pitches are the hardest to call!\n\n\nCode for plotting strike zones\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Just use 5000 pitches for clarity in the plot\nsubset = pitches.head(5000)\n\ninside_zones = list(range(1, 10))\noutside_zones_subset = sorted(subset[~subset[\"zone\"].isin(inside_zones)][\"zone\"].unique())\n\nrainbow_palette = sns.color_palette(\"turbo\", n_colors=len(inside_zones))\ngray_palette_subset = sns.light_palette(\"gray\", n_colors=len(outside_zones_subset), reverse=True)\n\nzone_colors_subset = {zone: color for zone, color in zip(inside_zones, rainbow_palette)}\nzone_colors_subset.update({zone: color for zone, color in zip(outside_zones_subset, gray_palette_subset)})\n\n# Incorrect pitches\nincorrect_pitches = pitches_not_hit[pitches_not_hit[\"called_correctly\"] == 0]\noutside_zones_incorrect = sorted(incorrect_pitches[~incorrect_pitches[\"zone\"].isin(inside_zones)][\"zone\"].unique())\ngray_palette_incorrect = sns.light_palette(\"gray\", n_colors=len(outside_zones_incorrect), reverse=True)\n\nzone_colors_incorrect = {zone: color for zone, color in zip(inside_zones, rainbow_palette)}\nzone_colors_incorrect.update({zone: color for zone, color in zip(outside_zones_incorrect, gray_palette_incorrect)})\n\n# Create side-by-side plots\nfig, axes = plt.subplots(1, 2, figsize=(16, 6), sharex=True, sharey=True)\n\n# Plot 1: All pitches\nsns.scatterplot(\n    data=subset,\n    x=\"plate_x\",\n    y=\"plate_z\",\n    hue=\"zone\",\n    palette=zone_colors_subset,\n    legend=False,\n    ax=axes[0]\n)\naxes[0].set_xlim(-3, 3)\naxes[0].set_ylim(0, 5)\naxes[0].set_title(\"All Pitch Zone Locations\")\naxes[0].set_xlabel(\"Plate X\")\naxes[0].set_ylabel(\"Plate Z\")\n\n# Plot 2: Incorrect pitches\nsns.scatterplot(\n    data=incorrect_pitches,\n    x=\"plate_x\",\n    y=\"plate_z\",\n    hue=\"zone\",\n    palette=zone_colors_incorrect,\n    legend=False,\n    ax=axes[1]\n)\naxes[1].set_xlim(-3, 3)\naxes[1].set_ylim(0, 5)\naxes[1].set_title(\"Incorrectly Called Pitch Zone Locations\")\naxes[1].set_xlabel(\"Plate X\")\naxes[1].set_ylabel(\"\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nOur Approach\nWe thought about using features in a few different ways, as we began with two large datasets, one about the information of the specific pitch, which included its speed, release information and landing information. We also had a dataset with player information that we merged. We created our target variable: weather the umpires call (strike or ball) matched the physical location of the pitch (strike or ball). Subsequently, we had to subset our data by only pitches that were not hit. We also downsampled our data to even out the number of pitches called correctly and incorrectly.\nWe started off by including all variables, but realized this made the model too good- there were combinations of features that would give the model an almost direct indication of the status of the target variable. For example, when the count was full, the pitch was a ball and the chance of the batting team scoring a run increased, the model learned that 100% of the time, that pitch was called correctly (the umpire also called it a ball). In order to deal with this, we limited the features to only the ones that didn’t give any indication as to where the pitch ended up.\nThis made our model perform decently well, around 60-70% accuracy, but we quickly realized that the model was only accurate when predicting missed calls for pitches that were balls and and bad for pitches that were strikes.\nThinking about this, we realized that there are a ton of pitches in our dataset that no matter the game situation or umpire bias wil be called correctly. These are pitches straight down the middle or pitches way outside of the zone. Realizing this, we subset out data to only include pitches within two inches on either side of the strike zone. This narrows down our analysis to only pitches that umpires might call wrong with a higher probability.\nOur analysis focused on evaluating the accuracy of umpire calls using two logistic regression models: a standard Scikit-Learn Logistic Regression model and a Custom Logistic Regression with Newton’s Method. The primary features used as predictors included key pitch characteristics such as type, velocity, and horizontal and vertical position at the plate, along with batter attributes like handedness (left or right), and game context including pitch count, inning, and game situation; importantly, in the first iterations of the models information surrounding where the pitch ended up around the plate was excluded and was later added back in for pitches close to the edge of the strike zone. The target variable for prediction was whether the umpire’s call was correct, represented as a binary outcome (correct or incorrect).\nBoth models were trained separately, with the Scikit-Learn model serving as a baseline comparison and the custom Newton-based model leveraging second-order optimization for potentially faster and more efficient parameter updates. Model evaluation was conducted using key metrics such as Accuracy, False Positive Rate (FPR), and False Negative Rate (FNR). Additionally, confusion matrices were generated for visual inspection of model performance across subgroups, and the predictions were benchmarked against actual umpire accuracy to identify significant deviations or biases in decision-making.\n\n\n\nResults\nThe Scikit-Learn model showed a higher overall accuracy compared to the Custom Newton model. However, the Custom Newton model exhibited more stable convergence during training and demonstrated competitive performance in edge cases near the strike zone. The Scikit-Learn model struggled with overfitting to specific pitch types, particularly balls however the Newton struggled to converge on the data. Perhaps with more computing to run many more iterations of our Newton descent it could have converged; the Newton did however demonstrated much better generalizability to both balls and strikes as well as left vs right handed batters. Confusion matrices were generated for left-handed and right-handed batters, revealing that both models performed better for right-handed batters, with fewer false positives and a clearer decision boundary compared to left batters.\nConfusion matrices were generated for left-handed and right-handed batters, highlighting key differences in model performance. Both models demonstrated improved accuracy for right-handed batters, with fewer false positives and a clearer decision boundary. This may suggest that the models are better calibrated for right-handed batting patterns or that there is an inherent bias in pitch tracking systems that favors right-handed batters. Conversely, left-handed batters experienced slightly higher error rates, particularly around borderline pitch locations. This discrepancy underlines the need for model adjustments to account for variability introduced by left-handed batting stances.\n\n\n\nimage.png\n\n\nInterestingly, the Custom Newton model consistently predicted umpires to always make the incorrect call for both handedness groups and for balls versus strikes. While this resulted in more balanced predictive power across these categories, it did not necessarily translate into higher accuracy. Given more computational resources, it is plausible that additional iterations of the Newton descent could have achieved better convergence and accuracy and removed this trend.\n\n\n\nimage-2.png\n\n\nFurther analysis focused on balls and strikes separately, revealing distinct behavioral patterns for each model. The Scikit-Learn model generally showed stronger stability across a broader range of pitch types and batter handedness. Its higher False Positive Rate (FPR) and False Negative Rate (FNR) scores were indicative of more conservative decision-making, particularly for borderline calls. This conservatism led to highly consistent identification of balls but resulted in nearly always missing whether or not the umpire made the correct call for strikes.\n\n\n\nimage-4.png\n\n\nOn the other hand, the Custom Newton model demonstrated slightly higher sensitivity when classifying strikes. This is likely due to the error in how the Newton model was predicting umpire calls. This error lead to increased false positives and much of the predictive power from the Newton model should be disregarded, espeically due to false convergence.\nThe inability of the Custom Newton model to fully converge may be attributed to a combination of factors: dataset size, limited computational power, or an absence of clear patterns within the data. With further optimization and computational resources, Newton-based optimization could potentially achieve more accurate predictions, especially for classifications whithin specific regions such as handedness and edge-zone strikes.\n\n\n\nimage-3.png\n\n\n\n\nConcluding Discussion\nOur project was successful in that we were able to use a model that worked to predict our target variable. Specifically, we were able to predict with reasonable accuracy whether a ball would be called incorrectly. But, we also had some failures in our project, like the failure of our custom model to converge. We found some interesting results in our audit for bias, for example differences in model accuracy between right- and left-handed batters. We felt that we met the goals that we set at the beginning of the project. We were successful in creating a model and finding some interesting results. Compared some of the other work in this area, we found results along the lines of what we expected. The features that we found to be most important aligned with some of the papers that we read in preparation for the project, like pitch count. We didn’t see anyone look at model performance across batter-handedness, so this was a particularly interesting result. If we had more time to work on the project, we would have two main priorities. First, we’d want to further investigate the discrepancy between model performance for balls and strikes. We discussed a few hypotheses about why we thought this difference occurred, but we’d want to further investigate the features we are using in our model to see if any of them were overly correlated to our target variable or where the pitch ended up relative to the strike zone over the plate. We would also want to continue working with our custom model to see if we could get it to converge. Although the issues with convergence could be attributed to lack of patterns in the data, with more tuning of parameters and the features we train our model on, we may have been able to see a better performance from the model.\n\n\nGroup Contributions Statement\nOur group spent a lot of time working in person together on the project, so we all contributed to each part of the source code. But, in general, each person contributed the most to the file with their name on it. Carly did all of the data pulling, data cleaning, outside research, and initial visualizations. Chloe worked on figuring out which features we should train our model on and initial evaluation of which features were most important. Ryan took the lead on the custom LR model and auditing for bias. We all worked on creating this blog post. Carly wrote the introduction, data explanation, values statement, and strike zone visualizations. Chloe worked on documenting our process, specifically how we subsetted the data and how we chose our features. Ryan worked on explaining our models and the results from our auditing. Overall, we all felt that we contributed equally to the project and we each took the lead on different parts of the project.\n\n\nPersonal Reflection\nI really enjoyed working on this project and I felt like I achieved most of my goals. It was frustrating that we weren’t able to get our custom model to converge, but I was proud that we were able to make changes to the custom LR model and get it to work (somewhat). It was challenging to work with a real dataset and come up with a way to predict a target variable. Often time when we are given an assignment, the dataset is already known to have patterns that we will be able to detect. Or, there is known bias that we will be able to see from doing an audit for bias. But, in this project, we didn’t know whether we would be able to find any patterns in the data. It could be frustrating at times to have to try out lots of different features, figure out where correlation might be coming from, or accurately assess the results of our models. I learned a lot from researching this project about different ways that statistics and machine learning are being used in baseball. Even though a lot of the work that I read in preparation for this project was more statistically rigorous then the work we did in this class, I enjoyed being able to apply some of the methods ourselves and generating similar results. I also learned about this importance of really understanding all of the features in a dataset and how they interact with each other. When it came time to communicate our findings, we needed to have a really solid understanding of how everything in our dataset was interacting in order to understand what was happening with our results. I learned a lot about how to go from data collection to a model to results from this project. I’ll definitely use this knowledge going forward in my career. I know I’m going to be working with lots of different datasets and I think this project gave me some foundational tools for understanding the interaction between features in a new dataset. This project also required patience and perseverance, both skills I can always use more practice on.\n\n\n\n\n\nReferences\n\nBarbee, Jasmine. 2020. “Prediction of Final Pitch Outcome in MLB Games Using Statistical Learning Methods.” California State University, Long Beach; thesis; ProQuest One Academic.\n\n\nGreen, Etan, and David P. Daniels. 2014. “What Does It Take to Call a Strike? Three Biases in Umpire Decision Making.” MIT Sloan Sports Analytics Conference.\n\n\nSidle, Glenn, and Hien Tran. 2017. “Using Multi-Class Classification Methods to Predict Baseball Pitch Types.” Sage Journals Journal of Sports Analytics."
  }
]